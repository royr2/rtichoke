[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "R'tichoke",
    "section": "",
    "text": "Multi-Task Learning with torch in R\n\n\n\n\n\n\nR\n\n\nDeep Learning\n\n\ntorch\n\n\nMulti-Task Learning\n\n\n\n\n\n\n\n\n\nMay 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Simple Neural Network in R with torch\n\n\n\n\n\n\nR\n\n\nDeep Learning\n\n\ntorch\n\n\n\n\n\n\n\n\n\nDec 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTutorial: Assessing Credit Score Prediction Reliability Using Bootstrap Resampling\n\n\n\n\n\n\nR\n\n\nCredit Risk Analytics\n\n\nBootstrapping\n\n\n\n\n\n\n\n\n\nNov 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Models in R with tidymodels\n\n\n\n\n\n\nR\n\n\nMachine Learning\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\nOct 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOptimizing XGBoost Hyperparameters Using Bayesian Optimization in R\n\n\n\n\n\n\nR\n\n\nAnalytics\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nSep 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Particle Swarm Optimization from Scratch in R\n\n\n\n\n\n\nR\n\n\nOptimization\n\n\nVisualization\n\n\n\n\n\n\n\n\n\nJul 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDeveloping Custom Charting Functions with ggplot2\n\n\n\n\n\n\nR\n\n\nData Visualization\n\n\nggplot2\n\n\n\n\n\n\n\n\n\nMay 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating Correlated Random Numbers in R Using Matrix Methods\n\n\n\n\n\n\nR\n\n\nStatistics\n\n\nSimulation\n\n\n\n\n\n\n\n\n\nMar 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating Binary Classification Models Using Gains Tables\n\n\n\n\n\n\nR\n\n\nCredit Risk Analytics\n\n\nModel Evaluation\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPortfolio Optimization Using PSO\n\n\n\n\n\n\nR\n\n\nFinance\n\n\nOptimization\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to R for Analytics\n\n\n\n\n\n\nR\n\n\nAnalytics\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\nMar 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMonotonic Binning Using XGBoost\n\n\n\n\n\n\nR\n\n\nCredit Risk Analytics\n\n\nXGBoost\n\n\n\n\n\n\n\n\n\nJan 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with Python using R and Reticulate\n\n\n\n\n\n\nR\n\n\nPython\n\n\nreticulate\n\n\n\n\n\n\n\n\n\nJan 15, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html",
    "href": "posts/monotonic-binning-using-xgboost.html",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "This post focuses on how to implement monotonic binning, a method that groups variable values into bins where event rates demonstrate consistent monotonic behavior. This methodology is essential in credit risk modeling, providing significant advantages in two critical areas:"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#required-libraries",
    "href": "posts/monotonic-binning-using-xgboost.html#required-libraries",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "We’ll use the following R packages for this demonstration:\n\nlibrary(recipes)  # For data preprocessing\nlibrary(dplyr)    # For data manipulation\nlibrary(xgboost)  # For creating monotonic bins\nlibrary(ggplot2)  # For visualization"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#sample-dataset",
    "href": "posts/monotonic-binning-using-xgboost.html#sample-dataset",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "For this demonstration, we’ll use a sample from the Lending Club dataset, which contains loan information including whether loans defaulted:\n\n# Load sample data from Lending Club dataset\nsample &lt;- read.csv(\"https://bit.ly/42ypcnJ\")\n\n# Check dimensions of the dataset\ndim(sample)\n\n[1] 10000   153"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#creating-a-target-variable",
    "href": "posts/monotonic-binning-using-xgboost.html#creating-a-target-variable",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "First, we need to create a binary target variable that indicates whether a loan defaulted (1) or not (0):\n\n# Define loan statuses that represent defaults\ncodes &lt;- c(\"Charged Off\", \"Does not meet the credit policy. Status:Charged Off\")\n\n# Create binary target variable\nmodel_data &lt;- sample %&gt;%\n  mutate(bad_flag = ifelse(loan_status %in% codes, 1, 0))"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#data-preparation",
    "href": "posts/monotonic-binning-using-xgboost.html#data-preparation",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "Next, we’ll preprocess the data using the recipes package to: 1. Select only numeric variables 2. Impute missing values with median values\n\n# Create a recipe for preprocessing\nrec &lt;- recipe(bad_flag ~ ., data = model_data) %&gt;%\n  step_select(where(is.numeric)) %&gt;%  # Keep only numeric variables\n  step_impute_median(all_predictors())  # Fill missing values with medians\n\n# Apply the preprocessing steps\nrec &lt;- prep(rec, training = model_data)\ntrain &lt;- bake(rec, new_data = model_data)"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#analyzing-directional-trends",
    "href": "posts/monotonic-binning-using-xgboost.html#analyzing-directional-trends",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "Before creating monotonic bins, it’s helpful to visualize the raw relationship between a predictor variable and the target. Let’s examine how the number of credit inquiries in the past 6 months relates to default rates:\n\n# Create dataframe with inquiries and default flag\ndata.frame(x = model_data$inq_last_6mths,\n           y = model_data$bad_flag) %&gt;%\n  filter(x &lt;= 5) %&gt;%  # Focus on 0-5 inquiries for clarity\n  group_by(x) %&gt;% \n  summarise(count = n(),  # Count observations in each group\n            events = sum(y)) %&gt;%  # Count defaults in each group\n  mutate(pct = events/count) %&gt;%  # Calculate default rate\n  ggplot(aes(x = factor(x), y = pct)) + \n  geom_col() + \n  theme_minimal() + \n  labs(x = \"# of inquiries in past 6 months\", \n       y = \"Default rate\",\n       title = \"Default rate vs number of inquiries\")\n\n\n\n\n\n\n\n\nNotice that while there’s a general upward trend (more inquiries correlate with higher default rates), the relationship isn’t perfectly monotonic. This is where our binning approach will help."
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#creating-monotonic-bins-with-xgboost",
    "href": "posts/monotonic-binning-using-xgboost.html#creating-monotonic-bins-with-xgboost",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "Now we’ll leverage XGBoost’s monotonicity constraints to create bins that have a strictly increasing relationship with default rates. The key parameter is monotone_constraints = 1, which forces the model to create splits that maintain a positive relationship with the target:\n\n# Train XGBoost model with monotonicity constraint\nmdl &lt;- xgboost(\n  data = train %&gt;%\n    select(inq_last_6mths) %&gt;%  # Use only the inquiries variable\n    as.matrix(),  \n  label = train[[\"bad_flag\"]],  # Target variable\n  nrounds = 5,  # Number of boosting rounds\n  params = list(\n    booster = \"gbtree\",\n    objective = \"binary:logistic\",\n    monotone_constraints = 1,  # Force positive relationship\n    max_depth = 1  # Simple trees with single splits\n  ),\n  verbose = 0  # Suppress output\n)"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#retrieving-split-points-and-creating-bins",
    "href": "posts/monotonic-binning-using-xgboost.html#retrieving-split-points-and-creating-bins",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "After training the model, we can extract the split points that XGBoost identified and use them to create our monotonic bins:\n\n# Extract split points from the model\nsplits &lt;- xgb.model.dt.tree(model = mdl)  \n\n# Create bin boundaries including -Inf and Inf for complete coverage\ncuts &lt;- c(-Inf, unique(sort(splits$Split)), Inf)\n\n# Create and visualize the monotonic bins\ndata.frame(target = train$bad_flag,\n           buckets = cut(train$inq_last_6mths, \n                         breaks = cuts, \n                         include.lowest = TRUE, \n                         right = TRUE)) %&gt;% \n  group_by(buckets) %&gt;%\n  summarise(total = n(),  # Count observations in each bin\n            events = sum(target == 1)) %&gt;%  # Count defaults in each bin\n  mutate(pct = events/total) %&gt;%  # Calculate default rate\n  ggplot(aes(x = buckets, y = pct)) + \n  geom_col() + \n  theme_minimal() + \n  labs(x = \"Bins\", \n       y = \"Default rate\",\n       title = \"Monotonic Bins for Inquiries\")\n\n\n\n\n\n\n\n\nNotice how the default rates now increase monotonically across the bins, making the relationship clearer and more interpretable compared to the raw data we visualized earlier."
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#creating-a-reusable-function",
    "href": "posts/monotonic-binning-using-xgboost.html#creating-a-reusable-function",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "To make this process more efficient for multiple variables, let’s create a reusable function that handles the entire binning workflow:\n\ncreate_bins &lt;- function(var, outcome, max_depth = 10, plot = TRUE){\n  # Determine relationship direction automatically\n  corr &lt;- cor(var, outcome, method = \"spearman\")\n  direction &lt;- ifelse(corr &gt; 0, 1, -1)  # 1 for positive, -1 for negative correlation\n  \n  # Build XGBoost model with appropriate monotonicity constraint\n  mdl &lt;- xgboost(\n    verbose = 0,\n    data = as.matrix(var),\n    label = outcome,\n    nrounds = 100,  # Single round is sufficient for binning\n    params = list(objective = \"binary:logistic\",\n                  monotone_constraints = direction,  # Apply constraint based on correlation\n                  max_depth = max_depth))  # Control tree complexity\n  \n  # Extract and return split points\n  splits &lt;- xgb.model.dt.tree(model = mdl)\n  cuts &lt;- c(-Inf, sort(unique(splits$Split)), Inf)  # Include boundaries for complete coverage\n  \n  # Optionally visualize the bins\n  if(plot) {\n    data.frame(target = outcome,\n               buckets = cut(var, \n                            breaks = cuts, \n                            include.lowest = TRUE, \n                            right = TRUE)) %&gt;% \n      group_by(buckets) %&gt;%\n      summarise(total = n(),\n                events = sum(target == 1)) %&gt;%\n      mutate(pct = events/total) %&gt;%\n      ggplot(aes(x = buckets, y = pct)) + \n      geom_col() + \n      theme_minimal() + \n      labs(x = \"Bins\", \n           y = \"Default rate\",\n           title = \"Monotonic Bins\")\n  }\n  \n  return(cuts)  # Return the bin boundaries\n}"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#example-usage",
    "href": "posts/monotonic-binning-using-xgboost.html#example-usage",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "You can use this function to create monotonic bins for any numeric variable by passing the variable and outcome columns:\n\n# Example: Create monotonic bins for annual income\nincome_bins &lt;- create_bins(\n  var = train$annual_inc,\n  outcome = train$bad_flag,\n  max_depth = 5\n)"
  },
  {
    "objectID": "posts/intro-to-r-analytics.html",
    "href": "posts/intro-to-r-analytics.html",
    "title": "Introduction to R for Analytics",
    "section": "",
    "text": "R is a powerful language specifically designed for data analysis and visualization. This guide demonstrates practical examples of using R for real-world analytics tasks."
  },
  {
    "objectID": "posts/intro-to-r-analytics.html#exploring-your-first-dataset",
    "href": "posts/intro-to-r-analytics.html#exploring-your-first-dataset",
    "title": "Introduction to R for Analytics",
    "section": "Exploring Your First Dataset",
    "text": "Exploring Your First Dataset\nR comes with several built-in datasets perfect for practice. Let’s start by examining the mtcars dataset:\n\n# View the first few rows\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n# Quick summary of the dataset structure\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n# Statistical summary of key variables\nsummary(mtcars[, c(\"mpg\", \"wt\", \"hp\")])\n\n      mpg              wt              hp       \n Min.   :10.40   Min.   :1.513   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:2.581   1st Qu.: 96.5  \n Median :19.20   Median :3.325   Median :123.0  \n Mean   :20.09   Mean   :3.217   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:3.610   3rd Qu.:180.0  \n Max.   :33.90   Max.   :5.424   Max.   :335.0  \n\n\nThe mtcars dataset contains information about 32 cars from Motor Trend magazine, including fuel efficiency (mpg), weight (wt), and horsepower (hp)."
  },
  {
    "objectID": "posts/intro-to-r-analytics.html#effective-data-visualization",
    "href": "posts/intro-to-r-analytics.html#effective-data-visualization",
    "title": "Introduction to R for Analytics",
    "section": "Effective Data Visualization",
    "text": "Effective Data Visualization\nVisualization is essential for understanding patterns in your data. Let’s create some informative plots:\n\nlibrary(ggplot2)\n\n# 1. A scatter plot with regression line\np1 &lt;- ggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(aes(size = hp, color = factor(cyl)), alpha = 0.7) +\n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"#2c3e50\") +\n  labs(title = \"Car Weight vs. Fuel Efficiency\",\n       subtitle = \"Size represents horsepower, color represents cylinders\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\", name = \"Cylinders\")\n\n# 2. Distribution of fuel efficiency\np2 &lt;- ggplot(mtcars, aes(x = mpg, fill = factor(cyl))) +\n  geom_histogram(bins = 10, alpha = 0.7, position = \"identity\") +\n  labs(title = \"Distribution of Fuel Efficiency\",\n       x = \"Miles Per Gallon\",\n       y = \"Count\") +\n  scale_fill_brewer(palette = \"Set1\", name = \"Cylinders\") +\n  theme_minimal()\n\n# Display plots (if using patchwork)\nlibrary(patchwork)\np1 / p2\n\n\n\n\n\n\n\n\nThese visualizations reveal:\n\nA clear negative correlation between car weight and fuel efficiency\nHigher cylinder cars tend to be heavier with lower MPG\nThe MPG distribution varies significantly by cylinder count"
  },
  {
    "objectID": "posts/intro-to-r-analytics.html#practical-data-transformation",
    "href": "posts/intro-to-r-analytics.html#practical-data-transformation",
    "title": "Introduction to R for Analytics",
    "section": "Practical Data Transformation",
    "text": "Practical Data Transformation\nData rarely comes in the exact format you need. The dplyr package makes transformations straightforward:\n\n# Load required packages\nlibrary(dplyr)\nlibrary(tibble)  # For rownames_to_column function\n\n# Create an enhanced version of the dataset\nmtcars_enhanced &lt;- mtcars %&gt;%\n  # Add car names as a column (they're currently row names)\n  rownames_to_column(\"car_name\") %&gt;%\n  # Create useful derived metrics\n  mutate(\n    # Efficiency ratio (higher is better)\n    efficiency_ratio = mpg / wt,\n    \n    # Power-to-weight ratio (higher is better)\n    power_to_weight = hp / wt,\n    \n    # Categorize cars by efficiency\n    efficiency_category = case_when(\n      mpg &gt; 25 ~ \"High Efficiency\",\n      mpg &gt; 15 ~ \"Medium Efficiency\",\n      TRUE ~ \"Low Efficiency\"\n    )\n  ) %&gt;%\n  # Arrange from most to least efficient\n  arrange(desc(efficiency_ratio))\n\n# Display the top 5 most efficient cars\nhead(mtcars_enhanced[, c(\"car_name\", \"mpg\", \"wt\", \"hp\", \"efficiency_ratio\", \"efficiency_category\")], 5)\n\n        car_name  mpg    wt  hp efficiency_ratio efficiency_category\n1   Lotus Europa 30.4 1.513 113         20.09253     High Efficiency\n2    Honda Civic 30.4 1.615  52         18.82353     High Efficiency\n3 Toyota Corolla 33.9 1.835  65         18.47411     High Efficiency\n4       Fiat 128 32.4 2.200  66         14.72727     High Efficiency\n5      Fiat X1-9 27.3 1.935  66         14.10853     High Efficiency"
  },
  {
    "objectID": "posts/intro-to-r-analytics.html#answering-business-questions-with-data",
    "href": "posts/intro-to-r-analytics.html#answering-business-questions-with-data",
    "title": "Introduction to R for Analytics",
    "section": "Answering Business Questions with Data",
    "text": "Answering Business Questions with Data\nThe enhanced dataset can be used to answer practical questions:\n\n# Question 1: What are the average characteristics by cylinder count?\ncylinder_analysis &lt;- mtcars_enhanced %&gt;%\n  group_by(cyl) %&gt;%\n  summarize(\n    count = n(),\n    avg_mpg = mean(mpg),\n    avg_weight = mean(wt),\n    avg_horsepower = mean(hp),\n    avg_efficiency_ratio = mean(efficiency_ratio),\n    avg_power_to_weight = mean(power_to_weight)\n  ) %&gt;%\n  arrange(cyl)\n\n# Display the results\ncylinder_analysis\n\n# A tibble: 3 × 7\n    cyl count avg_mpg avg_weight avg_horsepower avg_efficiency_ratio\n  &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;                &lt;dbl&gt;\n1     4    11    26.7       2.29           82.6                12.7 \n2     6     7    19.7       3.12          122.                  6.44\n3     8    14    15.1       4.00          209.                  3.95\n# ℹ 1 more variable: avg_power_to_weight &lt;dbl&gt;\n\n# Question 2: Which transmission type is more fuel efficient?\ntransmission_efficiency &lt;- mtcars_enhanced %&gt;%\n  # am: 0 = automatic, 1 = manual\n  mutate(transmission = if_else(am == 1, \"Manual\", \"Automatic\")) %&gt;%\n  group_by(transmission) %&gt;%\n  summarize(\n    count = n(),\n    avg_mpg = mean(mpg),\n    median_mpg = median(mpg),\n    mpg_std_dev = sd(mpg)\n  )\n\n# Display the results\ntransmission_efficiency\n\n# A tibble: 2 × 5\n  transmission count avg_mpg median_mpg mpg_std_dev\n  &lt;chr&gt;        &lt;int&gt;   &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1 Automatic       19    17.1       17.3        3.83\n2 Manual          13    24.4       22.8        6.17\n\n# Visualize the difference\nggplot(mtcars, aes(x = factor(am, labels = c(\"Automatic\", \"Manual\")), y = mpg, fill = factor(am))) +\n  geom_boxplot(alpha = 0.7) +\n  geom_jitter(width = 0.1, alpha = 0.5) +\n  labs(title = \"Fuel Efficiency by Transmission Type\",\n       x = \"Transmission Type\",\n       y = \"Miles Per Gallon\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/intro-to-r-analytics.html#correlation-analysis-for-decision-making",
    "href": "posts/intro-to-r-analytics.html#correlation-analysis-for-decision-making",
    "title": "Introduction to R for Analytics",
    "section": "Correlation Analysis for Decision Making",
    "text": "Correlation Analysis for Decision Making\nUnderstanding relationships between variables is crucial for business decisions:\n\n# Calculate correlations\ncor_matrix &lt;- cor(mtcars[, c(\"mpg\", \"wt\", \"hp\", \"disp\", \"qsec\")])\ncor_df &lt;- round(cor_matrix, 2)\n\n# Display correlation matrix\ncor_df\n\n       mpg    wt    hp  disp  qsec\nmpg   1.00 -0.87 -0.78 -0.85  0.42\nwt   -0.87  1.00  0.66  0.89 -0.17\nhp   -0.78  0.66  1.00  0.79 -0.71\ndisp -0.85  0.89  0.79  1.00 -0.43\nqsec  0.42 -0.17 -0.71 -0.43  1.00\n\n# Visualize correlations (requires the corrplot package)\nlibrary(corrplot)\ncorrplot(cor_matrix, method = \"circle\", type = \"upper\", \n         tl.col = \"black\", tl.srt = 45, addCoef.col = \"black\")\n\n\n\n\n\n\n\n# Scatter plot matrix of key variables\npairs(mtcars[, c(\"mpg\", \"wt\", \"hp\", \"disp\")], \n      main = \"Scatter Plot Matrix of Key Variables\",\n      pch = 21, bg = \"lightblue\", cex = 1.2)"
  },
  {
    "objectID": "posts/intro-to-r-analytics.html#working-with-real-world-datasets",
    "href": "posts/intro-to-r-analytics.html#working-with-real-world-datasets",
    "title": "Introduction to R for Analytics",
    "section": "Working with Real-World Datasets",
    "text": "Working with Real-World Datasets\nThe famous Iris dataset demonstrates a complete workflow:\n\n# Load packages\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Examine the dataset\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n# Calculate summary statistics by species\niris_stats &lt;- iris %&gt;%\n  group_by(Species) %&gt;%\n  summarize(across(where(is.numeric), \n                   list(mean = mean, \n                        median = median,\n                        sd = sd,\n                        min = min,\n                        max = max)))\n\n# View summary for Sepal.Length\niris_stats %&gt;% select(Species, starts_with(\"Sepal.Length\"))\n\n# A tibble: 3 × 6\n  Species Sepal.Length_mean Sepal.Length_median Sepal.Length_sd Sepal.Length_min\n  &lt;fct&gt;               &lt;dbl&gt;               &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n1 setosa               5.01                 5             0.352              4.3\n2 versic…              5.94                 5.9           0.516              4.9\n3 virgin…              6.59                 6.5           0.636              4.9\n# ℹ 1 more variable: Sepal.Length_max &lt;dbl&gt;\n\n# Create a visualization comparing all measurements across species\niris_long &lt;- iris %&gt;%\n  pivot_longer(\n    cols = -Species,\n    names_to = \"Measurement\",\n    values_to = \"Value\"\n  )\n\n# Box plots with data points\nggplot(iris_long, aes(x = Species, y = Value, fill = Species)) +\n  geom_boxplot(alpha = 0.6) +\n  geom_jitter(width = 0.15, alpha = 0.5, color = \"darkgrey\") +\n  facet_wrap(~Measurement, scales = \"free_y\") +\n  labs(title = \"Iris Measurements Across Species\",\n       subtitle = \"Box plots with individual observations\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n# Find the most distinguishing features between species\niris_wide &lt;- iris %&gt;%\n  pivot_longer(cols = -Species, names_to = \"Measurement\", values_to = \"Value\") %&gt;%\n  group_by(Measurement, Species) %&gt;%\n  summarise(mean_value = mean(Value), .groups = \"drop\") %&gt;%\n  pivot_wider(names_from = Species, values_from = mean_value) %&gt;%\n  mutate(versicolor_vs_setosa = abs(versicolor - setosa),\n         virginica_vs_setosa = abs(virginica - setosa),\n         virginica_vs_versicolor = abs(virginica - versicolor),\n         max_difference = pmax(versicolor_vs_setosa, virginica_vs_setosa, virginica_vs_versicolor))\n\n# Display the results ordered by maximum difference\niris_wide %&gt;% arrange(desc(max_difference))\n\n# A tibble: 4 × 8\n  Measurement  setosa versicolor virginica versicolor_vs_setosa\n  &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;                &lt;dbl&gt;\n1 Petal.Length  1.46        4.26      5.55                2.80 \n2 Petal.Width   0.246       1.33      2.03                1.08 \n3 Sepal.Length  5.01        5.94      6.59                0.93 \n4 Sepal.Width   3.43        2.77      2.97                0.658\n# ℹ 3 more variables: virginica_vs_setosa &lt;dbl&gt;, virginica_vs_versicolor &lt;dbl&gt;,\n#   max_difference &lt;dbl&gt;"
  },
  {
    "objectID": "posts/intro-to-r-analytics.html#handling-missing-data-in-practice",
    "href": "posts/intro-to-r-analytics.html#handling-missing-data-in-practice",
    "title": "Introduction to R for Analytics",
    "section": "Handling Missing Data in Practice",
    "text": "Handling Missing Data in Practice\nLet’s tackle the common challenge of missing data using a practical example:\n\n# Create a simulated customer dataset with missing values\nset.seed(123) # For reproducibility\n\ncustomers &lt;- data.frame(\n  customer_id = 1:100,\n  age = sample(18:70, 100, replace = TRUE),\n  income = round(rnorm(100, 50000, 15000)),\n  years_as_customer = sample(0:20, 100, replace = TRUE),\n  purchase_frequency = sample(1:10, 100, replace = TRUE)\n)\n\n# Introduce missing values randomly\nset.seed(456)\ncustomers$age[sample(1:100, 10)] &lt;- NA\ncustomers$income[sample(1:100, 15)] &lt;- NA\ncustomers$purchase_frequency[sample(1:100, 5)] &lt;- NA\n\n# 1. Identify missing data\nmissing_summary &lt;- sapply(customers, function(x) sum(is.na(x)))\nmissing_summary\n\n       customer_id                age             income  years_as_customer \n                 0                 10                 15                  0 \npurchase_frequency \n                 5 \n\n# 2. Visualize the pattern of missing data\nlibrary(naniar) # May need to install this package\nvis_miss(customers)\n\n\n\n\n\n\n\n# 3. Handle missing data with multiple approaches\n\n# Option A: Remove rows with any missing values\nclean_customers &lt;- na.omit(customers)\nnrow(customers) - nrow(clean_customers) # Number of rows removed\n\n[1] 26\n\n# Option B: Impute with mean/median (numeric variables only)\nimputed_customers &lt;- customers %&gt;%\n  mutate(\n    age = ifelse(is.na(age), median(age, na.rm = TRUE), age),\n    income = ifelse(is.na(income), mean(income, na.rm = TRUE), income),\n    purchase_frequency = ifelse(is.na(purchase_frequency), \n                               median(purchase_frequency, na.rm = TRUE), \n                               purchase_frequency)\n  )\n\n# Option C: Predictive imputation (using age to predict income)\nlibrary(mice) # For more sophisticated imputation\n# Quick imputation model - in practice you'd use more parameters\nimputed_data &lt;- mice(customers, m = 5, method = \"pmm\", printFlag = FALSE)\ncustomers_complete &lt;- complete(imputed_data)\n\n# Compare results by calculating customer value score\ncalculate_value &lt;- function(df) {\n  df %&gt;%\n    mutate(customer_value = (income/10000) * (purchase_frequency/10) * log(years_as_customer + 1)) %&gt;%\n    arrange(desc(customer_value)) %&gt;%\n    select(customer_id, customer_value, everything())\n}\n\n# Top 5 customers by value (original with NAs removed)\nhead(calculate_value(clean_customers), 5)\n\n  customer_id customer_value age income years_as_customer purchase_frequency\n1           7       24.63960  67  82249                19                 10\n2          54       15.73965  22  70961                15                  8\n3          59       15.67045  50  70649                15                  8\n4          84       15.09251  21  55732                14                 10\n5          72       14.27848  23  61853                12                  9\n\n# Top 5 customers by value (with imputed values)\nhead(calculate_value(customers_complete), 5)\n\n  customer_id customer_value age income years_as_customer purchase_frequency\n1           7       24.63960  67  82249                19                 10\n2          54       15.73965  22  70961                15                  8\n3          59       15.67045  50  70649                15                  8\n4          84       15.09251  21  55732                14                 10\n5          72       14.27848  23  61853                12                  9"
  },
  {
    "objectID": "posts/intro-to-r-analytics.html#time-series-analysis-for-business-trends",
    "href": "posts/intro-to-r-analytics.html#time-series-analysis-for-business-trends",
    "title": "Introduction to R for Analytics",
    "section": "Time Series Analysis for Business Trends",
    "text": "Time Series Analysis for Business Trends\nTime series analysis is essential for understanding business trends and forecasting:\n\n# Load packages\nlibrary(forecast)\nlibrary(tseries)\n\n# Examine the built-in AirPassengers dataset (monthly air passengers from 1949 to 1960)\ndata(AirPassengers)\nclass(AirPassengers)\n\n[1] \"ts\"\n\n# Plot the time series\nautoplots &lt;- autoplot(AirPassengers) +\n  labs(title = \"Monthly Air Passengers (1949-1960)\",\n       y = \"Passenger Count\",\n       x = \"Year\") +\n  theme_minimal()\n\n# Decompose the time series into seasonal components\ndecomposed &lt;- decompose(AirPassengers, \"multiplicative\")\nautoplot(decomposed) +\n  labs(title = \"Decomposition of Air Passengers Time Series\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Forecasting future values using auto.arima\nfit &lt;- auto.arima(AirPassengers)\nforecasts &lt;- forecast(fit, h = 24) # Forecast 2 years ahead\n\n# Plot the forecasts\nplot(forecasts, \n     main = \"Air Passengers Forecast (24 months)\",\n     xlab = \"Year\", \n     ylab = \"Passenger Count\")\n\n\n\n\n\n\n\n# Summary of the forecast model\nsummary(fit)\n\nSeries: AirPassengers \nARIMA(2,1,1)(0,1,0)[12] \n\nCoefficients:\n         ar1     ar2      ma1\n      0.5960  0.2143  -0.9819\ns.e.  0.0888  0.0880   0.0292\n\nsigma^2 = 132.3:  log likelihood = -504.92\nAIC=1017.85   AICc=1018.17   BIC=1029.35\n\nTraining set error measures:\n                 ME     RMSE     MAE      MPE     MAPE     MASE        ACF1\nTraining set 1.3423 10.84619 7.86754 0.420698 2.800458 0.245628 -0.00124847"
  },
  {
    "objectID": "posts/generating-correlated-random-numbers.html",
    "href": "posts/generating-correlated-random-numbers.html",
    "title": "Generating Correlated Random Numbers in R Using Matrix Methods",
    "section": "",
    "text": "The generation of random data with specified correlation patterns can be useful in statistical simulation and this tutorial provides a methodology for creating correlated random numbers in R. The techniques presented enable the development of realistic synthetic datasets with precisely controlled correlation structures, essential for robust statistical analysis and model validation."
  },
  {
    "objectID": "posts/generating-correlated-random-numbers.html#the-cholesky-method-in-four-steps",
    "href": "posts/generating-correlated-random-numbers.html#the-cholesky-method-in-four-steps",
    "title": "Generating Correlated Random Numbers in R from Scratch",
    "section": "The Cholesky Method in Four Steps",
    "text": "The Cholesky Method in Four Steps\n\n# 1. Define your target correlation matrix\ncor_mat &lt;- matrix(c(1, 0.3, \n                   0.3, 1), nrow = 2, byrow = TRUE)\n\n# 2. Apply Cholesky decomposition\nchol_mat &lt;- chol(cor_mat)\n\n# 3. Generate uncorrelated random numbers\nold_random &lt;- matrix(rnorm(2000), ncol = 2)\n\n# 4. Transform to create correlation\nnew_random &lt;- old_random %*% chol_mat\n\n# Verify the correlation\ncor(new_random)\n\n          [,1]      [,2]\n[1,] 1.0000000 0.2906056\n[2,] 0.2906056 1.0000000\n\n\nThat’s it! The new_random matrix now contains values with approximately your target correlation structure. This technique uses Cholesky decomposition to create a transformation matrix that induces the desired correlation when applied to uncorrelated data."
  },
  {
    "objectID": "posts/generating-correlated-random-numbers.html#watch-out-for-these-pitfalls",
    "href": "posts/generating-correlated-random-numbers.html#watch-out-for-these-pitfalls",
    "title": "Tutorial: Generating Correlated Random Numbers in R Using Matrix Methods",
    "section": "Watch Out for These Pitfalls",
    "text": "Watch Out for These Pitfalls\n\n1. Start with Truly Random Data\nYour input data must be uncorrelated for this method to work correctly:\n\n# What happens with already correlated input?\nsimulate_correlation &lt;- function(input_correlation, target = 0.3) {\n  results &lt;- replicate(1000, {\n    # Create input with specified correlation\n    x &lt;- rnorm(1000)\n    y &lt;- input_correlation * x + rnorm(1000, sd = sqrt(1 - input_correlation^2))\n    \n    # Apply our method\n    old_random &lt;- cbind(x, y)\n    chol_mat &lt;- chol(matrix(c(1, target, target, 1), ncol = 2))\n    new_random &lt;- old_random %*% chol_mat\n    \n    # Return resulting correlation\n    cor(new_random)[1,2]\n  })\n  return(results)\n}\n\n# Compare results with different input correlations\npar(mfrow = c(1, 2))\nhist(simulate_correlation(0.8), main = \"Starting with Correlated Data\",\n     xlim = c(0, 1), col = \"salmon\")\nhist(simulate_correlation(0.001), main = \"Starting with Random Data\",\n     xlim = c(0, 1), col = \"lightblue\")\n\n\n\n\n\n\n\n\nWhen your input data already has correlation patterns, the Cholesky method can’t properly override them to create your target correlation.\n\n\n2. Use the Same Distribution for All Variables\n\n# Different distributions cause problems\nset.seed(123)\nx1 &lt;- rchisq(1000, df = 3)  # Chi-squared (skewed)\ny1 &lt;- rnorm(1000)           # Normal (symmetric)\nold_mixed &lt;- cbind(x1, y1)\n\n# Same distribution works better\nx2 &lt;- rchisq(1000, df = 3)\ny2 &lt;- rchisq(1000, df = 3)\nold_same &lt;- cbind(x2, y2)\n\n# Apply the same transformation to both\nchol_mat &lt;- chol(matrix(c(1, 0.7, 0.7, 1), ncol = 2))\nnew_mixed &lt;- old_mixed %*% chol_mat\nnew_same &lt;- old_same %*% chol_mat\n\n# Compare results\ncat(\"Target correlation: 0.7\\n\")\n\nTarget correlation: 0.7\n\ncat(\"Mixed distributions result:\", round(cor(new_mixed)[1,2], 3), \"\\n\")\n\nMixed distributions result: 0.915 \n\ncat(\"Same distribution result:\", round(cor(new_same)[1,2], 3))\n\nSame distribution result: 0.699\n\n\nMixing different distributions (like normal and chi-squared) can lead to unexpected correlation patterns after transformation.\n\n\n3. Distribution Properties Can Change\n\n# Original positive-only distribution\nx &lt;- rchisq(1000, df = 3)  # Always positive\ny &lt;- rchisq(1000, df = 3)  # Always positive\nold_random &lt;- cbind(x, y)\n\n# Apply negative correlation\nchol_mat &lt;- chol(matrix(c(1, -0.7, -0.7, 1), ncol = 2))\nnew_random &lt;- old_random %*% chol_mat\n\n# Check what happened\ncat(\"Original data range:\", round(range(old_random), 2), \"\\n\")\n\nOriginal data range: 0.02 19.93 \n\ncat(\"Transformed data range:\", round(range(new_random), 2), \"\\n\")\n\nTransformed data range: -12.81 19.93 \n\ncat(\"Negative values in result:\", sum(new_random &lt; 0), \"out of\", length(new_random))\n\nNegative values in result: 488 out of 2000\n\n\nThe Cholesky transformation can fundamentally change your data’s properties - like introducing negative values into a previously positive-only distribution."
  },
  {
    "objectID": "posts/generating-correlated-random-numbers.html#the-easy-way-using-mvtnorm",
    "href": "posts/generating-correlated-random-numbers.html#the-easy-way-using-mvtnorm",
    "title": "Tutorial: Generating Correlated Random Numbers in R Using Matrix Methods",
    "section": "The Easy Way: Using mvtnorm",
    "text": "The Easy Way: Using mvtnorm\nFor most real applications, the mvtnorm package offers a simpler solution:\n\n# Load the package\nlibrary(mvtnorm)\n\n# Define means and covariance matrix\nmeans &lt;- c(10, 20)  # Mean for each variable\nsigma &lt;- matrix(c(4, 2,   # Covariance matrix\n                  2, 3), ncol = 2)\n\n# See the implied correlation\ncov2cor(sigma)\n\n          [,1]      [,2]\n[1,] 1.0000000 0.5773503\n[2,] 0.5773503 1.0000000\n\n# Generate correlated normal data in one step\nx &lt;- rmvnorm(n = 1000, mean = means, sigma = sigma)\n\n# Verify the result\nround(cor(x), 3)\n\n      [,1]  [,2]\n[1,] 1.000 0.613\n[2,] 0.613 1.000"
  },
  {
    "objectID": "posts/generating-correlated-random-numbers.html#when-to-use-each-method",
    "href": "posts/generating-correlated-random-numbers.html#when-to-use-each-method",
    "title": "Tutorial: Generating Correlated Random Numbers in R Using Matrix Methods",
    "section": "When to Use Each Method",
    "text": "When to Use Each Method\nUse the Cholesky method when: - You need to understand the mathematical principles - You’re working with non-normal distributions - You need to create custom correlation structures\nUse mvtnorm when: - You need multivariate normal data quickly - You want precise control over means and variances - You’re working with many variables"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html",
    "href": "posts/building-particle-swarm-optimizer.html",
    "title": "Implementing Particle Swarm Optimization from Scratch in R",
    "section": "",
    "text": "Nature-inspired optimization algorithms demonstrate remarkable efficiency in solving complex optimization problems. This post provides a implementation of Particle Swarm Optimization (PSO) from fundamental principles in R. The methodology presented enables efficient exploration of complex solution spaces through coordinated swarm intelligence."
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#required-libraries",
    "href": "posts/building-particle-swarm-optimizer.html#required-libraries",
    "title": "Building a Particle Swarm Optimizer from Scratch in R",
    "section": "Required Libraries",
    "text": "Required Libraries\n\n# Load required packages\nlibrary(dplyr)     # For data manipulation\nlibrary(ggplot2)   # For visualization\nlibrary(gganimate) # For animations\nlibrary(metR)      # For geom_arrow"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#the-challenge-a-complex-optimization-surface",
    "href": "posts/building-particle-swarm-optimizer.html#the-challenge-a-complex-optimization-surface",
    "title": "Building a Particle Swarm Optimizer from Scratch in R",
    "section": "The Challenge: A Complex Optimization Surface",
    "text": "The Challenge: A Complex Optimization Surface\nWe’ll test our optimizer on Ackley’s function – a challenging benchmark with many local minima that can trap optimization algorithms:\n\nobj_func &lt;- function(x, y){\n  # Modified Ackley function with global minimum at (1,1)\n  -20 * exp(-0.2 * sqrt(0.5 *((x-1)^2 + (y-1)^2))) - \n    exp(0.5*(cos(2*pi*x) + cos(2*pi*y))) + exp(1) + 20\n}\n\n# Create a visualization grid\nx &lt;- seq(-5, 5, length.out = 50)\ny &lt;- seq(-5, 5, length.out = 50)\ngrid &lt;- expand.grid(x, y, stringsAsFactors = FALSE)\ngrid$z &lt;- obj_func(grid[,1], grid[,2])\n\n# Create a contour plot\ncontour_plot &lt;- ggplot(grid, aes(x = Var1, y = Var2)) +\n  geom_contour_filled(aes(z = z), color = \"black\", alpha = 0.5) +\n  scale_fill_brewer(palette = \"Spectral\") + \n  theme_minimal() + \n  labs(x = \"x\", y = \"y\", title = \"Ackley's Function\")\n\ncontour_plot"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#how-pso-works",
    "href": "posts/building-particle-swarm-optimizer.html#how-pso-works",
    "title": "Building a Particle Swarm Optimizer from Scratch in R",
    "section": "How PSO Works",
    "text": "How PSO Works\nPSO mimics how birds find food by combining individual memory with social information:\n\nScatter random “particles” across the search space\nEach particle remembers its personal best position\nThe swarm shares information about the global best position\nParticles adjust their movement based on both personal and swarm knowledge\n\nThe movement equation balances three forces:\n\\[v_{new} = w \\cdot v_{current} + c_1 \\cdot r_1 \\cdot (p_{best} - p_{current}) + c_2 \\cdot r_2 \\cdot (g_{best} - p_{current})\\]\nWhere: - w: Inertia weight (momentum) - c1: Personal influence (memory) - c2: Social influence (cooperation) - r1,r2: Random values adding exploration"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#building-pso-step-by-step",
    "href": "posts/building-particle-swarm-optimizer.html#building-pso-step-by-step",
    "title": "Building a Particle Swarm Optimizer from Scratch in R",
    "section": "Building PSO Step by Step",
    "text": "Building PSO Step by Step\n\nStep 1: Initialize the Swarm\nFirst, we create a random swarm of particles and place them across our search space:\n\n# Set parameters\nn_particles &lt;- 20\nw &lt;- 0.5     # Inertia weight\nc1 &lt;- 0.05   # Personal learning rate\nc2 &lt;- 0.1    # Social learning rate\n\n# Create random particle positions\nx_range &lt;- seq(-5, 5, length.out = 20)\ny_range &lt;- seq(-5, 5, length.out = 20)\nX &lt;- data.frame(\n  x = sample(x_range, n_particles, replace = FALSE),\n  y = sample(y_range, n_particles, replace = FALSE)\n)\n\n# Visualize initial positions\ncontour_plot + \n  geom_point(data = X, aes(x, y), color = \"red\", size = 2.5) + \n  labs(title = \"Initial Particle Positions\")\n\n\n\n\n\n\n\n\n\n\nStep 2: Track Best Positions and Initialize Velocities\nNext, we track each particle’s personal best position and the swarm’s global best position:\n\n# Initialize random velocities\ndX &lt;- matrix(runif(n_particles * 2), ncol = 2) * w\n\n# Set initial personal best positions\npbest &lt;- X\npbest_obj &lt;- obj_func(X[,1], X[,2])\n\n# Find global best position\ngbest &lt;- pbest[which.min(pbest_obj),]\ngbest_obj &lt;- min(pbest_obj)\n\n# Visualize with arrows showing pull toward global best\nX_dir &lt;- X %&gt;% \n  mutate(g_x = gbest[1,1], \n         g_y = gbest[1,2], \n         angle = atan((g_y - y)/(g_x - x))*180/pi,\n         angle = ifelse(g_x &lt; x, 180 + angle, angle))\n\ncontour_plot + \n  geom_point(data = X, aes(x, y), color = \"red\", size = 2.5) + \n  geom_segment(data = X_dir, \n               aes(x = x, y = y, \n                   xend = x + 0.5*cos(angle*pi/180), \n                   yend = y + 0.5*sin(angle*pi/180)), \n               arrow = arrow(length = unit(0.1, \"cm\")), \n               color = \"blue\") + \n  labs(title = \"Forces Acting on Particles\")\n\n\n\n\n\n\n\n\n\n\nStep 3: Update Particle Positions\nNow we update each particle’s position based on its velocity and the forces acting on it:\n\n# Calculate new velocities using PSO equation\ndX &lt;- w * dX + \n      c1*runif(1)*(pbest - X) + \n      c2*runif(1)*(as.matrix(gbest) - X)\n\n# Update positions\nX &lt;- X + dX\n\n# Evaluate function at new positions\nobj &lt;- obj_func(X[,1], X[,2])\n\n# Update personal best positions if improved\nidx &lt;- which(obj &lt;= pbest_obj)\npbest[idx,] &lt;- X[idx,]\npbest_obj[idx] &lt;- obj[idx]\n\n# Update global best position\nidx &lt;- which.min(pbest_obj)\ngbest &lt;- pbest[idx,]\ngbest_obj &lt;- min(pbest_obj)\n\n# Visualize updated positions\nX_dir &lt;- X %&gt;% \n  mutate(g_x = gbest[1,1], \n         g_y = gbest[1,2], \n         angle = atan((g_y - y)/(g_x - x))*180/pi,\n         angle = ifelse(g_x &lt; x, 180 + angle, angle))\n\ncontour_plot + \n  geom_point(data = X, aes(x, y), color = \"red\", size = 2.5) + \n  geom_segment(data = X_dir, \n               aes(x = x, y = y, \n                   xend = x + 0.5*cos(angle*pi/180), \n                   yend = y + 0.5*sin(angle*pi/180)), \n               arrow = arrow(length = unit(0.1, \"cm\")), \n               color = \"blue\") + \n  labs(title = \"Particles After First Update\")"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#complete-pso-implementation",
    "href": "posts/building-particle-swarm-optimizer.html#complete-pso-implementation",
    "title": "Building a Particle Swarm Optimizer from Scratch in R",
    "section": "Complete PSO Implementation",
    "text": "Complete PSO Implementation\nNow let’s package everything into a reusable function:\n\npso_optim &lt;- function(obj_func,      # Function to minimize\n                      c1 = 0.05,      # Personal learning rate\n                      c2 = 0.05,      # Social learning rate\n                      w = 0.8,        # Inertia weight\n                      n_particles = 20,  # Swarm size\n                      init_fact = 0.1,   # Initial velocity factor\n                      n_iter = 50        # Maximum iterations\n){\n  # Define search domain\n  x &lt;- seq(-5, 5, length.out = 100)\n  y &lt;- seq(-5, 5, length.out = 100)\n  \n  # Initialize particles\n  X &lt;- cbind(sample(x, n_particles, replace = FALSE),\n             sample(y, n_particles, replace = FALSE))\n  dX &lt;- matrix(runif(n_particles * 2) * init_fact, ncol = 2)\n  \n  # Initialize best positions\n  pbest &lt;- X\n  pbest_obj &lt;- obj_func(x = X[,1], y = X[,2])\n  gbest &lt;- pbest[which.min(pbest_obj),]\n  gbest_obj &lt;- min(pbest_obj)\n  \n  # Store positions for visualization\n  loc_df &lt;- data.frame(X, iter = 0)\n  iter &lt;- 1\n  \n  # Main optimization loop\n  while(iter &lt; n_iter){\n    # Update velocities\n    dX &lt;- w * dX + \n          c1*runif(1)*(pbest - X) + \n          c2*runif(1)*t(gbest - t(X))\n    \n    # Update positions\n    X &lt;- X + dX\n    \n    # Evaluate and update best positions\n    obj &lt;- obj_func(x = X[,1], y = X[,2])\n    idx &lt;- which(obj &lt;= pbest_obj)\n    pbest[idx,] &lt;- X[idx,]\n    pbest_obj[idx] &lt;- obj[idx]\n    \n    # Update global best\n    idx &lt;- which.min(pbest_obj)\n    gbest &lt;- pbest[idx,]\n    gbest_obj &lt;- min(pbest_obj)\n    \n    # Store for visualization\n    iter &lt;- iter + 1\n    loc_df &lt;- rbind(loc_df, data.frame(X, iter = iter))\n  }\n  \n  return(list(X = loc_df, \n              obj = gbest_obj, \n              obj_loc = paste0(gbest, collapse = \",\")))\n}\n\nLet’s test our optimizer on the Ackley function:\n\n# Run the PSO algorithm\nout &lt;- pso_optim(obj_func,\n                 c1 = 0.01,    # Low personal influence\n                 c2 = 0.05,    # Moderate social influence\n                 w = 0.5,      # Medium inertia\n                 n_particles = 50,\n                 init_fact = 0.1,\n                 n_iter = 200)\n\n# Check the result (global minimum should be at (1,1))\nout$obj_loc\n\n[1] \"0.999973311243923,1.00001654547655\""
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#visualizing-the-swarm-in-action",
    "href": "posts/building-particle-swarm-optimizer.html#visualizing-the-swarm-in-action",
    "title": "Building a Particle Swarm Optimizer from Scratch in R",
    "section": "Visualizing the Swarm in Action",
    "text": "Visualizing the Swarm in Action\nThe real beauty of PSO is watching the particles converge on the solution:\n\n# Create animation of the optimization process\nggplot(out$X) +\n  geom_contour(data = grid, aes(x = Var1, y = Var2, z = z), color = \"black\") +\n  geom_point(aes(X1, X2)) +\n  labs(x = \"X\", y = \"Y\") +\n  transition_time(iter) +\n  ease_aes(\"linear\")"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#fine-tuning-your-swarm",
    "href": "posts/building-particle-swarm-optimizer.html#fine-tuning-your-swarm",
    "title": "Building a Particle Swarm Optimizer from Scratch in R",
    "section": "Fine-Tuning Your Swarm",
    "text": "Fine-Tuning Your Swarm\nThe PSO algorithm’s behavior can be dramatically altered by adjusting three key parameters:\n\nInertia Weight (w)\n\nHigh values (&gt;0.8): Particles maintain momentum and explore widely\nLow values (&lt;0.4): Particles slow down and focus on refining solutions\n\nPersonal Learning Rate (c1)\n\nHigh values: Particles favor their own discoveries\nLow values: Particles ignore their history\n\nSocial Learning Rate (c2)\n\nHigh values: Particles rush toward the global best\nLow values: Particles explore independently\n\n\nCommon parameter combinations: - Exploration focus: High w (0.9), balanced c1/c2 (0.5/0.5) - Exploitation focus: Low w (0.4), higher c2 than c1 (0.1/0.7)"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#enhancing-your-pso-implementation",
    "href": "posts/building-particle-swarm-optimizer.html#enhancing-your-pso-implementation",
    "title": "Tutorial: Implementing Particle Swarm Optimization from Scratch in R",
    "section": "Enhancing Your PSO Implementation",
    "text": "Enhancing Your PSO Implementation\nFor real-world applications, consider these improvements:\n\nAdd boundary constraints to keep particles within valid regions\nImplement adaptive parameters that change during optimization\nAdd convergence-based stopping criteria\nExtend to higher dimensions for more complex problems\n\nThe R package pso offers a production-ready implementation!"
  },
  {
    "objectID": "posts/assessing-score-reliability.html",
    "href": "posts/assessing-score-reliability.html",
    "title": "Tutorial: Assessing Credit Score Prediction Reliability Using Bootstrap Resampling",
    "section": "",
    "text": "Credit scoring models demonstrate optimal performance within the central regions of the score distribution, yet exhibit diminished reliability at the distribution extremes where data becomes sparse. This tutorial provides a comprehensive methodology for employing bootstrap resampling techniques to quantify prediction variability across different score ranges, thereby enabling practitioners to identify regions where their models demonstrate the highest degree of dependability."
  },
  {
    "objectID": "posts/assessing-score-reliability.html#why-estimation-variance-matters",
    "href": "posts/assessing-score-reliability.html#why-estimation-variance-matters",
    "title": "Understanding Variability in Credit Score Predictions",
    "section": "Why Estimation Variance Matters",
    "text": "Why Estimation Variance Matters\nSmaller sample sizes lead to higher variance in estimates, especially for extreme values. While statistics like means remain stable with limited data, tail percentiles (95th, 99th) show much more volatility. This matters for credit scoring, where very high and very low scores represent these unstable tail regions.\n\n# Number of samples to be drawn from a probability distribution\nn_samples &lt;- 1000\n\n# Number of times, sampling should be repeated\nrepeats &lt;- 100\n\n# Mean and std-dev for a standard normal distribution\nmu &lt;- 5\nstd_dev &lt;- 2\n\n# Sample\nsamples &lt;- rnorm(n_samples * repeats, mean = 10)\n\n# Fit into a matrix like object with `n_samples' number of rows \n# and `repeats` number of columns\nsamples &lt;- matrix(samples, nrow = n_samples, ncol = repeats)\n\n# Compute mean across each column\nsample_means &lt;- apply(samples, 1, mean)\n\n# Similarly, compute 75% and 95% quantile across each column\nsample_75_quantile &lt;- apply(samples, 1, quantile, p = 0.75)\nsample_95_quantile &lt;- apply(samples, 1, quantile, p = 0.95)\nsample_99_quantile &lt;- apply(samples, 1, quantile, p = 0.99)\n\n# Compare coefficient of variation\nsd(sample_means)/mean(sample_means)\n\n[1] 0.01017306\n\nsd(sample_75_quantile)/mean(sample_75_quantile)\n\n[1] 0.0127586\n\nsd(sample_95_quantile)/mean(sample_75_quantile)\n\n[1] 0.01873297\n\n# Plot the distributions\ncombined_vec &lt;- c(sample_means, sample_75_quantile, sample_95_quantile, sample_99_quantile)\n\nplot(density(sample_means), \n     col = \"#6F69AC\", \n     lwd = 3, \n     main = \"Estimating the mean vs tail quantiles\", \n     xlab = \"\", \n     xlim = c(min(combined_vec), max(combined_vec)))\n\nlines(density(sample_75_quantile), col = \"#95DAC1\", lwd = 3)\nlines(density(sample_95_quantile), col = \"#FFEBA1\", lwd = 3)\nlines(density(sample_99_quantile), col = \"#FD6F96\", lwd = 3)\ngrid()\n\nlegend(\"topright\", \n       fill = c(\"#6F69AC\", \"#95DAC1\", \"#FFEBA1\", \"#FD6F96\"), \n       legend = c(\"Mean\", \"75% Quantile\", \"95% Quantile\", \"99% Quantile\"), \n       cex = 0.7)\n\n\n\n\n\n\n\n\nThe plot shows how uncertainty increases dramatically when estimating extreme values. The distribution for the mean (purple) is much narrower than for the 99th percentile (pink). This directly translates to credit scoring – where very high or low scores have greater uncertainty.\n\n# Load required packages\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(rsample)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/assessing-score-reliability.html#data-acquisition-and-preprocessing",
    "href": "posts/assessing-score-reliability.html#data-acquisition-and-preprocessing",
    "title": "Tutorial: Assessing Credit Score Prediction Reliability Using Bootstrap Resampling",
    "section": "Data Acquisition and Preprocessing",
    "text": "Data Acquisition and Preprocessing\nIn this tutorial, we will utilize a sample from the Lending Club dataset. Loans classified as “Charged Off” will be designated as defaults. The observed class imbalance represents a typical characteristic of credit portfolios and contributes significantly to prediction challenges at distribution extremes.\n\n# Load sample data (sample of the lending club data)\nsample &lt;- read.csv(\"http://bit.ly/42ypcnJ\")\n\n# Mark which loan status will be tagged as default\ncodes &lt;- c(\"Charged Off\", \"Does not meet the credit policy. Status:Charged Off\")\n\n# Apply above codes and create target\nsample %&lt;&gt;% mutate(bad_flag = ifelse(loan_status %in% codes, 1, 0))\n\n# Replace missing values with a default value\nsample[is.na(sample)] &lt;- -1\n\n# Get summary tally\ntable(sample$bad_flag)\n\n\n   0    1 \n8838 1162"
  },
  {
    "objectID": "posts/assessing-score-reliability.html#implementing-bootstrap-resampling-strategy",
    "href": "posts/assessing-score-reliability.html#implementing-bootstrap-resampling-strategy",
    "title": "Tutorial: Assessing Credit Score Prediction Reliability Using Bootstrap Resampling",
    "section": "Implementing Bootstrap Resampling Strategy",
    "text": "Implementing Bootstrap Resampling Strategy\nThis section demonstrates the creation of 100 bootstrap samples to quantify the variation in model predictions across different score ranges. Bootstrap resampling is a statistical technique that generates multiple simulated datasets to measure prediction uncertainty without requiring additional data collection.\n\n# Create 100 samples\nboot_sample &lt;- bootstraps(data = sample, times = 100)\n\nhead(boot_sample, 3)\n\n# A tibble: 3 × 2\n  splits               id          \n  &lt;list&gt;               &lt;chr&gt;       \n1 &lt;split [10000/3723]&gt; Bootstrap001\n2 &lt;split [10000/3693]&gt; Bootstrap002\n3 &lt;split [10000/3643]&gt; Bootstrap003\n\n# Each row represents a separate bootstrapped sample with an analysis set and assessment set\nboot_sample$splits[[1]]\n\n&lt;Analysis/Assess/Total&gt;\n&lt;10000/3723/10000&gt;\n\n# Show the first 5 rows and 5 columns of the first sample\nanalysis(boot_sample$splits[[1]]) %&gt;% .[1:5, 1:5]\n\n     V1        id member_id loan_amnt funded_amnt\n1 17422 122984190        -1      8000        8000\n2 54472  61792204        -1     35000       35000\n3  5450   5935335        -1     32350       32350\n4 98045 136054338        -1      8000        8000\n5 42849   3355024        -1     18075       18075\n\n\nEach bootstrap sample consists of random draws with replacement from the original dataset, creating controlled variations that effectively reveal model sensitivity to different data compositions."
  },
  {
    "objectID": "posts/assessing-score-reliability.html#developing-the-predictive-model-framework",
    "href": "posts/assessing-score-reliability.html#developing-the-predictive-model-framework",
    "title": "Tutorial: Assessing Credit Score Prediction Reliability Using Bootstrap Resampling",
    "section": "Developing the Predictive Model Framework",
    "text": "Developing the Predictive Model Framework\nThis tutorial employs logistic regression as the predictive modeling technique. Logistic regression represents the industry standard for credit risk modeling due to its interpretability and regulatory acceptance. The model specification incorporates typical credit variables including loan amount, income, and credit history metrics.\n\nglm_model &lt;- function(df){\n  \n  # Fit a simple model with a set specification\n  mdl &lt;- glm(bad_flag ~\n               loan_amnt + funded_amnt + annual_inc + delinq_2yrs +\n               inq_last_6mths + mths_since_last_delinq + fico_range_low +\n               mths_since_last_record + revol_util + total_pymnt,\n             family = \"binomial\",\n             data = df)\n  \n  # Return fitted values\n  return(predict(mdl))\n}\n\n# Test the function\n# Retrieve a data frame\ntrain &lt;- analysis(boot_sample$splits[[1]])\n\n# Predict\npred &lt;- glm_model(train)\n\n# Check output\nrange(pred)  # Output is on log odds scale\n\n[1] -13.501500   1.536822\n\n\nThe function returns predictions in log-odds format, which will subsequently be transformed to a more intuitive credit score scale in later steps."
  },
  {
    "objectID": "posts/assessing-score-reliability.html#iterative-model-training-and-prediction-collection",
    "href": "posts/assessing-score-reliability.html#iterative-model-training-and-prediction-collection",
    "title": "Tutorial: Assessing Credit Score Prediction Reliability Using Bootstrap Resampling",
    "section": "Iterative Model Training and Prediction Collection",
    "text": "Iterative Model Training and Prediction Collection\n\n# First apply the glm fitting function to each of the sample\n# Note the use of lapply\noutput &lt;- lapply(boot_sample$splits, function(x){\n  train &lt;- analysis(x)\n  pred &lt;- glm_model(train)\n\n  return(pred)\n})\n\n# Collate all predictions into a vector \nboot_preds &lt;- do.call(c, output)\nrange(boot_preds)\n\n[1] -136.947338    4.484198\n\n# Get outliers\nq_high &lt;- quantile(boot_preds, 0.99)\nq_low &lt;- quantile(boot_preds, 0.01)\n\n# Truncate the overall distribution to within the lower 1% and upper 1% quantiles\n# Doing this since it creates issues later on when scaling the output\nboot_preds[boot_preds &gt; q_high] &lt;- q_high\nboot_preds[boot_preds &lt; q_low] &lt;- q_low\n\nrange(boot_preds)\n\n[1] -5.0400444 -0.2234081\n\n# Convert to a data frame\nboot_preds &lt;- data.frame(pred = boot_preds, \n                         id = rep(1:length(boot_sample$splits), each = nrow(sample)))\nhead(boot_preds)\n\n       pred id\n1 -1.906047  1\n2 -2.981736  1\n3 -2.291323  1\n4 -2.550728  1\n5 -1.648253  1\n6 -4.052317  1\n\n\nIn this step, we apply the logistic regression model to each bootstrap sample and systematically collect the resulting predictions. Subsequently, we truncate extreme values (beyond the 1st and 99th percentiles) to remove outliers—a procedure analogous to capping techniques commonly employed in production credit models."
  },
  {
    "objectID": "posts/assessing-score-reliability.html#transforming-predictions-to-credit-score-scale",
    "href": "posts/assessing-score-reliability.html#transforming-predictions-to-credit-score-scale",
    "title": "Tutorial: Assessing Credit Score Prediction Reliability Using Bootstrap Resampling",
    "section": "Transforming Predictions to Credit Score Scale",
    "text": "Transforming Predictions to Credit Score Scale\nThis section demonstrates the conversion of log-odds predictions to a recognizable credit score format using the industry-standard Points to Double Odds (PDO) methodology. By employing parameters consistent with real-world credit systems (PDO=30, Anchor=700), we transform the model predictions into intuitive scores where higher numerical values indicate lower credit risk.\n\nscaling_func &lt;- function(vec, PDO = 30, OddsAtAnchor = 5, Anchor = 700){\n  beta &lt;- PDO / log(2)\n  alpha &lt;- Anchor - PDO * OddsAtAnchor\n  \n  # Simple linear scaling of the log odds\n  scr &lt;- alpha - beta * vec  \n  \n  # Round off\n  return(round(scr, 0))\n}\n\nboot_preds$scores &lt;- scaling_func(boot_preds$pred, 30, 2, 700)\n\n# Chart the distribution of predictions across all the samples\nggplot(boot_preds, aes(x = scores, color = factor(id))) + \n  geom_density() + \n  theme_minimal() + \n  theme(legend.position = \"none\") + \n  scale_color_grey() + \n  labs(title = \"Predictions from bootstrapped samples\", \n       subtitle = \"Density function\", \n       x = \"Predictions (Log odds)\", \n       y = \"Density\")"
  },
  {
    "objectID": "posts/assessing-score-reliability.html#quantifying-prediction-uncertainty-across-score-ranges",
    "href": "posts/assessing-score-reliability.html#quantifying-prediction-uncertainty-across-score-ranges",
    "title": "Tutorial: Assessing Credit Score Prediction Reliability Using Bootstrap Resampling",
    "section": "Quantifying Prediction Uncertainty Across Score Ranges",
    "text": "Quantifying Prediction Uncertainty Across Score Ranges\nThis section provides a methodology to directly measure prediction reliability variation across different score ranges through the calculation of standard deviation within each score bin. This approach enables precise quantification of uncertainty at different score levels.\n\n# Create bins using quantiles\nbreaks &lt;- quantile(boot_preds$scores, probs = seq(0, 1, length.out = 20))\nboot_preds$bins &lt;- cut(boot_preds$scores, breaks = unique(breaks), include.lowest = T, right = T)\n\n# Chart standard deviation of model predictions across each score bin\nboot_preds %&gt;%\n  group_by(bins) %&gt;%\n  summarise(std_dev = sd(scores)) %&gt;%\n  ggplot(aes(x = bins, y = std_dev)) +\n  geom_col(color = \"black\", fill = \"#90AACB\") +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 90)) + \n  theme(legend.position = \"none\") + \n  labs(title = \"Variability in model predictions across samples\", \n       subtitle = \"(measured using standard deviation)\", \n       x = \"Score Range\", \n       y = \"Standard Deviation\")\n\n\n\n\n\n\n\n\nAs anticipated, the model’s predictions demonstrate enhanced reliability within a specific range of values (700-800), while exhibiting significant variability in the lowest and highest score buckets.\nThe visualization reveals a characteristic “U-shaped” pattern of prediction variability—a well-documented phenomenon in credit risk modeling. The highest uncertainty manifests in the extreme score ranges (very high and very low scores), while predictions in the middle range demonstrate greater stability. The analysis confirms the initial hypothesis: variability reaches its maximum at score extremes and achieves its minimum in the middle range (600-800). This finding provides direct guidance for credit policy development—scores in the middle range demonstrate the highest reliability, while decisions at the extremes should incorporate additional caution due to elevated uncertainty.\n\nPractical Business Applications\nThese findings yield direct business applications:\n\nHigh Score Management: For extremely high scores, implement additional verification steps before automated approval\nLow Score Management: For very low scores, consider manual review procedures rather than automatic rejection"
  },
  {
    "objectID": "posts/assessing-score-reliability.html#advanced-approach-isolating-training-data-effects",
    "href": "posts/assessing-score-reliability.html#advanced-approach-isolating-training-data-effects",
    "title": "Understanding Variability in Credit Score Predictions",
    "section": "Advanced Approach: Isolating Training Data Effects",
    "text": "Advanced Approach: Isolating Training Data Effects\nCredit: Richard Warnung\nFor a more controlled analysis, we can train models on bootstrap samples but evaluate them on the same validation set. This isolates the impact of training data variation:\n\nVs &lt;- function(boot_split){\n  # Train model on the bootstrapped data\n  train &lt;- analysis(boot_split)\n  \n  # Fit model\n  mdl &lt;- glm(bad_flag ~\n               loan_amnt + funded_amnt + annual_inc + delinq_2yrs +\n               inq_last_6mths + mths_since_last_delinq + fico_range_low +\n               mths_since_last_record + revol_util + total_pymnt,\n             family = \"binomial\",\n             data = train)\n  \n  # Apply to a common validation set\n  validate_preds &lt;- predict(mdl, newdata = validate_set)\n  \n  # Return predictions\n  return(validate_preds)\n}\n\nThis method provides a clearer picture of how variations in training data affect model predictions, which is valuable when evaluating model updates in production."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R'tichoke",
    "section": "",
    "text": "R’tichoke is a repository of R programming content, tutorials, and resources. Whether you’re just starting your journey with R or you’re an experienced data scientist looking to expand your toolkit, you’ll find valuable content here to help you grow your skills."
  },
  {
    "objectID": "index.html#welcome-to-rtichoke",
    "href": "index.html#welcome-to-rtichoke",
    "title": "R'tichoke",
    "section": "",
    "text": "R’tichoke is a repository of R programming content, tutorials, and resources. Whether you’re just starting your journey with R or you’re an experienced data scientist looking to expand your toolkit, you’ll find valuable content here to help you grow your skills."
  },
  {
    "objectID": "index.html#latest-articles",
    "href": "index.html#latest-articles",
    "title": "R'tichoke",
    "section": "Latest Articles",
    "text": "Latest Articles\nCheck out the blog for tutorials and tips on R programming."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "R'tichoke",
    "section": "Getting Started",
    "text": "Getting Started\nNew to R? Start here:\n\nInstalling R and RStudio - Set up your development environment\nIntroduction to Analytics with R - Learn the basics\nData Wrangling with dplyr - Level up your data manipulation skills\nInteractive R in Your Browser with WebR - Try out R right in your browser!"
  },
  {
    "objectID": "get-started/reading-data.html",
    "href": "get-started/reading-data.html",
    "title": "Reading Data into R",
    "section": "",
    "text": "R provides built-in functions for importing data from various file formats. Here are the most common ones:"
  },
  {
    "objectID": "get-started/reading-data.html#reading-data-into-r-using-base-functions",
    "href": "get-started/reading-data.html#reading-data-into-r-using-base-functions",
    "title": "Reading Data into R",
    "section": "",
    "text": "R provides built-in functions for importing data from various file formats. Here are the most common ones:\n\n\nComma-separated values (CSV) files are one of the most common data formats:\n\n# Create a sample CSV file\nwrite.csv(mtcars[1:5, ], \"sample_cars.csv\", row.names = TRUE)\n\n# Read the CSV file\ncars_data &lt;- read.csv(\"sample_cars.csv\")\nhead(cars_data)\n\n                  X  mpg cyl disp  hp drat    wt  qsec vs am gear carb\n1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n# Read with specific options\ncars_data2 &lt;- read.csv(\"sample_cars.csv\", \n                      header = TRUE,       # First row contains column names\n                      sep = \",\",           # Separator is a comma\n                      stringsAsFactors = FALSE, # Don't convert strings to factors\n                      na.strings = c(\"NA\", \"N/A\", \"\")) # Values to treat as NA\nhead(cars_data2)\n\n                  X  mpg cyl disp  hp drat    wt  qsec vs am gear carb\n1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n\n\n\n\nTab-delimited files are another common format:\n\n# Create a sample tab-delimited file\nwrite.table(mtcars[1:5, ], \"sample_cars.txt\", sep = \"\\t\", row.names = TRUE)\n\n# Read the tab-delimited file\ncars_data_tab &lt;- read.delim(\"sample_cars.txt\")\nhead(cars_data_tab)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n# Or use read.table with tab separator\ncars_data_tab2 &lt;- read.table(\"sample_cars.txt\", \n                            header = TRUE, \n                            sep = \"\\t\")\nhead(cars_data_tab2)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n\n\n\n\nFixed-width files have fields of consistent width:\n\n# Create a sample fixed-width file\ncat(\"John  Smith 35\\nMary  Jones 28\\nDavid Brown 42\\n\", file = \"sample_people.txt\")\n\n# Read the fixed-width file\npeople_data &lt;- read.fwf(\"sample_people.txt\", \n                       widths = c(5, 6, 3),  # Width of each column\n                       col.names = c(\"First\", \"Last\", \"Age\"))\npeople_data\n\n  First   Last Age\n1 John   Smith  35\n2 Mary   Jones  28\n3 David  Brown  42\n\n\n\n\n\nR has its own binary file format for saving and loading R objects:\n\n# Save R objects to a file\nsample_data &lt;- list(x = 1:10, y = letters[1:10])\nsave(sample_data, file = \"sample_data.RData\")\n\n# Load the saved objects\nload(\"sample_data.RData\")\nsample_data\n\n$x\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$y\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\n\n# Save a single object\nsaveRDS(mtcars[1:5, ], \"sample_cars.rds\")\n\n# Read the saved object\ncars_subset &lt;- readRDS(\"sample_cars.rds\")\nhead(cars_subset)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n\n\n\n\nYou can read data directly from the web:\n\n# Read CSV from a URL (example with a small dataset)\nurl &lt;- \"https://raw.githubusercontent.com/datasets/iris/master/data/iris.csv\"\niris_data &lt;- try(read.csv(url), silent = TRUE)\n\n# Check if the data was loaded successfully\nif (!inherits(iris_data, \"try-error\")) {\n  head(iris_data)\n} else {\n  print(\"Could not access the URL. Check your internet connection.\")\n}\n\n[1] \"Could not access the URL. Check your internet connection.\"\n\n\n\n\n\nWhile not part of base R, the readxl package is commonly used:\n\n# Check if readxl is installed\nif (!requireNamespace(\"readxl\", quietly = TRUE)) {\n  message(\"The readxl package is not installed. You can install it with: install.packages('readxl')\")\n} else {\n  library(readxl)\n  # This would read an Excel file if it existed\n  # excel_data &lt;- read_excel(\"sample.xlsx\", sheet = 1)\n}\n\n\n\n\nBase R provides the DBI package for database connections:\n\n# Example of connecting to SQLite (not run)\n# if (!requireNamespace(\"RSQLite\", quietly = TRUE)) {\n#   message(\"The RSQLite package is not installed\")\n# } else {\n#   library(DBI)\n#   con &lt;- dbConnect(RSQLite::SQLite(), \":memory:\")\n#   dbWriteTable(con, \"mtcars\", mtcars)\n#   data &lt;- dbGetQuery(con, \"SELECT * FROM mtcars WHERE cyl = 4\")\n#   dbDisconnect(con)\n# }\n\n\n\n\nR provides functions to work with file paths:\n\n# Get current working directory\ngetwd()\n\n[1] \"C:/Users/riddh/OneDrive/Desktop/rtichoke-github/get-started\"\n\n# List files in the current directory\nlist.files(pattern = \".csv\")\n\n[1] \"sample_cars.csv\"\n\n# Check if a file exists\nfile.exists(\"sample_cars.csv\")\n\n[1] TRUE\n\n# Get full path to a file\nnormalizePath(\"sample_cars.csv\", mustWork = FALSE)\n\n[1] \"C:\\\\Users\\\\riddh\\\\OneDrive\\\\Desktop\\\\rtichoke-github\\\\get-started\\\\sample_cars.csv\"\n\n\n\n\n\nLet’s remove the sample files we created:\n\n# List of files to remove\nfiles_to_remove &lt;- c(\"sample_cars.csv\", \"sample_cars.txt\", \n                    \"sample_people.txt\", \"sample_data.RData\", \n                    \"sample_cars.rds\")\n\n# Remove files\nfor (file in files_to_remove) {\n  if (file.exists(file)) {\n    file.remove(file)\n  }\n}\n\nRemember to check the documentation with ?read.csv or similar commands to explore all available options for these functions."
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html",
    "href": "get-started/parallel-computing-in-r.html",
    "title": "Parallel Computing in R",
    "section": "",
    "text": "# Install packages if needed (uncomment to run)\n# install.packages(c(\"parallel\", \"foreach\", \"doParallel\", \"tictoc\"))\n\n# Load the essential packages\nlibrary(parallel)    # Base R parallel functions\nlibrary(foreach)     # For parallel loops\nlibrary(doParallel)  # Backend for foreach\nlibrary(tictoc)      # For timing comparisons"
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#what-is-parallel-computing-and-why-use-it",
    "href": "get-started/parallel-computing-in-r.html#what-is-parallel-computing-and-why-use-it",
    "title": "Parallel Computing in R",
    "section": "",
    "text": "When R scripts take hours to complete, parallel computing can provide significant performance improvements. Instead of running calculations one after another (sequentially), parallel computing enables multiple calculations to run simultaneously by utilizing all available CPU cores on the computer.\nBenefits: - Speed: Execute code significantly faster - Efficiency: Optimize computer resource utilization\n- Scalability: Handle larger datasets and more complex models"
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#quick-setup-required-packages",
    "href": "get-started/parallel-computing-in-r.html#quick-setup-required-packages",
    "title": "Parallel Computing in R",
    "section": "Quick Setup: Required Packages",
    "text": "Quick Setup: Required Packages\nThe analysis begins by installing and loading the necessary packages:\n\n# Install packages if needed (uncomment to run)\n# install.packages(c(\"parallel\", \"foreach\", \"doParallel\", \"tictoc\"))\n\n# Load the essential packages\nlibrary(parallel)    # Base R parallel functions\nlibrary(foreach)     # For parallel loops\nlibrary(doParallel)  # Backend for foreach\nlibrary(tictoc)      # For timing comparisons"
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#how-many-cores-do-you-have",
    "href": "get-started/parallel-computing-in-r.html#how-many-cores-do-you-have",
    "title": "Parallel Computing in R",
    "section": "",
    "text": "The first step is to check how many CPU cores are available on your computer:\n\n# Detect the number of CPU cores\ndetectCores()\n\n[1] 16\n\n\nIt’s usually good practice to leave one core free for your operating system, so we’ll typically use detectCores() - 1 for our parallel operations."
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#your-first-parallel-code-the-basics",
    "href": "get-started/parallel-computing-in-r.html#your-first-parallel-code-the-basics",
    "title": "Parallel Computing in R",
    "section": "",
    "text": "Let’s create a simple function that takes some time to run, then compare how long it takes to run sequentially versus in parallel:\n\n# A function that takes time to execute\nslow_function &lt;- function(x) {\n  Sys.sleep(0.5)  # Simulate computation time (half a second)\n  return(x^2)     # Return the square of x\n}\n\n# Create a list of numbers to process\nnumbers &lt;- 1:10\n\n\n\nThis method works on all operating systems including Windows:\n\n# Step 1: Create a cluster of workers\ncl &lt;- makeCluster(detectCores() - 1)\n\n# Step 2: Export any functions our workers need\nclusterExport(cl, \"slow_function\")\n\n# Run the sequential version and time it\ntic(\"Sequential version\")\nresult_sequential &lt;- lapply(numbers, slow_function)\ntoc()\n\nSequential version: 5.06 sec elapsed\n\n# Run the parallel version and time it\ntic(\"Parallel version\")\nresult_parallel &lt;- parLapply(cl, numbers, slow_function)\ntoc()\n\nParallel version: 0.53 sec elapsed\n\n# Step 3: Always stop the cluster when done!\nstopCluster(cl)\n\n# Verify both methods give the same results\nall.equal(result_sequential, result_parallel)\n\n[1] TRUE\n\n\n\n\n\nIf you’re on Mac or Linux, you can use this simpler approach:\n\n# For Mac/Linux users only\ntic(\"Parallel mclapply (Mac/Linux only)\")\nresult_parallel &lt;- mclapply(numbers, slow_function, mc.cores = detectCores() - 1)\ntoc()"
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#the-foreach-package-a-more-intuitive-approach",
    "href": "get-started/parallel-computing-in-r.html#the-foreach-package-a-more-intuitive-approach",
    "title": "Parallel Computing in R",
    "section": "The foreach Package: A More Intuitive Approach",
    "text": "The foreach Package: A More Intuitive Approach\nMany R practitioners find the foreach package easier to understand and implement. The package functions like a loop but can execute in parallel:\n\n# Step 1: Create and register a parallel backend\ncl &lt;- makeCluster(detectCores() - 1)\nregisterDoParallel(cl)\n\n# Run sequential foreach with %do%\ntic(\"Sequential foreach\")\nresult_sequential &lt;- foreach(i = 1:10) %do% {\n  slow_function(i)\n}\ntoc()\n\nSequential foreach: 5.04 sec elapsed\n\n# Run parallel foreach with %dopar%\ntic(\"Parallel foreach\")\nresult_parallel &lt;- foreach(i = 1:10) %dopar% {\n  slow_function(i)\n}\ntoc()\n\nParallel foreach: 0.58 sec elapsed\n\n# Always stop the cluster when finished\nstopCluster(cl)\n\n# Verify results\nall.equal(result_sequential, result_parallel)\n\n[1] TRUE\n\n\n\nCombining Results with foreach\nOne notable feature of foreach is the ease with which results can be combined:\n\n# Create and register a parallel backend\ncl &lt;- makeCluster(detectCores() - 1)\nregisterDoParallel(cl)\n\n# Sum all results automatically with .combine='+'\ntic(\"Parallel sum of squares\")\ntotal &lt;- foreach(i = 1:100, .combine = '+') %dopar% {\n  i^2\n}\ntoc()\n\nParallel sum of squares: 0.11 sec elapsed\n\n# Stop the cluster\nstopCluster(cl)\n\n# Verify the result\nprint(paste(\"Result obtained:\", total))\n\n[1] \"Result obtained: 338350\"\n\nprint(paste(\"Correct answer:\", sum((1:100)^2)))\n\n[1] \"Correct answer: 338350\""
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#a-real-example-matrix-operations",
    "href": "get-started/parallel-computing-in-r.html#a-real-example-matrix-operations",
    "title": "Parallel Computing in R",
    "section": "A Real Example: Matrix Operations",
    "text": "A Real Example: Matrix Operations\nLet’s try something more realistic. Matrix operations are perfect for parallelization:\n\n# A more computationally intensive function\nmatrix_function &lt;- function(n) {\n  # Create a random n×n matrix\n  m &lt;- matrix(rnorm(n*n), ncol = n)\n  # Calculate eigenvalues (computationally expensive)\n  eigen(m)\n  return(sum(diag(m)))\n}\n\n# Let's process 8 matrices of size 300×300\nmatrix_sizes &lt;- rep(300, 8)\n\n\nPerformance Comparison\nLet’s compare how different methods perform:\n\n# Sequential execution\ntic(\"Sequential\")\nsequential_result &lt;- lapply(matrix_sizes, matrix_function)\nsequential_time &lt;- toc(quiet = TRUE)\nsequential_time &lt;- sequential_time$toc - sequential_time$tic\n\n# Parallel with parLapply\ncl &lt;- makeCluster(detectCores() - 1)\nclusterExport(cl, \"matrix_function\")\ntic(\"parLapply\")\nparlapply_result &lt;- parLapply(cl, matrix_sizes, matrix_function)\nparlapply_time &lt;- toc(quiet = TRUE)\nparlapply_time &lt;- parlapply_time$toc - parlapply_time$tic\nstopCluster(cl)\n\n# Parallel with foreach\ncl &lt;- makeCluster(detectCores() - 1)\nregisterDoParallel(cl)\ntic(\"foreach\")\nforeach_result &lt;- foreach(s = matrix_sizes) %dopar% {\n  matrix_function(s)\n}\nforeach_time &lt;- toc(quiet = TRUE)\nforeach_time &lt;- foreach_time$toc - foreach_time$tic\nstopCluster(cl)\n\n# Create a results table\nresults &lt;- data.frame(\n  Method = c(\"Sequential\", \"parLapply\", \"foreach\"),\n  Time = c(sequential_time, parlapply_time, foreach_time),\n  Speedup = c(1, sequential_time/parlapply_time, sequential_time/foreach_time)\n)\n\n# Display the results\nresults\n\n      Method Time  Speedup\n1 Sequential 1.22 1.000000\n2  parLapply 0.22 5.545455\n3    foreach 0.23 5.304348\n\n\n\n\nVisualizing the Results\n\n# Load ggplot2 for visualization\nlibrary(ggplot2)\n\n# Plot execution times\nggplot(results, aes(x = reorder(Method, -Time), y = Time, fill = Method)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Execution Time Comparison\",\n       x = \"Method\", y = \"Time (seconds)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n# Plot speedup\nggplot(results, aes(x = reorder(Method, Speedup), y = Speedup, fill = Method)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Speedup Comparison\",\n       x = \"Method\", y = \"Times faster than sequential\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#when-to-use-parallel-computing",
    "href": "get-started/parallel-computing-in-r.html#when-to-use-parallel-computing",
    "title": "Parallel Computing in R",
    "section": "",
    "text": "Parallel computing isn’t always the right choice. Here’s when to use it:\n✅ Good for parallelization: - Independent calculations (like applying the same function to different data chunks) - Computationally intensive tasks (simulations, bootstrap resampling) - Tasks that take more than a few seconds to run sequentially\n❌ Not good for parallelization: - Very quick operations (parallelization overhead may exceed the time saved) - Tasks with heavy dependencies between steps - I/O-bound operations (reading/writing files)"
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#quick-tips-for-success",
    "href": "get-started/parallel-computing-in-r.html#quick-tips-for-success",
    "title": "Parallel Computing in R",
    "section": "",
    "text": "Always stop your clusters with stopCluster(cl) when you’re done\nLeave one core free for your operating system\nStart small and test with a subset of your data\nWatch your memory usage - each worker needs its own copy of the data"
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#next-steps",
    "href": "get-started/parallel-computing-in-r.html#next-steps",
    "title": "Parallel Computing in R",
    "section": "",
    "text": "Once you’re comfortable with these basics, you can explore:\n• The future package for more advanced parallel computing • The furrr package for parallel versions of purrr functions\n• Parallel computing with large datasets using data.table or dplyr • Distributed computing across multiple machines\n\n\nHere’s a more advanced example using bootstrap sampling, which is a perfect use case for parallel computing:\n\n# Generate sample data\nset.seed(123)\nsample_data &lt;- rnorm(1000, mean = 100, sd = 15)\n\n# Function to perform bootstrap sampling\nbootstrap_mean &lt;- function(data, n_bootstrap = 1000) {\n  bootstrap_samples &lt;- replicate(n_bootstrap, {\n    sample_indices &lt;- sample(length(data), replace = TRUE)\n    mean(data[sample_indices])\n  })\n  return(bootstrap_samples)\n}\n\n# Sequential bootstrap\ntic(\"Sequential bootstrap\")\nsequential_bootstrap &lt;- bootstrap_mean(sample_data, 5000)\ntoc()\n\nSequential bootstrap: 0.67 sec elapsed\n\n# Parallel bootstrap using foreach\ncl &lt;- makeCluster(detectCores() - 1)\nregisterDoParallel(cl)\n\ntic(\"Parallel bootstrap\")\nparallel_bootstrap &lt;- foreach(i = 1:5000, .combine = c) %dopar% {\n  sample_indices &lt;- sample(length(sample_data), replace = TRUE)\n  mean(sample_data[sample_indices])\n}\ntoc()\n\nParallel bootstrap: 2.54 sec elapsed\n\nstopCluster(cl)\n\n# Compare results\ncat(\"Sequential mean:\", mean(sequential_bootstrap), \"\\n\")\n\nSequential mean: 100.2462 \n\ncat(\"Parallel mean:\", mean(parallel_bootstrap), \"\\n\")\n\nParallel mean: 100.2543 \n\ncat(\"Original data mean:\", mean(sample_data), \"\\n\")\n\nOriginal data mean: 100.2419 \n\n\n\n\n\n\n# Check memory usage\ncat(\"Available memory info:\\n\")\n\nAvailable memory info:\n\ncat(\"Total RAM:\", round(as.numeric(system(\"wmic OS get TotalVisibleMemorySize /value\", intern = TRUE)[2]) / 1024^2, 1), \"GB\\n\")\n\nTotal RAM: NA GB\n\n# Tips for managing memory in parallel computing:\n# 1. Use smaller chunks of data\n# 2. Return only necessary results from workers\n# 3. Clear unnecessary objects before parallel processing\n# 4. Monitor memory usage with tools like 'top' or Task Manager\n\nThis tutorial provides a solid foundation for getting started with parallel computing in R. Remember to always test your parallel code with small datasets first, and be mindful of the overhead that comes with setting up parallel workers!"
  },
  {
    "objectID": "get-started/r-package-management.html",
    "href": "get-started/r-package-management.html",
    "title": "Installing and Managing R Packages",
    "section": "",
    "text": "R’s true power comes from its vast ecosystem of packages. This guide shows how to effectively install, update, and manage packages for data analysis projects."
  },
  {
    "objectID": "get-started/r-package-management.html#installing-packages",
    "href": "get-started/r-package-management.html#installing-packages",
    "title": "Installing and Managing R Packages",
    "section": "Installing Packages",
    "text": "Installing Packages\nR packages can be installed from CRAN (the Comprehensive R Archive Network) using the install.packages() function:\n\n# Install a single package\ninstall.packages(\"dplyr\")\n\n# Install multiple packages at once\ninstall.packages(c(\"ggplot2\", \"tidyr\", \"readr\"))\n\nSome packages may require selecting a CRAN mirror for downloading. Simply choose a location nearby from the list that appears.\nTo set a CRAN mirror manually:\n\n# Set CRAN mirror manually (example: RStudio mirror)\noptions(repos = c(CRAN = \"https://cran.rstudio.com/\"))"
  },
  {
    "objectID": "get-started/r-package-management.html#loading-packages",
    "href": "get-started/r-package-management.html#loading-packages",
    "title": "Installing and Managing R Packages",
    "section": "Loading Packages",
    "text": "Loading Packages\nOnce installed, packages need to be loaded in each R session before using them:\n\n# Load a package\nlibrary(ggplot2)\n\n# Functions from the package can now be used\nggplot(mtcars, aes(x = wt, y = mpg)) + \n  geom_point() + \n  theme_minimal()"
  },
  {
    "objectID": "get-started/r-package-management.html#checking-installed-packages",
    "href": "get-started/r-package-management.html#checking-installed-packages",
    "title": "Installing and Managing R Packages",
    "section": "Checking Installed Packages",
    "text": "Checking Installed Packages\nTo see what packages are installed on the system:\n\n# List all installed packages\ninstalled.packages()[, c(\"Package\", \"Version\")]\n\n# Check if a specific package is installed\n\"dplyr\" %in% rownames(installed.packages())"
  },
  {
    "objectID": "get-started/r-package-management.html#updating-packages",
    "href": "get-started/r-package-management.html#updating-packages",
    "title": "Installing and Managing R Packages",
    "section": "Updating Packages",
    "text": "Updating Packages\nKeeping packages up-to-date ensures you have the latest features and bug fixes:\n\n# Update all packages\nupdate.packages()\n\n# Update without asking for confirmation\nupdate.packages(ask = FALSE)"
  },
  {
    "objectID": "get-started/r-package-management.html#installing-from-github",
    "href": "get-started/r-package-management.html#installing-from-github",
    "title": "Installing and Managing R Packages",
    "section": "Installing from GitHub",
    "text": "Installing from GitHub\nMany cutting-edge packages are available on GitHub before they reach CRAN:\n\n# First, install the devtools package if you haven't already\ninstall.packages(\"devtools\")\n\n# Then use it to install packages from GitHub\nlibrary(devtools)\ninstall_github(\"tidyverse/ggplot2\")"
  },
  {
    "objectID": "get-started/r-package-management.html#package-dependencies",
    "href": "get-started/r-package-management.html#package-dependencies",
    "title": "Installing and Managing R Packages",
    "section": "Package Dependencies",
    "text": "Package Dependencies\nR automatically handles dependencies (other packages required by your target package). However, sometimes you may encounter issues with dependencies that require manual intervention:\n\n# Force reinstallation of a package and its dependencies\ninstall.packages(\"problematic_package\", dependencies = TRUE)"
  },
  {
    "objectID": "get-started/r-package-management.html#creating-a-reproducible-environment",
    "href": "get-started/r-package-management.html#creating-a-reproducible-environment",
    "title": "Installing and Managing R Packages",
    "section": "Creating a Reproducible Environment",
    "text": "Creating a Reproducible Environment\nFor collaborative or production work, it’s important to track package versions:\n\n# Record packages and versions with renv\ninstall.packages(\"renv\")\nlibrary(renv)\nrenv::init()      # Initialize a project environment\nrenv::snapshot()  # Save the current state of packages\n\nThe renv package creates isolated, reproducible environments similar to Python’s virtual environments."
  },
  {
    "objectID": "get-started/r-package-management.html#managing-package-conflicts",
    "href": "get-started/r-package-management.html#managing-package-conflicts",
    "title": "Installing and Managing R Packages",
    "section": "Managing Package Conflicts",
    "text": "Managing Package Conflicts\nSometimes packages have functions with the same name, causing conflicts:\n\n# Specify the package explicitly\ndplyr::filter(df, x &gt; 10)  # Use filter from dplyr\nstats::filter(x, rep(1/3, 3))  # Use filter from stats"
  },
  {
    "objectID": "get-started/r-package-management.html#pro-tip-package-installation-script",
    "href": "get-started/r-package-management.html#pro-tip-package-installation-script",
    "title": "Installing and Managing R Packages",
    "section": "Pro Tip: Package Installation Script",
    "text": "Pro Tip: Package Installation Script\nFor projects requiring multiple packages, create an installation script:\n\n# Create a function to check and install packages\ninstall_if_missing &lt;- function(pkg) {\n  if (!require(pkg, character.only = TRUE)) {\n    install.packages(pkg)\n    library(pkg, character.only = TRUE)\n  }\n}\n\n# List all required packages\npackages &lt;- c(\"tidyverse\", \"data.table\", \"caret\", \"lubridate\", \"janitor\")\n\n# Install all packages\ninvisible(sapply(packages, install_if_missing))\n\nThis script installs packages only if they’re not already available, saving time when setting up on a new machine or sharing code with collaborators."
  },
  {
    "objectID": "get-started/variables.html",
    "href": "get-started/variables.html",
    "title": "Variables in R",
    "section": "",
    "text": "Variables in R store data that can be referenced and manipulated throughout code. The following demonstrates how to create and work with variables:\n\n\n\n# Using the assignment operator (&lt;-)\nx &lt;- 10\ny &lt;- \"Hello, R!\"\nz &lt;- TRUE\n\n# Print the variables\nx\n\n[1] 10\n\ny\n\n[1] \"Hello, R!\"\n\nz\n\n[1] TRUE\n\n\n\n\n\n\n# Using the equals sign (=)\nage = 25\n\n# Using the assignment operator in reverse (-&gt;)\n\"Data Scientist\" -&gt; job_title\n\n# Print the variables\nage\n\n[1] 25\n\njob_title\n\n[1] \"Data Scientist\"\n\n\n\n\n\n\nNames can contain letters, numbers, dots (.) and underscores (_)\nNames must start with a letter or a dot\nIf a name starts with a dot, it cannot be followed by a number\nNames are case-sensitive (Value and value are different variables)\n\n\n# Valid variable names\nvalid_name &lt;- 1\nvalidName &lt;- 2\nvalid.name &lt;- 3\n.hidden &lt;- 4\n\n# Print variables\nvalid_name\n\n[1] 1\n\nvalidName\n\n[1] 2\n\nvalid.name\n\n[1] 3\n\n.hidden\n\n[1] 4\n\n\n\n\n\nR has several basic data types:\n\n# Numeric\nnum &lt;- 42.5\ntypeof(num)\n\n[1] \"double\"\n\n# Integer (note the L suffix)\nint &lt;- 42L\ntypeof(int)\n\n[1] \"integer\"\n\n# Character\ntext &lt;- \"R programming\"\ntypeof(text)\n\n[1] \"character\"\n\n# Logical\nflag &lt;- TRUE\ntypeof(flag)\n\n[1] \"logical\"\n\n\n\n\n\n\n# Check if a variable is of a specific type\nis.numeric(num)\n\n[1] TRUE\n\nis.character(text)\n\n[1] TRUE\n\n# Convert between types\nas.character(num)\n\n[1] \"42.5\"\n\nas.numeric(\"100\")\n\n[1] 100\n\nas.logical(1)\n\n[1] TRUE\n\n\n\n\n\n\n# Get information about a variable\nx &lt;- c(1, 2, 3, 4, 5)\nclass(x)\n\n[1] \"numeric\"\n\nlength(x)\n\n[1] 5\n\nstr(x)\n\n num [1:5] 1 2 3 4 5\n\n\nNote that R is dynamically typed, so variables can change types during execution. This flexibility is one of R’s strengths for data analysis."
  },
  {
    "objectID": "get-started/variables.html#creating-variables-in-r",
    "href": "get-started/variables.html#creating-variables-in-r",
    "title": "Variables in R",
    "section": "",
    "text": "Variables in R store data that can be referenced and manipulated throughout code. The following demonstrates how to create and work with variables:\n\n\n\n# Using the assignment operator (&lt;-)\nx &lt;- 10\ny &lt;- \"Hello, R!\"\nz &lt;- TRUE\n\n# Print the variables\nx\n\n[1] 10\n\ny\n\n[1] \"Hello, R!\"\n\nz\n\n[1] TRUE\n\n\n\n\n\n\n# Using the equals sign (=)\nage = 25\n\n# Using the assignment operator in reverse (-&gt;)\n\"Data Scientist\" -&gt; job_title\n\n# Print the variables\nage\n\n[1] 25\n\njob_title\n\n[1] \"Data Scientist\"\n\n\n\n\n\n\nNames can contain letters, numbers, dots (.) and underscores (_)\nNames must start with a letter or a dot\nIf a name starts with a dot, it cannot be followed by a number\nNames are case-sensitive (Value and value are different variables)\n\n\n# Valid variable names\nvalid_name &lt;- 1\nvalidName &lt;- 2\nvalid.name &lt;- 3\n.hidden &lt;- 4\n\n# Print variables\nvalid_name\n\n[1] 1\n\nvalidName\n\n[1] 2\n\nvalid.name\n\n[1] 3\n\n.hidden\n\n[1] 4\n\n\n\n\n\nR has several basic data types:\n\n# Numeric\nnum &lt;- 42.5\ntypeof(num)\n\n[1] \"double\"\n\n# Integer (note the L suffix)\nint &lt;- 42L\ntypeof(int)\n\n[1] \"integer\"\n\n# Character\ntext &lt;- \"R programming\"\ntypeof(text)\n\n[1] \"character\"\n\n# Logical\nflag &lt;- TRUE\ntypeof(flag)\n\n[1] \"logical\"\n\n\n\n\n\n\n# Check if a variable is of a specific type\nis.numeric(num)\n\n[1] TRUE\n\nis.character(text)\n\n[1] TRUE\n\n# Convert between types\nas.character(num)\n\n[1] \"42.5\"\n\nas.numeric(\"100\")\n\n[1] 100\n\nas.logical(1)\n\n[1] TRUE\n\n\n\n\n\n\n# Get information about a variable\nx &lt;- c(1, 2, 3, 4, 5)\nclass(x)\n\n[1] \"numeric\"\n\nlength(x)\n\n[1] 5\n\nstr(x)\n\n num [1:5] 1 2 3 4 5\n\n\nNote that R is dynamically typed, so variables can change types during execution. This flexibility is one of R’s strengths for data analysis."
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installing R and RStudio",
    "section": "",
    "text": "This guide covers the installation of R and RStudio, the essential tools for R programming.\n\n\nR is a free, open-source programming language for statistical computing and graphics.\n\n\n\nRStudio is an integrated development environment (IDE) for R with syntax highlighting, code completion, and visualization tools."
  },
  {
    "objectID": "installation.html#getting-started-with-r",
    "href": "installation.html#getting-started-with-r",
    "title": "Installing R and RStudio",
    "section": "",
    "text": "This guide covers the installation of R and RStudio, the essential tools for R programming.\n\n\nR is a free, open-source programming language for statistical computing and graphics.\n\n\n\nRStudio is an integrated development environment (IDE) for R with syntax highlighting, code completion, and visualization tools."
  },
  {
    "objectID": "installation.html#installation-guide",
    "href": "installation.html#installation-guide",
    "title": "Installing R and RStudio",
    "section": "Installation Guide",
    "text": "Installation Guide\n\nInstalling R\n\nWindows\n\nGo to the CRAN (Comprehensive R Archive Network) website\nClick on “Download R for Windows”\nClick on “base”\nClick on the download link for the latest version (e.g., “Download R-4.x.x for Windows”)\nRun the downloaded installer and follow the prompts\n\nAccept default settings or customize as needed\nNote installation location\n\n\n\n\nmacOS\n\nGo to the CRAN website\nClick on “Download R for macOS”\nDownload the latest .pkg file for the system\nOpen the downloaded file and follow installation instructions\n\n\n\nLinux (Ubuntu/Debian)\n\nOpen a terminal window\nUpdate your system’s package index:\nsudo apt update\nInstall R:\nsudo apt install r-base\n\n\n\n\nInstalling RStudio\nAfter installing R, install RStudio:\n\nGo to the RStudio download page\nDownload the installer for the operating system\nRun the installer and follow prompts"
  },
  {
    "objectID": "installation.html#verifying-your-installation",
    "href": "installation.html#verifying-your-installation",
    "title": "Installing R and RStudio",
    "section": "Verifying Your Installation",
    "text": "Verifying Your Installation\nTo verify that R and RStudio are installed correctly:\n\nOpen RStudio\nIn the Console pane (usually at the bottom left), type:\nR.version\nPress Enter. You should see information about your R installation."
  },
  {
    "objectID": "installation.html#installing-r-packages",
    "href": "installation.html#installing-r-packages",
    "title": "Installing R and RStudio",
    "section": "Installing R Packages",
    "text": "Installing R Packages\nR’s functionality can be extended with packages. Here’s how to install a package:\n\nIn RStudio, go to the Console\nType the following command to install a package (replace “packagename” with the actual package name):\ninstall.packages(\"packagename\")\nFor example, to install the tidyverse collection of packages:\ninstall.packages(\"tidyverse\")\n\n\nEssential Packages for Beginners\nConsider installing these useful packages to get started:\n# Run these commands in the RStudio console\ninstall.packages(\"tidyverse\")  # Data manipulation and visualization\ninstall.packages(\"rmarkdown\")  # For creating dynamic documents\ninstall.packages(\"knitr\")      # For report generation\ninstall.packages(\"shiny\")      # For interactive web applications"
  },
  {
    "objectID": "installation.html#troubleshooting",
    "href": "installation.html#troubleshooting",
    "title": "Installing R and RStudio",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nCommon Issues on Windows\n\nPermission errors: Run RStudio as administrator\nPath too long errors: Install R in a directory with a shorter path\n\n\n\nCommon Issues on macOS\n\nPackage installation failures: Install development tools:\nxcode-select --install\n\n\n\nCommon Issues on Linux\n\nMissing dependencies: Install R dependencies:\nsudo apt install libcurl4-openssl-dev libssl-dev libxml2-dev"
  },
  {
    "objectID": "installation.html#next-steps",
    "href": "installation.html#next-steps",
    "title": "Installing R and RStudio",
    "section": "Next Steps",
    "text": "Next Steps\nWith R and RStudio installed, proceed to Introduction to Analytics with R to begin learning."
  },
  {
    "objectID": "posts/bayesian-optimization-xgboost.html",
    "href": "posts/bayesian-optimization-xgboost.html",
    "title": "Optimizing XGBoost Hyperparameters Using Bayesian Optimization in R",
    "section": "",
    "text": "Hyperparameter tuning for machine learning models represents a computationally intensive and time-consuming process. This tutorial demonstrates the implementation of Bayesian optimization techniques to efficiently identify optimal XGBoost hyperparameters, thereby reducing computational overhead while enhancing model performance. The methodology presented provides a systematic approach to hyperparameter optimization that significantly outperforms traditional grid search methods."
  },
  {
    "objectID": "posts/bayesian-optimization-xgboost.html#required-packages",
    "href": "posts/bayesian-optimization-xgboost.html#required-packages",
    "title": "Using Bayesian Optimization to Tune XGBoost Models in R",
    "section": "Required Packages",
    "text": "Required Packages\n\n# Load required packages\nlibrary(xgboost)\nlibrary(ParBayesianOptimization)\nlibrary(mlbench)\nlibrary(dplyr)\nlibrary(recipes)\nlibrary(rsample)"
  },
  {
    "objectID": "posts/bayesian-optimization-xgboost.html#data-preparation",
    "href": "posts/bayesian-optimization-xgboost.html#data-preparation",
    "title": "Using Bayesian Optimization to Tune XGBoost Models in R",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe’ll use the Boston Housing dataset – a classic regression problem with both numeric and categorical variables.\n\n# Load the Boston Housing dataset\ndata(\"BostonHousing2\")\n\n# Quick look at the data structure\nstr(BostonHousing2)\n\n'data.frame':   506 obs. of  19 variables:\n $ town   : Factor w/ 92 levels \"Arlington\",\"Ashland\",..: 54 77 77 46 46 46 69 69 69 69 ...\n $ tract  : int  2011 2021 2022 2031 2032 2033 2041 2042 2043 2044 ...\n $ lon    : num  -71 -71 -70.9 -70.9 -70.9 ...\n $ lat    : num  42.3 42.3 42.3 42.3 42.3 ...\n $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n $ cmedv  : num  24 21.6 34.7 33.4 36.2 28.7 22.9 22.1 16.5 18.9 ...\n $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n $ chas   : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n $ rm     : num  6.58 6.42 7.18 7 7.15 ...\n $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...\n $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...\n $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...\n $ tax    : int  296 242 242 222 222 222 311 311 311 311 ...\n $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n $ b      : num  397 397 393 395 397 ...\n $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...\n\n\nXGBoost requires numeric inputs, so we’ll use the recipes package to transform our categorical variables:\n\n# Create a recipe for preprocessing\nrec &lt;- recipe(cmedv ~ ., data = BostonHousing2) %&gt;%\n  # Collapse categories where population is &lt; 3%\n  step_other(town, chas, threshold = .03, other = \"Other\") %&gt;% \n  # Create dummy variables for all factor variables \n  step_dummy(all_nominal_predictors())\n\n# Train the recipe on the dataset\nprep &lt;- prep(rec, training = BostonHousing2)\n\n# Create the final model matrix\nmodel_df &lt;- bake(prep, new_data = BostonHousing2)\n\n# Check the column names after one-hot encoding\ncolnames(model_df)\n\n [1] \"tract\"                  \"lon\"                    \"lat\"                   \n [4] \"medv\"                   \"crim\"                   \"zn\"                    \n [7] \"indus\"                  \"nox\"                    \"rm\"                    \n[10] \"age\"                    \"dis\"                    \"rad\"                   \n[13] \"tax\"                    \"ptratio\"                \"b\"                     \n[16] \"lstat\"                  \"cmedv\"                  \"town_Boston.Savin.Hill\"\n[19] \"town_Cambridge\"         \"town_Lynn\"              \"town_Newton\"           \n[22] \"town_Other\"             \"chas_X1\"               \n\n\nNext, we’ll split our data into training and testing sets:\n\n# Create a 70/30 train-test split\nsplits &lt;- rsample::initial_split(model_df, prop = 0.7)\ntrain_df &lt;- rsample::training(splits)\ntest_df &lt;- rsample::testing(splits)\n\n# Prepare the training data for XGBoost\nX &lt;- train_df %&gt;%\n  select(!medv, !cmedv) %&gt;%\n  as.matrix()\n\n# Get the target variable\ny &lt;- train_df %&gt;% pull(cmedv)\n\n# Create cross-validation folds\nfolds &lt;- list(\n  fold1 = as.integer(seq(1, nrow(X), by = 5)),\n  fold2 = as.integer(seq(2, nrow(X), by = 5))\n)"
  },
  {
    "objectID": "posts/bayesian-optimization-xgboost.html#setting-up-bayesian-optimization",
    "href": "posts/bayesian-optimization-xgboost.html#setting-up-bayesian-optimization",
    "title": "Using Bayesian Optimization to Tune XGBoost Models in R",
    "section": "Setting Up Bayesian Optimization",
    "text": "Setting Up Bayesian Optimization\nBayesian optimization requires two key components:\n\nAn objective function that evaluates model performance\nThe parameter bounds we want to explore\n\n\n# Our objective function takes hyperparameters as inputs\nobj_func &lt;- function(eta, max_depth, min_child_weight, subsample, lambda, alpha) {\n  \n  param &lt;- list(\n    # Learning parameters\n    eta = eta,                       # Learning rate\n    max_depth = max_depth,           # Tree depth\n    min_child_weight = min_child_weight, # Min observations per node\n    subsample = subsample,           # Data subsampling\n    lambda = lambda,                 # L2 regularization\n    alpha = alpha,                   # L1 regularization\n    \n    booster = \"gbtree\",             # Use tree model\n    objective = \"reg:squarederror\",  # Regression task\n    eval_metric = \"mape\"            # Mean Absolute Percentage Error\n  )\n  \n  xgbcv &lt;- xgb.cv(params = param,\n                  data = X,\n                  label = y,\n                  nround = 50,\n                  folds = folds,\n                  prediction = TRUE,\n                  early_stopping_rounds = 5,\n                  verbose = 0,\n                  maximize = FALSE)\n  \n  lst &lt;- list(\n    # First argument must be named as \"Score\"\n    # Function finds maxima so inverting the output\n    Score = -min(xgbcv$evaluation_log$test_mape_mean),\n    \n    # Get number of trees for the best performing model\n    nrounds = xgbcv$best_iteration\n  )\n  \n  return(lst)\n}\n\n# Define the search space for each parameter\nbounds &lt;- list(\n  eta = c(0.001, 0.2),             # Learning rate range\n  max_depth = c(1L, 10L),           # Tree depth range\n  min_child_weight = c(1, 50),      # Min observations range\n  subsample = c(0.1, 1),            # Subsampling range\n  lambda = c(1, 10),                # L2 regularization range\n  alpha = c(1, 10)                  # L1 regularization range\n)"
  },
  {
    "objectID": "posts/bayesian-optimization-xgboost.html#running-bayesian-optimization",
    "href": "posts/bayesian-optimization-xgboost.html#running-bayesian-optimization",
    "title": "Tutorial: Optimizing XGBoost Hyperparameters Using Bayesian Optimization in R",
    "section": "Running Bayesian Optimization",
    "text": "Running Bayesian Optimization\nNow we’ll run the optimization process to intelligently search for the best parameters:\n\nset.seed(1234)\nbayes_out &lt;- bayesOpt(\n  FUN = obj_func,                    # Our objective function\n  bounds = bounds,                   # Parameter bounds\n  initPoints = length(bounds) + 2,   # Initial random points\n  iters.n = 10,                      # Number of iterations\n  verbose = 0                        # Suppress output\n)\n\n# View top results\nbayes_out$scoreSummary[1:5, c(3:8, 13)]\n\n          eta max_depth min_child_weight subsample   lambda    alpha      Score\n        &lt;num&gt;     &lt;num&gt;            &lt;num&gt;     &lt;num&gt;    &lt;num&gt;    &lt;num&gt;      &lt;num&gt;\n1: 0.13392137         8         4.913332 0.2105925 4.721124 3.887629 -0.1292920\n2: 0.19400811         2        25.454160 0.9594105 9.329695 3.173695 -0.1790158\n3: 0.16079775         2        14.035652 0.5118349 1.229953 5.093530 -0.1662595\n4: 0.08957707         4        12.534842 0.3844404 4.358837 1.788342 -0.1672395\n5: 0.02876388         4        36.586761 0.8107181 6.137100 6.039125 -0.3320015\n\n# Get the best parameters\nbest_params &lt;- getBestPars(bayes_out)\ndata.frame(best_params)\n\n        eta max_depth min_child_weight subsample lambda    alpha\n1 0.1251447        10                1         1      1 5.905011"
  },
  {
    "objectID": "posts/bayesian-optimization-xgboost.html#training-the-final-model",
    "href": "posts/bayesian-optimization-xgboost.html#training-the-final-model",
    "title": "Tutorial: Optimizing XGBoost Hyperparameters Using Bayesian Optimization in R",
    "section": "Training the Final Model",
    "text": "Training the Final Model\nWith the optimal hyperparameters identified, we can now train our final XGBoost model.\n\n# Combine best params with base params\nopt_params &lt;- append(\n  list(booster = \"gbtree\", \n       objective = \"reg:squarederror\", \n       eval_metric = \"mae\"), \n  best_params\n)\n\n# Run cross-validation to determine optimal number of rounds\nxgbcv &lt;- xgb.cv(\n  params = opt_params,\n  data = X,\n  label = y,\n  nround = 100,\n  folds = folds,\n  prediction = TRUE,\n  early_stopping_rounds = 5,\n  verbose = 0,\n  maximize = FALSE\n)\n\n# Get optimal number of rounds\nnrounds = xgbcv$best_iteration\n\n# Fit the final XGBoost model\nmdl &lt;- xgboost(\n  data = X, \n  label = y, \n  params = opt_params, \n  maximize = FALSE, \n  early_stopping_rounds = 5, \n  nrounds = nrounds, \n  verbose = 0\n)\n\n# Make predictions on the test set\nactuals &lt;- test_df$cmedv\npredicted &lt;- test_df %&gt;%\n  select_at(mdl$feature_names) %&gt;%\n  as.matrix() %&gt;%\n  predict(mdl, newdata = .)\n\n# Evaluate performance using Mean Absolute Percentage Error (MAPE)\nmape &lt;- mean(abs(actuals - predicted)/actuals)\ncat(\"MAPE on test set:\", mape)\n\nMAPE on test set: 0.006424492"
  },
  {
    "objectID": "posts/bayesian-optimization-xgboost.html#why-bayesian-optimization",
    "href": "posts/bayesian-optimization-xgboost.html#why-bayesian-optimization",
    "title": "Tutorial: Optimizing XGBoost Hyperparameters Using Bayesian Optimization in R",
    "section": "Why Bayesian Optimization",
    "text": "Why Bayesian Optimization\nBayesian optimization offers several key advantages over traditional grid search:\n\nEfficiency: Finds optimal parameters in fewer iterations\nIntelligence: Learns from previous evaluations to focus on promising areas\nScalability: Remains efficient even with many hyperparameters\nSpeed: Completes in a fraction of the time while achieving comparable or better results\n\nThis approach becomes increasingly valuable as model complexity grows. For production models, consider increasing the iterations (iters.n) to ensure thorough exploration of the parameter space.\nThe ParBayesianOptimization package makes this powerful technique accessible to R users, allowing you to build better models with less computational overhead."
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html",
    "href": "posts/custom-charting-functions-ggplot2.html",
    "title": "Developing Custom Charting Functions with ggplot2",
    "section": "",
    "text": "While R provides numerous options for two-dimensional graphics and data visualization, ggplot2 offers great functionality, features, and visual quality. This tutorial shows how to develop customized charting functions for specific visualization types, utilizing ggplot2 as the foundational visualization engine. The approach enables the creation of reusable, standardized visualization components suitable for production environments and analytical workflows."
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#required-libraries",
    "href": "posts/custom-charting-functions-ggplot2.html#required-libraries",
    "title": "Custom Charting Functions Using ggplot2",
    "section": "Required Libraries",
    "text": "Required Libraries\n\n# Load required packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(stringr)"
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#sample-dataset",
    "href": "posts/custom-charting-functions-ggplot2.html#sample-dataset",
    "title": "Custom Charting Functions Using ggplot2",
    "section": "Sample Dataset",
    "text": "Sample Dataset\nFor this demonstration, we’ll use a summarized version of the COVID-19 Data Repository hosted by Johns Hopkins University.\n\n# Load COVID-19 data\ndf &lt;- read.csv(\"https://bit.ly/3G8G63u\")\n\n# Get top 5 countries by death count\ntop_countries &lt;- df %&gt;% \n  group_by(country) %&gt;% \n  summarise(count = sum(deaths_daily)) %&gt;% \n  top_n(5) %&gt;% \n  .$country\n\nprint(top_countries)\n\n[1] \"Brazil\" \"India\"  \"Mexico\" \"Russia\" \"US\"    \n\n\nLet’s prepare our data for visualization by creating a 7-day moving average of daily confirmed cases for the top five countries:\n\n# Create a data frame with the required information\n# Note that a centered 7-day moving average is used\nplotdf &lt;- df %&gt;% \n  mutate(date = as.Date(date, format = \"%m/%d/%Y\")) %&gt;% \n  filter(country %in% top_countries) %&gt;% \n  group_by(country, date) %&gt;% \n  summarise(count = sum(confirmed_daily)) %&gt;%\n  arrange(country, date) %&gt;% \n  group_by(country) %&gt;% \n  mutate(MA = zoo::rollapply(count, FUN = mean, width = 7, by = 1, fill = NA, align = \"center\"))"
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#building-a-simple-line-chart-function",
    "href": "posts/custom-charting-functions-ggplot2.html#building-a-simple-line-chart-function",
    "title": "Custom Charting Functions Using ggplot2",
    "section": "Building a Simple Line Chart Function",
    "text": "Building a Simple Line Chart Function\nLet’s start by creating a basic line chart function. Note the use of aes_string() instead of just aes(). This allows us to supply arguments to ggplot2 as strings, making our function more flexible.\n\n# Function definition\nline_chart &lt;- function(df, \n                       x, \n                       y, \n                       group_color = NULL, \n                       line_width = 1, \n                       line_type = 1){\n  \n  ggplot(df, aes(x = !! sym(x), \n                 y = !! sym(y), \n                 color = !! sym(group_color))) + \n    geom_line(linewidth = line_width, \n              linetype = line_type)\n}\n\n# Test run\nline_chart(plotdf,\n           x = \"date\",\n           y = \"MA\",\n           group_color = \"country\", \n           line_type = 1, \n           line_width = 1.2)"
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#creating-a-custom-theme",
    "href": "posts/custom-charting-functions-ggplot2.html#creating-a-custom-theme",
    "title": "Custom Charting Functions Using ggplot2",
    "section": "Creating a Custom Theme",
    "text": "Creating a Custom Theme\nNow that we know how to encapsulate the call to ggplot2 in a more intuitive manner, we can create a customized theme for our charts. This is useful since this theme can be applied to any chart.\n\ncustom_theme &lt;- function(plt, \n                         base_size = 11, \n                         base_line_size = 1, \n                         palette = \"Set1\"){\n  \n  # Note the use of \"+\" and not \"%&gt;%\"\n  plt + \n    # Adjust overall font size\n    theme_minimal(base_size = base_size, \n                  base_line_size = base_line_size) + \n    \n    # Put legend at the bottom\n    theme(legend.position = \"bottom\") + \n    \n    # Different colour scale\n    scale_color_brewer(palette = palette)\n}\n\n# Test run\nline_chart(plotdf, \"date\", \"MA\", \"country\") %&gt;% custom_theme()"
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#enhancing-our-functions",
    "href": "posts/custom-charting-functions-ggplot2.html#enhancing-our-functions",
    "title": "Custom Charting Functions Using ggplot2",
    "section": "Enhancing Our Functions",
    "text": "Enhancing Our Functions\nLet’s add more features to our line_chart() function to make it more versatile:\n\nline_chart &lt;- function(df, \n                       x, y, \n                       group_color = NULL, \n                       line_width = 1, \n                       line_type = 1, \n                       xlab = NULL, \n                       ylab = NULL, \n                       title = NULL, \n                       subtitle = NULL, \n                       caption = NULL){\n  # Base plot\n  ggplot(df, aes(x = !! sym(x), \n                 y = !! sym(y), \n                 color = !! sym(group_color))) + \n    \n    # Line chart \n    geom_line(size = line_width, \n              linetype = line_type) + \n    \n    # Titles and subtitles\n    labs(x = xlab, \n         y = ylab, \n         title = title, \n         subtitle = subtitle, \n         caption = caption)\n}\n\nWe’ll also enhance our custom_theme() function to handle different axis formatting options:\n\ncustom_theme &lt;- function(plt, \n                         palette = \"Set1\", \n                         format_x_axis_as = NULL, \n                         format_y_axis_as = NULL, \n                         x_axis_scale = 1, \n                         y_axis_scale = 1, \n                         x_axis_text_size = 10, \n                         y_axis_text_size = 10, \n                         base_size = 11, \n                         base_line_size = 1, \n                         x_angle = 45){\n  \n  mappings &lt;- names(unlist(plt$mapping))\n  \n  p &lt;- plt + \n    \n    # Adjust overall font size\n    theme_minimal(base_size = base_size, \n                  base_line_size = base_line_size) + \n    \n    # Put legend at the bottom\n    theme(legend.position = \"bottom\", \n          axis.text.x = element_text(angle = x_angle)) + \n    \n    # Different colour palette\n    {if(\"colour\" %in% mappings) scale_color_brewer(palette = palette)}+\n    \n    {if(\"fill\" %in% mappings) scale_fill_brewer(palette = palette)}+\n    \n    # Change some theme options\n    theme(plot.background = element_rect(fill = \"#f7f7f7\"), \n          plot.subtitle = element_text(face = \"italic\"), \n          axis.title.x = element_text(face = \"bold\", \n                                      size = x_axis_text_size), \n          axis.title.y = element_text(face = \"bold\", \n                                      size = y_axis_text_size)) + \n    \n    # Change x-axis formatting\n    {if(!is.null(format_x_axis_as))\n      switch(format_x_axis_as, \n             \"date\" = scale_x_date(breaks = pretty_breaks(n = 12)), \n             \"number\" = scale_x_continuous(labels = number_format(accuracy = 0.1, \n                                                                  decimal.mark = \",\", \n                                                                  scale = x_axis_scale)), \n             \"percent\" = scale_x_continuous(labels = percent))} + \n    \n    # Change y-axis formatting\n    {if(!is.null(format_y_axis_as))\n      \n      switch(format_y_axis_as, \n             \"date\" = scale_y_date(breaks = pretty_breaks(n = 12)), \n             \"number\" = scale_y_continuous(labels = number_format(accuracy = 0.1, \n                                                                  decimal.mark = \",\", \n                                                                  scale = y_axis_scale)), \n             \"percent\" = scale_y_continuous(labels = percent))}\n  \n  # Capitalise all names\n  vec &lt;- lapply(p$labels, str_to_title)\n  names(vec) &lt;- names(p$labels)\n  p$labels &lt;- vec\n  \n  return(p)\n}"
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#putting-it-all-together",
    "href": "posts/custom-charting-functions-ggplot2.html#putting-it-all-together",
    "title": "Custom Charting Functions Using ggplot2",
    "section": "Putting It All Together",
    "text": "Putting It All Together\nNow let’s see how our enhanced functions work together to create a polished visualization:\n\nline_chart(plotdf,\n           x = \"date\", \n           y = \"MA\", \n           group_color = \"country\", \n           xlab = \"Date\", \n           ylab = \"Moving Avg. (in '000)\", \n           title = \"Daily COVID19 Case Load\", \n           subtitle = \"Top 5 countries by volume\") %&gt;% \n  \n  custom_theme(format_x_axis_as = \"date\", \n               format_y_axis_as = \"number\", \n               y_axis_scale = 0.001)"
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#applying-the-custom-theme-to-other-chart-types",
    "href": "posts/custom-charting-functions-ggplot2.html#applying-the-custom-theme-to-other-chart-types",
    "title": "Custom Charting Functions Using ggplot2",
    "section": "Applying the Custom Theme to Other Chart Types",
    "text": "Applying the Custom Theme to Other Chart Types\nThe beauty of our custom_theme() function is that it can be applied to any ggplot2 object. Let’s create a bar chart to demonstrate this flexibility:\n\np &lt;- plotdf %&gt;%  \n  mutate(month = format(date, \"%m-%b\")) %&gt;% \n  ggplot(aes(x = month, y = MA, fill = country)) + \n  geom_col(position = \"dodge\") + \n  labs(title = \"Monthly COVID19 Case load trend\", \n       subtitle = \"Top 5 countries\", \n       x = \"Month\", \n       y = \"Moving Average ('000)\")\n\ncustom_theme(p, \n             palette = \"Set2\", \n             format_y_axis_as = \"number\", \n             y_axis_scale = 0.001)"
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#benefits-of-custom-charting-functions",
    "href": "posts/custom-charting-functions-ggplot2.html#benefits-of-custom-charting-functions",
    "title": "Custom Charting Functions Using ggplot2",
    "section": "Benefits of Custom Charting Functions",
    "text": "Benefits of Custom Charting Functions\nCreating custom charting functions with ggplot2 offers several advantages:\n\nConsistency: Ensures all charts in your reports or dashboards have a consistent look and feel.\nEfficiency: Reduces the amount of code you need to write for commonly used chart types.\nMaintainability: Makes it easier to update the style of all charts by modifying a single function.\nSimplicity: Abstracts away the complexity of ggplot2 for team members who may not be as familiar with the package."
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#when-to-use-custom-functions-vs.-direct-ggplot2",
    "href": "posts/custom-charting-functions-ggplot2.html#when-to-use-custom-functions-vs.-direct-ggplot2",
    "title": "Custom Charting Functions Using ggplot2",
    "section": "When to Use Custom Functions vs. Direct ggplot2",
    "text": "When to Use Custom Functions vs. Direct ggplot2\nIt’s worth noting that building customized charting functions using ggplot2 is most useful when you need to create the same type of chart(s) repeatedly. When doing exploratory work, using ggplot2 directly is often easier and more flexible since you can build all kinds of charts (or layer different chart types) within the same pipeline."
  },
  {
    "objectID": "posts/getting-started-with-reticulate.html",
    "href": "posts/getting-started-with-reticulate.html",
    "title": "Getting Started with Python using R and Reticulate",
    "section": "",
    "text": "Want to use Python’s powerful libraries without leaving R? The reticulate package gives you the best of both worlds - R’s elegant data handling and visualization with Python’s machine learning and scientific computing tools. This post dives into how to set up a python environment using RStudio and the reticulate package and use this powerful bridge between languages. Here’s a quick 4-step process to get started."
  },
  {
    "objectID": "posts/getting-started-with-reticulate.html#quick-setup-in-4-steps",
    "href": "posts/getting-started-with-reticulate.html#quick-setup-in-4-steps",
    "title": "Getting Started with Python using R and reticulate",
    "section": "Quick Setup in 4 Steps",
    "text": "Quick Setup in 4 Steps\n\n1. Install reticulate\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\n\n\n\n2. Install Python via Miniconda\nThe easiest approach is to let reticulate handle Python installation for you:\n\ninstall_miniconda(path = \"c:/miniconda\")\n\n\n\n3. Connect to Python\nReticulate creates a default environment called r-reticulate. Let’s connect to it:\n\n# Check available environments\nconda_list()\n\n# Connect to the default environment\nuse_condaenv(\"r-reticulate\")\n\n\n\n4. Install Python Packages\nNow you can install any Python packages you need:\n\npy_install(c(\"pandas\", \"scikit-learn\", \"matplotlib\"))"
  },
  {
    "objectID": "posts/getting-started-with-reticulate.html#three-ways-to-use-python-in-r",
    "href": "posts/getting-started-with-reticulate.html#three-ways-to-use-python-in-r",
    "title": "Getting Started with Python using R and reticulate",
    "section": "Three Ways to Use Python in R",
    "text": "Three Ways to Use Python in R\n\n1. Import Python Modules Directly\n\n# Import pandas and use it like any R package\npd &lt;- import(\"pandas\")\n\n# Create a pandas Series\npd$Series(c(1, 2, 3, 4, 5))\n\n# Import numpy for numerical operations\nnp &lt;- import(\"numpy\")\nnp$mean(c(1:100))  # Calculate mean using numpy\n\n\n\n2. Write Python Code in R Markdown\nYou can mix R and Python code in the same document by using Python code chunks:\n\n# This is Python code!\nimport pandas as pd\nimport numpy as np\n\n# Create a simple DataFrame\ndf = pd.DataFrame({\n    'A': np.random.randn(5),\n    'B': np.random.randn(5)\n})\n\nprint(df.describe())\n\n\n\n3. Use Python Libraries in R Workflows\nThe most powerful approach is using Python’s machine learning libraries within R:\n\n# Import scikit-learn\nsk &lt;- import(\"sklearn.linear_model\")\n\n# Create and fit a linear regression model\nmodel &lt;- sk$LinearRegression()\nmodel$fit(X = as.matrix(mtcars[, c(\"disp\", \"hp\", \"wt\")]), \n         y = mtcars$mpg)\n\n# Get predictions and coefficients\npredictions &lt;- model$predict(as.matrix(mtcars[, c(\"disp\", \"hp\", \"wt\")]))\ncoefficients &lt;- data.frame(\n  Feature = c(\"Intercept\", \"disp\", \"hp\", \"wt\"),\n  Coefficient = c(model$intercept_, model$coef_)\n)\n\ncoefficients"
  },
  {
    "objectID": "posts/getting-started-with-reticulate.html#real-world-applications",
    "href": "posts/getting-started-with-reticulate.html#real-world-applications",
    "title": "Getting Started with Python using R and Reticulate",
    "section": "Real-World Applications",
    "text": "Real-World Applications\nHere are some ways to combine R and Python in your data science workflow:\n\nData Science Pipeline\n\n# 1. Data cleaning with R's tidyverse\nlibrary(readr)\nclean_data &lt;- read_csv(\"data.csv\") %&gt;%\n  filter(!is.na(important_column)) %&gt;%\n  mutate(new_feature = feature1 / feature2)\n\n# 2. Machine learning with Python's scikit-learn\nsk &lt;- import(\"sklearn.ensemble\")\nmodel &lt;- sk$RandomForestClassifier(n_estimators=100)\nmodel$fit(X = as.matrix(clean_data[, features]), \n         y = clean_data$target)\n\n# 3. Visualization with R's ggplot2\npredictions &lt;- model$predict_proba(as.matrix(clean_data[, features]))[,2]\nclean_data %&gt;%\n  mutate(prediction = predictions) %&gt;%\n  ggplot(aes(x=feature1, y=feature2, color=prediction)) +\n  geom_point() +\n  scale_color_viridis_c()\n\n\n\nThe Choice Between R and Python\nUse R for:\n\nData manipulation with dplyr/data.table\nStatistical modeling and hypothesis testing\nPublication-quality visualization\nInteractive reports and dashboards\n\nUse Python for:\n\nDeep learning with TensorFlow/PyTorch\nNatural language processing\nComputer vision\nAdvanced machine learning algorithms\n\nHowever, with reticulate, you don’t have to choose! Use the best tool for each part of your analysis!"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html",
    "href": "posts/measuring-model-performance-gains-table.html",
    "title": "Evaluating Binary Classification Models Using Gains Tables",
    "section": "",
    "text": "In credit risk modeling and binary classification applications, analysts employ gains tables (also known as KS tables) as a fundamental tool for measuring and quantifying model performance. This tutorial dives into the construction and interpretion of gains tables using R. ## Theoretical Foundation: Understanding Gains Tables\nA gains table systematically discretizes the population (typically a validation or test dataset) into groups based on the model’s output predictions (probability scores, log odds, or risk scores). Each group conventionally represents 10% of the total population (deciles), though alternative binning strategies may be employed. The output presents summary statistics for each group and analyzes the cumulative distributions of events (defaults) and non-events to quantify the model’s discriminatory performance."
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#what-is-a-gains-table",
    "href": "posts/measuring-model-performance-gains-table.html#what-is-a-gains-table",
    "title": "Measuring Model Performance Using a Gains Table",
    "section": "",
    "text": "A gains table discretizes the population (typically a test or validation set) into groups based on the model’s output (probability, log odds, or scores). Usually, each group represents 10% of the total population (deciles). The table then presents summary statistics for each group and analyzes the cumulative distributions of events (defaults) and non-events to quantify the model’s performance."
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#required-libraries",
    "href": "posts/measuring-model-performance-gains-table.html#required-libraries",
    "title": "Measuring Model Performance Using a Gains Table",
    "section": "",
    "text": "# Load required packages\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(knitr)\nlibrary(scales)"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#sample-dataset",
    "href": "posts/measuring-model-performance-gains-table.html#sample-dataset",
    "title": "Measuring Model Performance Using a Gains Table",
    "section": "",
    "text": "We’ll use a sample from the Lending Club dataset, which contains information about loans and their outcomes.\n\n# Load the sample data\nsample &lt;- read.csv(\"https://bit.ly/42ypcnJ\")\n\n# Check dimensions\ndim(sample)\n\n[1] 10000   153"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#defining-the-target-variable",
    "href": "posts/measuring-model-performance-gains-table.html#defining-the-target-variable",
    "title": "Measuring Model Performance Using a Gains Table",
    "section": "",
    "text": "First, we need to create a target (outcome) variable to model. Since this is a credit risk use case, we’ll identify borrowers who defaulted on their payments.\n\n# Check unique loan statuses\nunique(sample$loan_status)\n\n[1] \"Fully Paid\"                                         \n[2] \"Current\"                                            \n[3] \"Charged Off\"                                        \n[4] \"Late (31-120 days)\"                                 \n[5] \"Late (16-30 days)\"                                  \n[6] \"In Grace Period\"                                    \n[7] \"Does not meet the credit policy. Status:Fully Paid\" \n[8] \"Does not meet the credit policy. Status:Charged Off\"\n\n# Define \"bad\" loans as those that are charged off\ncodes &lt;- c(\"Charged Off\", \"Does not meet the credit policy. Status:Charged Off\")\n\n# Create a binary flag for defaults\nsample %&lt;&gt;% mutate(bad_flag = ifelse(loan_status %in% codes, 1, 0))\n\n# Check overall event rates\nsample %&gt;% \n  summarise(events = sum(bad_flag == 1), \n            non_events = sum(bad_flag == 0)) %&gt;% \n  mutate(event_rate = events/(events + non_events))\n\n  events non_events event_rate\n1   1162       8838     0.1162"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#building-a-simple-model",
    "href": "posts/measuring-model-performance-gains-table.html#building-a-simple-model",
    "title": "Measuring Model Performance Using a Gains Table",
    "section": "",
    "text": "Next, let’s build a quick model, the output of which we’ll use to create the gains table.\n\n# Replace NA values with a default value\nsample[is.na(sample)] &lt;- -1\n\n# Clean the data\nsample %&lt;&gt;% \n  # Remove cases where home ownership and payment plan are not reported\n  filter(!home_ownership %in% c(\"\", \"NONE\"),\n         pymnt_plan != \"\") %&gt;% \n  # Convert categorical variables to factors\n  mutate(home_ownership = factor(home_ownership), \n         pymnt_plan = factor(pymnt_plan))\n\n# Train-test split (70-30)\nidx &lt;- sample(1:nrow(sample), size = 0.7 * nrow(sample), replace = FALSE)\ntrain &lt;- sample[idx,]\ntest &lt;- sample[-idx,]\n\n\n# Build a logistic regression model\nmdl &lt;- glm(\n  formula = bad_flag ~ \n    loan_amnt + term + mths_since_last_delinq + total_pymnt + \n    home_ownership + acc_now_delinq + \n    inq_last_6mths + delinq_amnt + \n    mths_since_last_record + mths_since_recent_revol_delinq + \n    mths_since_last_major_derog + mths_since_recent_inq + \n    mths_since_recent_bc + num_accts_ever_120_pd,\n  family = \"binomial\", \n  data = train\n)\n\n# Generate predictions on the test set\ntest$pred &lt;- predict(mdl, newdata = test)"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#creating-the-gains-table",
    "href": "posts/measuring-model-performance-gains-table.html#creating-the-gains-table",
    "title": "Measuring Model Performance Using a Gains Table",
    "section": "",
    "text": "Now let’s build the gains table step by step:\n\n\n\n# Create deciles based on model predictions\nq &lt;- quantile(test$pred, probs = seq(0, 1, length.out = 11))\n\n# Add bins to test dataset\ntest$bins &lt;- cut(test$pred, breaks = q, include.lowest = TRUE, \n                right = TRUE, ordered_result = TRUE)\n\n# Check the bin levels (note they're in increasing order)\nlevels(test$bins)\n\n [1] \"[-5.53,-3.44]\" \"(-3.44,-2.95]\" \"(-2.95,-2.66]\" \"(-2.66,-2.45]\"\n [5] \"(-2.45,-2.25]\" \"(-2.25,-2.08]\" \"(-2.08,-1.86]\" \"(-1.86,-1.6]\" \n [9] \"(-1.6,-1.19]\"  \"(-1.19,6.36]\" \n\n\n\n\n\n\n# Create initial gains table with counts\ngains_table &lt;- test %&gt;% \n  group_by(bins) %&gt;% \n  summarise(total = n(), \n            events = sum(bad_flag == 1), \n            non_events = sum(bad_flag == 0))\n\n# Add event rate column\ngains_table %&lt;&gt;%\n  mutate(event_rate = percent(events / total, 0.1, 100))\n\n# Display the table\nkable(gains_table)\n\n\n\n\nbins\ntotal\nevents\nnon_events\nevent_rate\n\n\n\n\n[-5.53,-3.44]\n300\n5\n295\n1.7%\n\n\n(-3.44,-2.95]\n300\n13\n287\n4.3%\n\n\n(-2.95,-2.66]\n300\n14\n286\n4.7%\n\n\n(-2.66,-2.45]\n300\n12\n288\n4.0%\n\n\n(-2.45,-2.25]\n300\n20\n280\n6.7%\n\n\n(-2.25,-2.08]\n300\n39\n261\n13.0%\n\n\n(-2.08,-1.86]\n300\n53\n247\n17.7%\n\n\n(-1.86,-1.6]\n300\n59\n241\n19.7%\n\n\n(-1.6,-1.19]\n300\n72\n228\n24.0%\n\n\n(-1.19,6.36]\n300\n70\n230\n23.3%\n\n\n\n\n\n\n\n\n\n# Add population percentage and cumulative distributions\ngains_table %&lt;&gt;%\n  mutate(pop_pct = percent(total/sum(total), 0.1, 100), \n         \n         # Calculate cumulative percentages\n         c.events_pct = cumsum(events) / sum(events),\n         c.non_events_pct = cumsum(non_events) / sum(non_events))\n\n# Display the updated table\nkable(gains_table)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbins\ntotal\nevents\nnon_events\nevent_rate\npop_pct\nc.events_pct\nc.non_events_pct\n\n\n\n\n[-5.53,-3.44]\n300\n5\n295\n1.7%\n10.0%\n0.0140056\n0.1116156\n\n\n(-3.44,-2.95]\n300\n13\n287\n4.3%\n10.0%\n0.0504202\n0.2202043\n\n\n(-2.95,-2.66]\n300\n14\n286\n4.7%\n10.0%\n0.0896359\n0.3284147\n\n\n(-2.66,-2.45]\n300\n12\n288\n4.0%\n10.0%\n0.1232493\n0.4373818\n\n\n(-2.45,-2.25]\n300\n20\n280\n6.7%\n10.0%\n0.1792717\n0.5433220\n\n\n(-2.25,-2.08]\n300\n39\n261\n13.0%\n10.0%\n0.2885154\n0.6420734\n\n\n(-2.08,-1.86]\n300\n53\n247\n17.7%\n10.0%\n0.4369748\n0.7355278\n\n\n(-1.86,-1.6]\n300\n59\n241\n19.7%\n10.0%\n0.6022409\n0.8267121\n\n\n(-1.6,-1.19]\n300\n72\n228\n24.0%\n10.0%\n0.8039216\n0.9129777\n\n\n(-1.19,6.36]\n300\n70\n230\n23.3%\n10.0%\n1.0000000\n1.0000000\n\n\n\n\n\n\n\n\n\n# Add KS statistic, capture rate, and cumulative event rate\ngains_table %&lt;&gt;%\n  mutate(\n    # KS statistic (difference between cumulative distributions)\n    ks = round(abs(c.events_pct - c.non_events_pct), 2), \n    \n    # Capture rate (percentage of total events captured)\n    cap_rate = percent(cumsum(events)/sum(events), 1, 100), \n    \n    # Cumulative event rate\n    c_event_rate = percent(cumsum(events)/cumsum(total), 0.1, 100), \n    \n    # Format percentage columns\n    c.events_pct = percent(c.events_pct, 0.1, 100),\n    c.non_events_pct = percent(c.non_events_pct, 0.1, 100))\n\n# Display the final table\nkable(gains_table)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbins\ntotal\nevents\nnon_events\nevent_rate\npop_pct\nc.events_pct\nc.non_events_pct\nks\ncap_rate\nc_event_rate\n\n\n\n\n[-5.53,-3.44]\n300\n5\n295\n1.7%\n10.0%\n1.4%\n11.2%\n0.10\n1%\n1.7%\n\n\n(-3.44,-2.95]\n300\n13\n287\n4.3%\n10.0%\n5.0%\n22.0%\n0.17\n5%\n3.0%\n\n\n(-2.95,-2.66]\n300\n14\n286\n4.7%\n10.0%\n9.0%\n32.8%\n0.24\n9%\n3.6%\n\n\n(-2.66,-2.45]\n300\n12\n288\n4.0%\n10.0%\n12.3%\n43.7%\n0.31\n12%\n3.7%\n\n\n(-2.45,-2.25]\n300\n20\n280\n6.7%\n10.0%\n17.9%\n54.3%\n0.36\n18%\n4.3%\n\n\n(-2.25,-2.08]\n300\n39\n261\n13.0%\n10.0%\n28.9%\n64.2%\n0.35\n29%\n5.7%\n\n\n(-2.08,-1.86]\n300\n53\n247\n17.7%\n10.0%\n43.7%\n73.6%\n0.30\n44%\n7.4%\n\n\n(-1.86,-1.6]\n300\n59\n241\n19.7%\n10.0%\n60.2%\n82.7%\n0.22\n60%\n9.0%\n\n\n(-1.6,-1.19]\n300\n72\n228\n24.0%\n10.0%\n80.4%\n91.3%\n0.11\n80%\n10.6%\n\n\n(-1.19,6.36]\n300\n70\n230\n23.3%\n10.0%\n100.0%\n100.0%\n0.00\n100%\n11.9%"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#creating-a-reusable-function",
    "href": "posts/measuring-model-performance-gains-table.html#creating-a-reusable-function",
    "title": "Measuring Model Performance Using a Gains Table",
    "section": "",
    "text": "Let’s encapsulate all the above steps into a single function that can be reused for any binary classification model:\n\ngains_table &lt;- function(act, pred, increasing = TRUE, nBins = 10) {\n  \n  # Create bins based on predictions\n  q &lt;- quantile(pred, probs = seq(0, 1, length.out = nBins + 1))\n  bins &lt;- cut(pred, breaks = q, include.lowest = TRUE, right = TRUE, ordered_result = TRUE)\n  \n  df &lt;- data.frame(act, pred, bins)\n  \n  df %&gt;% \n    # Group by bins and calculate statistics\n    group_by(bins) %&gt;% \n    summarise(total = n(), \n              events = sum(act == 1), \n              non_events = sum(act == 0)) %&gt;% \n    mutate(event_rate = percent(events / total, 0.1, 100)) %&gt;% \n    \n    # Sort the table based on the 'increasing' parameter\n    {if(increasing == TRUE) {\n      arrange(., bins)\n    } else {\n      arrange(., desc(bins))\n    }} %&gt;% \n    \n    # Add all performance metrics\n    mutate(pop_pct = percent(total/sum(total), 0.1, 100), \n           c.events_pct = cumsum(events) / sum(events),\n           c.non_events_pct = cumsum(non_events) / sum(non_events), \n           ks = round(abs(c.events_pct - c.non_events_pct), 2), \n           cap_rate = percent(cumsum(events)/sum(events), 1, 100), \n           c_event_rate = percent(cumsum(events)/cumsum(total), 0.1, 100), \n           c.events_pct = percent(c.events_pct, 0.1, 100),\n           c.non_events_pct = percent(c.non_events_pct, 0.1, 100))\n}\n\n\n\n\n# Generate a gains table with bins in descending order\ntab &lt;- gains_table(test$bad_flag, test$pred, FALSE, 10)\nkable(tab)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbins\ntotal\nevents\nnon_events\nevent_rate\npop_pct\nc.events_pct\nc.non_events_pct\nks\ncap_rate\nc_event_rate\n\n\n\n\n(-1.19,6.36]\n300\n70\n230\n23.3%\n10.0%\n19.6%\n8.7%\n0.11\n20%\n23.3%\n\n\n(-1.6,-1.19]\n300\n72\n228\n24.0%\n10.0%\n39.8%\n17.3%\n0.22\n40%\n23.7%\n\n\n(-1.86,-1.6]\n300\n59\n241\n19.7%\n10.0%\n56.3%\n26.4%\n0.30\n56%\n22.3%\n\n\n(-2.08,-1.86]\n300\n53\n247\n17.7%\n10.0%\n71.1%\n35.8%\n0.35\n71%\n21.2%\n\n\n(-2.25,-2.08]\n300\n39\n261\n13.0%\n10.0%\n82.1%\n45.7%\n0.36\n82%\n19.5%\n\n\n(-2.45,-2.25]\n300\n20\n280\n6.7%\n10.0%\n87.7%\n56.3%\n0.31\n88%\n17.4%\n\n\n(-2.66,-2.45]\n300\n12\n288\n4.0%\n10.0%\n91.0%\n67.2%\n0.24\n91%\n15.5%\n\n\n(-2.95,-2.66]\n300\n14\n286\n4.7%\n10.0%\n95.0%\n78.0%\n0.17\n95%\n14.1%\n\n\n(-3.44,-2.95]\n300\n13\n287\n4.3%\n10.0%\n98.6%\n88.8%\n0.10\n99%\n13.0%\n\n\n[-5.53,-3.44]\n300\n5\n295\n1.7%\n10.0%\n100.0%\n100.0%\n0.00\n100%\n11.9%"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#interpreting-the-gains-table",
    "href": "posts/measuring-model-performance-gains-table.html#interpreting-the-gains-table",
    "title": "Measuring Model Performance Using a Gains Table",
    "section": "",
    "text": "A gains table provides several key insights into model performance:\n\nMonotonicity: The event rates should consistently increase (or decrease) across bins. This confirms that the model effectively rank-orders risk.\nBin Consistency: If bin sizes are not consistent (ideally ~10% each), it suggests the model is assigning the same output/score to many borrowers (clumping), which could pose issues when deciding cutoffs.\nKS Statistic: The maximum value of the KS column indicates the model’s discriminatory power. A higher value (closer to 1) indicates better separation between good and bad borrowers.\nCapture Rate: Shows what percentage of all bad accounts are captured at each cutoff point.\nCumulative Event Rate: Indicates the bad rate among all accounts up to that bin, useful for setting approval thresholds."
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#practical-applications",
    "href": "posts/measuring-model-performance-gains-table.html#practical-applications",
    "title": "Tutorial: Evaluating Binary Classification Models Using Gains Tables",
    "section": "Practical Applications",
    "text": "Practical Applications\nIn credit risk management, the gains table helps with:\n\nSetting Cutoffs: Identifying appropriate score thresholds for approving or rejecting applications.\nStrategy Development: Creating tiered strategies (e.g., approve, review, decline) based on risk levels.\nPerformance Monitoring: Tracking model performance over time by comparing actual vs. expected distributions.\nModel Comparison: Evaluating different models by comparing their KS statistics and capture rates.\n\nThe gains table is a powerful tool for evaluating binary classification models, especially in credit risk applications. By providing a structured view of how well a model separates good and bad cases across the score distribution, it helps analysts make informed decisions about model quality and operational implementation."
  },
  {
    "objectID": "posts/particle-swarm-optimization.html",
    "href": "posts/particle-swarm-optimization.html",
    "title": "Portfolio Optimization Using PSO",
    "section": "",
    "text": "Portfolio optimization represents a critical task in investment management, where the goal involves allocating capital across different assets to maximize returns while controlling risk. This post explores how to use Particle Swarm Optimization (PSO) to perform mean-variance portfolio optimization with various constraints."
  },
  {
    "objectID": "posts/particle-swarm-optimization.html#required-libraries",
    "href": "posts/particle-swarm-optimization.html#required-libraries",
    "title": "Portfolio Optimization Using Particle Swarm Optimization in R",
    "section": "",
    "text": "Before we begin, let’s load the R packages we’ll need for this analysis:\n\n# Load required packages\nlibrary(pso)       # For PSO implementation (provides psoptim function)\nlibrary(ggplot2)   # For data visualization\nlibrary(dplyr)     # For data manipulation and transformation\nlibrary(quantmod)  # For downloading financial data\nlibrary(tidyr)     # For reshaping data (pivot_wider, gather functions)\nlibrary(plotly)    # For creating interactive 3D visualizations"
  },
  {
    "objectID": "posts/particle-swarm-optimization.html#data-collection-and-preparation",
    "href": "posts/particle-swarm-optimization.html#data-collection-and-preparation",
    "title": "Portfolio Optimization Using PSO",
    "section": "Data Collection and Preparation",
    "text": "Data Collection and Preparation\nThe first step in portfolio optimization involves gathering the necessary data. Historical price data is required to calculate returns and risk metrics.\n\nGetting Stock Tickers\nThis demonstration utilizes stocks from the NIFTY50 index, which includes the 50 largest Indian companies by market capitalization:\n\n# Read ticker list from NSE (National Stock Exchange of India) website\nticker_list &lt;- read.csv(\"https://raw.githubusercontent.com/royr2/datasets/refs/heads/main/ind_nifty50list.csv\")\n\n# View the first few rows to understand the data structure\nhead(ticker_list[,1:3], 5)\n\n                                Company.Name           Industry     Symbol\n1                     Adani Enterprises Ltd.    Metals & Mining   ADANIENT\n2 Adani Ports and Special Economic Zone Ltd.           Services ADANIPORTS\n3           Apollo Hospitals Enterprise Ltd.         Healthcare APOLLOHOSP\n4                          Asian Paints Ltd.  Consumer Durables ASIANPAINT\n5                             Axis Bank Ltd. Financial Services   AXISBANK\n\n\n\n\nDownloading Historical Price Data\nThe next step involves downloading historical price data for these stocks using the quantmod package, which provides an interface to Yahoo Finance:\n\n# Append \".NS\" to tickers for Yahoo Finance format (NS = National Stock Exchange)\ntickers &lt;- paste0(ticker_list$Symbol, \".NS\")\ntickers &lt;- tickers[!tickers %in% c(\"ETERNAL.NS\", \"JIOFIN.NS\")]\n\n# Initialize empty dataframe to store all ticker data\nticker_df &lt;- data.frame()\n\n# Create a progress bar to monitor the download process\n# pb &lt;- txtProgressBar(min = 1, max = length(tickers), style = 3)\n\n# Loop through each ticker and download its historical data\nfor(nms in tickers){\n  # Download data from Yahoo Finance\n  df &lt;- getSymbols(Symbols = nms, verbose = FALSE, auto.assign = FALSE)\n  \n  # Rename columns for clarity\n  colnames(df) &lt;- c(\"open\", \"high\", \"low\", \"close\", \"volume\", \"adjusted\")\n  df$date = rownames(df)\n  \n  # Convert to dataframe and add ticker and date information\n  df &lt;- data.frame(df)\n  df$ticker &lt;- nms\n  df$date &lt;- rownames(df)\n  \n  # Append to the main dataframe\n  ticker_df &lt;- rbind(ticker_df, df)\n  \n  Sys.sleep(0.2)\n  \n  # Update progress bar\n  # setTxtProgressBar(pb, which(tickers == nms))\n}\n\n# Reshape data to wide format with dates as rows and tickers as columns\n# This format facilitates the calculation of returns across all stocks\nprices_df &lt;- pivot_wider(data = ticker_df, id_cols = \"date\", names_from = \"ticker\", values_from = \"close\")\n\n# Remove rows with missing values to ensure complete data\nprices_df &lt;- na.omit(prices_df)\n\n# Check the date range of our data\nrange(prices_df$date)\n\n[1] \"2017-11-17\" \"2025-06-20\"\n\n# Check dimensions (number of trading days × number of stocks + date column)\ndim(prices_df)\n\n[1] 1874   49\n\n\n\n\nVisualizing the Data\nBefore proceeding with analysis, examining the data through visualization helps identify anomalies and understand general trends. The following visualization displays the price data for a subset of stocks (focusing on the metals industry):\n\n# Plot closing prices for metal stocks\nprices_df %&gt;% \n  # Convert from wide to long format for easier plotting with ggplot2\n  pivot_longer(-date, names_to = \"ticker\", values_to = \"price\") %&gt;% \n  \n  # Attach industry information from our original ticker list\n  left_join(ticker_list %&gt;% \n              mutate(ticker = paste0(Symbol, \".NS\")) %&gt;% \n              select(ticker, industry = Industry),\n            by = \"ticker\") %&gt;% \n  \n  # Convert date strings to Date objects\n  mutate(date = as.Date(date)) %&gt;% \n  \n  # Filter to show only metal industry stocks for clarity\n  filter(stringr::str_detect(tolower(industry), \"metal\")) %&gt;% \n  \n  # Create the line plot\n  ggplot(aes(x = date, y = price, color = ticker)) + \n  geom_line(linewidth = 0.8) + \n  theme_minimal() + \n  scale_color_brewer(palette = \"RdBu\") +  # Use a color-blind friendly palette\n  labs(title = \"Closing Prices\", \n       subtitle = \"Nifty 50 metal stocks\",\n       x = \"Date\", \n       y = \"Closing Price\") + \n  theme(legend.position = \"top\", \n        legend.title = element_text(colour = \"transparent\"), \n        axis.title.x = element_text(face = \"bold\"), \n        axis.title.y = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\nClosing prices for metal stocks\n\n\nThe visualization demonstrates the price movements of metal stocks over time. The data reveals periods of both correlation and divergence between different stocks, highlighting the importance of diversification in portfolio construction.\n\n\nCalculating Returns\nFor portfolio optimization, we need to work with returns rather than prices. Returns better represent the investment performance and have more desirable statistical properties (like stationarity):\n\n# Calculate daily returns for all stocks\n# Formula: (Price_today / Price_yesterday) - 1\nreturns_df &lt;- apply(prices_df[,-1], 2, function(vec){\n  ret &lt;- vec/lag(vec) - 1  # Simple returns calculation\n  return(ret)\n})\n\n# Convert to dataframe for easier manipulation\nreturns_df &lt;- as.data.frame(returns_df)\n\n# Remove first row which contains NA values (no previous day to calculate return)\nreturns_df &lt;- returns_df[-1,]  \n\n# Pre-compute average returns and covariance matrix for optimization\n# These constitute key inputs to the mean-variance optimization\nmean_returns &lt;- sapply(returns_df, mean)  # Expected returns\ncov_mat &lt;- cov(returns_df)  # Risk (covariance) matrix\n\nThe mean returns represent expectations for each asset’s performance, while the covariance matrix captures both the individual volatilities and the relationships between assets. These serve as the primary inputs to the optimization process."
  },
  {
    "objectID": "posts/particle-swarm-optimization.html#portfolio-optimization-framework",
    "href": "posts/particle-swarm-optimization.html#portfolio-optimization-framework",
    "title": "Portfolio Optimization Using PSO",
    "section": "Portfolio Optimization Framework",
    "text": "Portfolio Optimization Framework\n\nObjective Function\nThe core of portfolio optimization is the objective function, which defines the optimization target. In mean-variance optimization, the approach balances three key components:\n\nExpected returns (reward): The weighted average of expected returns for each asset\nPortfolio variance (risk): A measure of the portfolio’s volatility, calculated using the covariance matrix\nRisk aversion parameter: Controls the trade-off between risk and return (higher values prioritize risk reduction)\n\nThe implementation also incorporates constraints through penalty terms:\n\nobj_func &lt;- function(wts, \n                     risk_av = 10,  # Risk aversion parameter\n                     lambda1 = 10,  # Penalty weight for full investment constraint\n                     lambda2 = 1e2,  # Reserved for additional constraints\n                     ret_vec, cov_mat){\n  \n  # Calculate expected portfolio return (weighted average of asset returns)\n  port_returns &lt;- ret_vec %*% wts\n  \n  # Calculate portfolio risk (quadratic form using covariance matrix)\n  port_risk &lt;- t(wts) %*% cov_mat %*% wts\n  \n  # Mean-variance utility function: return - risk_aversion * risk\n  # This is the core Markowitz portfolio optimization formula\n  obj &lt;- port_returns - risk_av * port_risk\n  \n  # Add penalty for violating the full investment constraint (sum of weights = 1)\n  # The squared term ensures the penalty increases quadratically with violation size\n  obj &lt;- obj - lambda1 * (sum(wts) - 1)^2\n    # Return negative value since PSO minimizes by default, but the goal is to maximize\n  # the objective (higher returns, lower risk)\n  return(-obj)\n}\n\nThis objective function implements the classic mean-variance utility with a quadratic penalty for the full investment constraint. The risk aversion parameter allows us to move along the efficient frontier to find portfolios with different risk-return profiles."
  },
  {
    "objectID": "posts/particle-swarm-optimization.html#two-asset-example",
    "href": "posts/particle-swarm-optimization.html#two-asset-example",
    "title": "Portfolio Optimization Using PSO",
    "section": "Two-Asset Example",
    "text": "Two-Asset Example\nBefore tackling the full portfolio optimization problem, this section begins with a simple two-asset example. This approach helps visualize how PSO works and validates the methodology:\n\n# Use only the first two assets for this example\n# Calculate their average returns and covariance matrix\nmean_returns_small &lt;- apply(returns_df[,1:2], 2, mean)\ncov_mat_small &lt;- cov(returns_df[,1:2])\n\n# Define a custom PSO optimizer function to track the optimization process\npso_optim &lt;- function(obj_func,\n                      c1 = 0.05,      # Cognitive parameter (personal best influence)\n                      c2 = 0.05,      # Social parameter (global best influence)\n                      w = 0.8,        # Inertia weight (controls momentum)\n                      init_fact = 0.1, # Initial velocity factor\n                      n_particles = 20, # Number of particles in the swarm\n                      n_dim = 2,       # Dimensionality (number of assets)\n                      n_iter = 50,     # Maximum iterations\n                      upper = 1,       # Upper bound for weights\n                      lower = 0,       # Lower bound for weights (no short selling)\n                      n_avg = 10,      # Number of iterations for averaging\n                      ...){\n  \n  # Initialize particle positions randomly within bounds\n  X &lt;- matrix(runif(n_particles * n_dim), nrow = n_particles)\n  X &lt;- X * (upper - lower) + lower  # Scale to fit within bounds\n  \n  # Initialize particle velocities (movement speeds)\n  dX &lt;- matrix(runif(n_particles * n_dim) * init_fact, ncol = n_dim)\n  dX &lt;- dX * (upper - lower) + lower\n  \n  # Initialize personal best positions and objective values\n  pbest &lt;- X  # Each particle's best position so far\n  pbest_obj &lt;- apply(X, 1, obj_func, ...)  # Objective value at personal best\n  \n  # Initialize global best position and objective value\n  gbest &lt;- pbest[which.min(pbest_obj),]  # Best position across all particles\n  gbest_obj &lt;- min(pbest_obj)  # Best objective value found\n  \n  # Store initial positions for visualization\n  loc_df &lt;- data.frame(X, iter = 0, obj = pbest_obj)\n  iter &lt;- 1\n  \n  # Main PSO loop\n  while(iter &lt; n_iter){\n    \n    # Update velocities using PSO formula:\n    # New velocity = inertia + cognitive component + social component\n    dX &lt;- w * dX +                         # Inertia (continue in same direction)\n          c1*runif(1)*(pbest - X) +        # Pull toward personal best\n          c2*runif(1)*t(gbest - t(X))      # Pull toward global best\n    \n    # Update positions based on velocities\n    X &lt;- X + dX\n    \n    # Evaluate objective function at new positions\n    obj &lt;- apply(X, 1, obj_func, ...)\n    \n    # Update personal bests if new positions are better\n    idx &lt;- which(obj &lt;= pbest_obj)\n    pbest[idx,] &lt;- X[idx,]\n    pbest_obj[idx] &lt;- obj[idx]\n    \n    # Update global best if a better solution is found\n    idx &lt;- which.min(pbest_obj)\n    gbest &lt;- pbest[idx,]\n    gbest_obj &lt;- min(pbest_obj)\n    \n    # Store current state for visualization\n    iter &lt;- iter + 1\n    loc_df &lt;- rbind(loc_df, data.frame(X, iter = iter, obj = pbest_obj))\n  }\n  \n  # Return optimization results\n  lst &lt;- list(X = loc_df,          # All particle positions throughout optimization\n              obj = gbest_obj,     # Best objective value found\n              obj_loc = gbest)     # Weights that achieved the best objective\n  return(lst)\n}\n\n# Run the optimization for our two-asset portfolio\nout &lt;- pso_optim(obj_func,\n                 ret_vec = mean_returns_small,  # Expected returns\n                 cov_mat = cov_mat_small,       # Covariance matrix\n                 lambda1 = 10, risk_av = 100,    # Constraint and risk parameters\n                 n_particles = 100,              # Use 100 particles for better coverage\n                 n_dim = 2,                      # Two-asset portfolio\n                 n_iter = 200,                   # Run for 200 iterations\n                 upper = 1, lower = 0,           # Bounds for weights\n                 c1 = 0.02, c2 = 0.02,           # Lower influence parameters for stability\n                 w = 0.05, init_fact = 0.01)     # Low inertia for better convergence\n\n# Verify that the weights sum to approximately 1 (full investment constraint)\nsum(out$obj_loc)\n\n[1] 0.9963105\n\n\nIn this implementation, the tracking of all particle movements throughout the optimization process occurs. This enables visualization of how the swarm converges toward the optimal solution.\n\nVisualizing the Optimization Process\nOne advantage of starting with a two-asset example is the ability to visualize the entire search space and observe how the PSO algorithm explores it. The following creates a 3D visualization of the objective function landscape and the path each particle took during optimization:\n\n# Create a fine grid of points covering the feasible region (all possible weight combinations)\ngrid &lt;- expand.grid(x = seq(0, 1, by = 0.01),  # First asset weight from 0 to 1\n                    y = seq(0, 1, by = 0.01))   # Second asset weight from 0 to 1\n\n# Evaluate the objective function at each grid point to create the landscape\ngrid$obj &lt;- apply(grid, 1, obj_func, \n                  ret_vec = mean_returns_small, \n                  cov_mat = cov_mat_small, \n                  lambda1 = 10, risk_av = 100)\n\n# Create an interactive 3D plot showing both the objective function surface\n# and the particle trajectories throughout the optimization\np &lt;- plot_ly() %&gt;% \n  # Add the objective function surface as a mesh\n  add_mesh(data = grid, x = ~x, y = ~y, z = ~obj, \n           inherit = FALSE, color = \"red\") %&gt;% \n  \n  # Add particles as markers, colored by iteration to show progression\n  add_markers(data = out$X, x = ~X1, y = ~X2, z = ~obj, \n              color = ~ iter, inherit = FALSE, \n              marker = list(size = 2))\n\nThis visualization demonstrates: 1. The objective function landscape as a 3D surface 2. The particles (small dots) exploring the search space 3. How the swarm converges toward the optimal solution over iterations (color gradient)\nThe concentration of particles in certain regions indicates where the algorithm found promising solutions. The global best solution represents where the particles ultimately converge."
  },
  {
    "objectID": "posts/particle-swarm-optimization.html#multi-asset-portfolio-optimization",
    "href": "posts/particle-swarm-optimization.html#multi-asset-portfolio-optimization",
    "title": "Portfolio Optimization Using PSO",
    "section": "Multi-Asset Portfolio Optimization",
    "text": "Multi-Asset Portfolio Optimization\nWith the basic principles understood, the analysis now scales up to optimize a portfolio containing all the assets in the dataset. Instead of using the custom PSO implementation, this section leverages the more efficient psoptim function from the pso package:\n\n# Get the number of stocks in the dataset\nn_stocks &lt;- ncol(returns_df)\n\n# Run the PSO optimization for the full portfolio\nopt &lt;- psoptim(\n  # Initial particle positions (starting with equal weights)\n  par = rep(0, n_stocks),\n  \n  # Objective function to minimize\n  fn = obj_func,\n  \n  # Pass the expected returns and covariance matrix\n  ret_vec = mean_returns, \n  cov_mat = cov_mat,\n  \n  # Set constraint parameters\n  lambda1 = 10,  # Weight for full investment constraint\n  risk_av = 1000,  # Higher risk aversion for a more conservative portfolio\n  \n  # Set bounds for weights (no short selling allowed)\n  lower = rep(0, n_stocks),\n  upper = rep(1, n_stocks),\n  \n  # Configure the PSO algorithm\n  control = list(\n    maxit = 200,          # Maximum iterations\n    s = 100,               # Swarm size (number of particles)\n    maxit.stagnate = 500   # Stop if no improvement after this many iterations\n  )\n)\n\n# Calculate and display the expected return of the optimized portfolio\npaste(\"Portfolio returns:\", round(opt$par %*% mean_returns, 5))\n\n[1] \"Portfolio returns: 0.00065\"\n\n# Calculate and display the standard deviation (risk) of the optimized portfolio\npaste(\"Portfolio Std dev:\", round(sqrt(opt$par %*% cov_mat %*% opt$par), 5))\n\n[1] \"Portfolio Std dev: 0.00914\"\n\n# Verify that the weights sum to approximately 1 (full investment constraint)\nsum(opt$par)\n\n[1] 0.9923503\n\n\nThe optimization has identified a portfolio allocation that balances return and risk according to the specified risk aversion parameter. The high risk aversion value (1000) indicates prioritization of risk reduction over return maximization."
  },
  {
    "objectID": "posts/particle-swarm-optimization.html#adding-tracking-error-constraint",
    "href": "posts/particle-swarm-optimization.html#adding-tracking-error-constraint",
    "title": "Portfolio Optimization Using PSO",
    "section": "Adding Tracking Error Constraint",
    "text": "Adding Tracking Error Constraint\nOne advantage of PSO is its flexibility in handling various constraints. The following demonstrates this by adding a tracking error constraint, which is common in institutional portfolio management. Tracking error measures how closely a portfolio follows a benchmark:\n\n# Define benchmark portfolio (equally weighted across all stocks)\nbench_wts &lt;- rep(1/n_stocks, n_stocks)\n\n# Calculate the time series of benchmark returns\nbench_returns &lt;- as.matrix(returns_df) %*% t(t(bench_wts))\n\n# Create a new objective function that includes tracking error\nobj_func_TE &lt;- function(wts,  \n                        risk_av = 10,     # Risk aversion parameter\n                        lambda1 = 10,    # Full investment constraint weight\n                        lambda2 = 50,    # Tracking error constraint weight\n                        ret_vec, cov_mat){\n  \n  # Calculate portfolio metrics\n  port_returns &lt;- ret_vec %*% wts                      # Expected portfolio return\n  port_risk &lt;- t(wts) %*% cov_mat %*% wts             # Portfolio variance\n  port_returns_ts &lt;- as.matrix(returns_df) %*% t(t(wts))  # Time series of portfolio returns\n  \n  # Original mean-variance objective\n  obj &lt;- port_returns - risk_av * port_risk\n  \n  # Full investment constraint (weights sum to 1)\n  obj &lt;- obj - lambda1 * (sum(wts) - 1)^2\n  \n  # Tracking error constraint (penalize deviation from benchmark)\n  # Tracking error is measured as the standard deviation of the difference\n  # between portfolio returns and benchmark returns\n  obj &lt;- obj - lambda2 * sd(port_returns_ts - bench_returns)\n  \n  return(-obj)  # Return negative for minimization\n}\n\n# Run optimization with the tracking error constraint\nopt &lt;- psoptim(\n  # Initial particle positions\n  par = rep(0, n_stocks),\n  \n  # Use our new objective function with tracking error\n  fn = obj_func_TE,\n  \n  # Pass the expected returns and covariance matrix\n  ret_vec = mean_returns, \n  cov_mat = cov_mat,\n  \n  # Set constraint parameters\n  lambda1 = 10,    # Weight for full investment constraint\n  risk_av = 1000,  # Risk aversion parameter\n  \n  # Set bounds for weights\n  lower = rep(0, n_stocks),\n  upper = rep(1, n_stocks),\n  \n  # Configure the PSO algorithm\n  control = list(\n    maxit = 200,          # Maximum iterations\n    s = 100,               # Swarm size\n    maxit.stagnate = 500   # Stop if no improvement after this many iterations\n  )\n)\n\n# Calculate and display the expected return of the optimized portfolio\npaste(\"Portfolio returns:\", round(opt$par %*% mean_returns, 5))\n\n[1] \"Portfolio returns: 0.00074\"\n\n# Calculate and display the standard deviation (risk) of the optimized portfolio\npaste(\"Portfolio Std dev:\", round(sqrt(opt$par %*% cov_mat %*% opt$par), 5))\n\n[1] \"Portfolio Std dev: 0.01109\"\n\n# Verify that the weights sum to approximately 1\nsum(opt$par)\n\n[1] 0.9974361\n\n\nBy adding the tracking error constraint, the result is a portfolio that not only balances risk and return but also tracks the performance of an equally-weighted benchmark to a specified degree. The lambda2 parameter controls the closeness of benchmark tracking - higher values result in portfolios that more closely resemble the benchmark."
  },
  {
    "objectID": "posts/particle-swarm-optimization.html#advantages-and-limitations-of-pso",
    "href": "posts/particle-swarm-optimization.html#advantages-and-limitations-of-pso",
    "title": "Portfolio Optimization Using PSO",
    "section": "Advantages and Limitations of PSO",
    "text": "Advantages and Limitations of PSO\n\nAdvantages:\n\nFlexibility: PSO can handle non-convex, non-differentiable objective functions, making it suitable for complex portfolio constraints that traditional optimizers struggle with\nSimplicity: The algorithm is intuitive and relatively easy to implement compared to other global optimization techniques\nConstraints: Various constraints can be easily incorporated through penalty functions without reformulating the entire problem\nGlobal Search: PSO explores the search space more thoroughly and is less likely to get stuck in local optima compared to gradient-based methods\nParallelization: The algorithm is naturally parallelizable, as particles can be evaluated independently\n\n\n\nLimitations:\n\nVariability: Results can vary between runs due to the stochastic nature of the algorithm, potentially leading to inconsistent portfolio recommendations\nParameter Tuning: Performance significantly depends on parameters like inertia weight and acceleration coefficients, which may require careful tuning\nConvergence: There’s no mathematical guarantee of convergence to the global optimum, unlike some convex optimization methods\nComputational Cost: Can be computationally intensive for high-dimensional problems with many assets\nConstraint Handling: While flexible, the penalty function approach may not always satisfy constraints exactly"
  },
  {
    "objectID": "posts/particle-swarm-optimization.html#practical-applications",
    "href": "posts/particle-swarm-optimization.html#practical-applications",
    "title": "Portfolio Optimization Using PSO",
    "section": "Practical Applications",
    "text": "Practical Applications\nPSO-based portfolio optimization proves particularly valuable in scenarios where:\n\nTraditional quadratic programming approaches fail due to complex constraints\nThe objective function includes non-linear terms like higher moments (skewness, kurtosis)\nMultiple competing objectives need to be balanced\nThe portfolio needs to satisfy regulatory or client-specific constraints"
  },
  {
    "objectID": "posts/particle-swarm-optimization.html#conclusion",
    "href": "posts/particle-swarm-optimization.html#conclusion",
    "title": "Portfolio Optimization Using PSO",
    "section": "Conclusion",
    "text": "Conclusion\nParticle Swarm Optimization provides a powerful and flexible approach to portfolio optimization that can overcome many limitations of traditional methods. The algorithm can handle complex objective functions and constraints that might be difficult to solve with classical optimization techniques.\nThe approach demonstrated in this tutorial can be extended to include additional constraints such as:\n\nSector or industry exposure limits\nMaximum position sizes\nTurnover or transaction cost constraints\nRisk factor exposures and limits\nCardinality constraints (limiting the number of assets)\n\nFor more robust results in practice, practitioners should consider these enhancements:\n\nRun the algorithm multiple times with different random seeds and average the results\nImplement a hybrid approach that uses PSO for global exploration followed by a local optimizer for refinement\nAdd constraints gradually to better understand their impact on the portfolio\n\nPSO represents just one of many metaheuristic approaches that can be applied to portfolio optimization. Other techniques like genetic algorithms, simulated annealing, or differential evolution might also be worth exploring depending on specific requirements."
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html",
    "href": "posts/modeling-with-tidymodels.html",
    "title": "Building Models in R with tidymodels",
    "section": "",
    "text": "The tidymodels framework provides a cohesive set of packages for modeling and machine learning in R, following tidyverse principles. In this post, we’ll build a realistic credit scoring model using tidymodels.\nCredit scoring models are used by financial institutions to assess the creditworthiness of borrowers. These models predict the probability of default (failure to repay a loan) based on borrower characteristics and loan attributes. A good credit scoring model should effectively discriminate between high-risk and low-risk borrowers, be well-calibrated, and provide interpretable insights."
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#required-packages",
    "href": "posts/modeling-with-tidymodels.html#required-packages",
    "title": "Building Models in R with tidymodels",
    "section": "Required Packages",
    "text": "Required Packages\nFirst, let’s load all the required packages for our analysis:\n\nlibrary(tidymodels)\nlibrary(xgboost)\nlibrary(vip)        # For variable importance\nlibrary(stringr)    # For string manipulation functions\nlibrary(probably)   # For calibration plots\nlibrary(ROSE)       # For imbalanced data visualization\nlibrary(corrplot)   # For correlation visualization"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#creating-a-realistic-credit-scoring-dataset",
    "href": "posts/modeling-with-tidymodels.html#creating-a-realistic-credit-scoring-dataset",
    "title": "Tutorial: Comprehensive Credit Scoring Model Development Using tidymodels",
    "section": "Creating a Realistic Credit Scoring Dataset",
    "text": "Creating a Realistic Credit Scoring Dataset\nWe’ll simulate a realistic credit dataset with common variables found in credit scoring models. The data will include demographic information, loan characteristics, and credit history variables.\n\nset.seed(123)\nn &lt;- 10000  # Larger sample size for more realistic modeling\n\n# Create base features with realistic distributions\ndata &lt;- tibble(\n  customer_id = paste0(\"CUS\", formatC(1:n, width = 6, format = \"d\", flag = \"0\")),\n  \n  # Demographics - with realistic age distribution for credit applicants\n  age = pmax(18, pmin(80, round(rnorm(n, 38, 13)))),\n  income = pmax(12000, round(rlnorm(n, log(52000), 0.8))),\n  employment_length = pmax(0, round(rexp(n, 1/6))),  # Exponential distribution for job tenure\n  home_ownership = sample(c(\"RENT\", \"MORTGAGE\", \"OWN\"), n, replace = TRUE, prob = c(0.45, 0.40, 0.15)),\n  \n  # Loan characteristics - with more realistic correlations\n  loan_amount = round(rlnorm(n, log(15000), 0.7) / 100) * 100,  # Log-normal for loan amounts\n  loan_term = sample(c(36, 60, 120), n, replace = TRUE, prob = c(0.6, 0.3, 0.1)),\n  \n  # Credit history - with more realistic distributions\n  credit_score = round(pmin(850, pmax(300, rnorm(n, 700, 90)))),\n  dti_ratio = pmax(0, pmin(65, rlnorm(n, log(20), 0.4))),  # Debt-to-income ratio\n  delinq_2yrs = rpois(n, 0.4),  # Number of delinquencies in past 2 years\n  inq_last_6mths = rpois(n, 0.7),  # Number of inquiries in last 6 months\n  open_acc = pmax(1, round(rnorm(n, 10, 4))),  # Number of open accounts\n  pub_rec = rbinom(n, 2, 0.06),  # Number of public records\n  revol_util = pmin(100, pmax(0, rnorm(n, 40, 20))),  # Revolving utilization\n  total_acc = pmax(open_acc, open_acc + round(rnorm(n, 8, 6)))  # Total accounts\n)\n\n# Add realistic correlations between variables\ndata &lt;- data %&gt;%\n  mutate(\n    # Interest rate depends on credit score and loan term\n    interest_rate = 25 - (credit_score - 300) * (15/550) + \n                    ifelse(loan_term == 36, -1, ifelse(loan_term == 60, 0, 1.5)) +\n                    rnorm(n, 0, 1.5),\n    \n    # Loan purpose with realistic probabilities\n    loan_purpose = sample(\n      c(\"debt_consolidation\", \"credit_card\", \"home_improvement\", \"major_purchase\", \"medical\", \"other\"), \n      n, replace = TRUE, \n      prob = c(0.45, 0.25, 0.10, 0.08, 0.07, 0.05)\n    ),\n    \n    # Add some derived features that have predictive power\n    payment_amount = (loan_amount * (interest_rate/100/12) * (1 + interest_rate/100/12)^loan_term) / \n                    ((1 + interest_rate/100/12)^loan_term - 1),\n    payment_to_income_ratio = (payment_amount * 12) / income\n  )\n\n# Create a more realistic default probability model with non-linear effects\nlogit_default &lt;- with(data, {\n  -4.5 +  # Base intercept for ~10% default rate\n    -0.03 * (age - 18) +  # Age effect (stronger for younger borrowers)\n    -0.2 * log(income/10000) +  # Log-transformed income effect\n    -0.08 * employment_length +  # Employment length effect\n    ifelse(home_ownership == \"OWN\", -0.7, ifelse(home_ownership == \"MORTGAGE\", -0.3, 0)) +  # Home ownership\n    0.3 * log(loan_amount/1000) +  # Log-transformed loan amount\n    ifelse(loan_term == 36, 0, ifelse(loan_term == 60, 0.4, 0.8)) +  # Loan term\n    0.15 * interest_rate +  # Interest rate effect\n    ifelse(loan_purpose == \"debt_consolidation\", 0.5, \n           ifelse(loan_purpose == \"credit_card\", 0.4, \n                  ifelse(loan_purpose == \"medical\", 0.6, 0))) +  # Loan purpose\n    -0.01 * (credit_score - 300) +  # Credit score (stronger effect at lower scores)\n    0.06 * dti_ratio +  # DTI ratio effect\n    0.4 * delinq_2yrs +  # Delinquencies effect (stronger effect for first delinquency)\n    0.3 * inq_last_6mths +  # Inquiries effect\n    -0.1 * log(open_acc + 1) +  # Open accounts (log-transformed)\n    0.8 * pub_rec +  # Public records (strong effect)\n    0.02 * revol_util +  # Revolving utilization\n    1.2 * payment_to_income_ratio +  # Payment to income ratio (strong effect)\n    rnorm(n, 0, 0.8)  # Add some noise for realistic variation\n})\n\n# Generate default flag with realistic default rate\nprob_default &lt;- plogis(logit_default)\ndata$default &lt;- factor(rbinom(n, 1, prob_default), levels = c(0, 1), labels = c(\"no\", \"yes\"))\n\n# Check class distribution\ntable(data$default)\n\n\n  no  yes \n9425  575 \n\n\n\nprop.table(table(data$default))\n\n\n    no    yes \n0.9425 0.0575 \n\n\n\n# Visualize the default rate\nggplot(data, aes(x = default, fill = default)) +\n  geom_bar(aes(y = ..prop.., group = 1)) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"Class Distribution in Credit Dataset\",\n       y = \"Percentage\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Visualize the relationship between key variables and default rate\nggplot(data, aes(x = credit_score, y = as.numeric(default) - 1)) +\n  geom_smooth(method = \"loess\") +\n  labs(title = \"Default Rate by Credit Score\", \n       x = \"Credit Score\", y = \"Default Probability\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Examine correlation between numeric predictors\ncredit_cors &lt;- data %&gt;%\n  select(age, income, employment_length, loan_amount, interest_rate, \n         credit_score, dti_ratio, delinq_2yrs, revol_util, payment_to_income_ratio) %&gt;%\n  cor()\n\ncorrplot(credit_cors, method = \"circle\", type = \"upper\", \n         tl.col = \"black\", tl.srt = 45, tl.cex = 0.7)"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#data-splitting",
    "href": "posts/modeling-with-tidymodels.html#data-splitting",
    "title": "Tutorial: Comprehensive Credit Scoring Model Development Using tidymodels",
    "section": "Data Splitting",
    "text": "Data Splitting\nCredit default datasets are typically imbalanced, with defaults being the minority class. We’ll use a stratified split to maintain the class distribution.\n\n# Create initial train/test split (80/20)\nset.seed(456)\ninitial_split &lt;- initial_split(data, prop = 0.8, strata = default)\ntrain_data &lt;- training(initial_split)\ntest_data &lt;- testing(initial_split)\n\n# Create validation set from training data (75% train, 25% validation)\nset.seed(789)\nvalidation_split &lt;- initial_split(train_data, prop = 0.75, strata = default)\n\n# Check class imbalance in training data\ntrain_class_counts &lt;- table(training(validation_split)$default)\ntrain_class_props &lt;- prop.table(train_class_counts)\n\ncat(\"Training data class distribution:\\n\")\n\nTraining data class distribution:\n\nprint(train_class_counts)\n\n\n  no  yes \n5661  339 \n\ncat(\"\\nPercentage:\\n\")\n\n\nPercentage:\n\nprint(train_class_props * 100)\n\n\n   no   yes \n94.35  5.65 \n\n# Visualize class imbalance\nROSE::roc.curve(training(validation_split)$default == \"yes\", \n                training(validation_split)$credit_score,\n                plotit = TRUE,\n                main = \"ROC Curve for Credit Score Alone\")\n\n\n\n\n\n\n\n\nArea under the curve (AUC): 0.753"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#feature-engineering-and-preprocessing",
    "href": "posts/modeling-with-tidymodels.html#feature-engineering-and-preprocessing",
    "title": "Building Models in R with tidymodels",
    "section": "Feature Engineering and Preprocessing",
    "text": "Feature Engineering and Preprocessing\nThe following section develops a preprocessing recipe incorporating domain-specific feature engineering techniques relevant to credit risk modeling, including class imbalance handling strategies.\n\n# Examine the distributions of key variables\npar(mfrow = c(2, 2))\nhist(training(validation_split)$credit_score, \n     main = \"Credit Score Distribution\", xlab = \"Credit Score\")\n\nhist(training(validation_split)$dti_ratio, \n     main = \"DTI Ratio Distribution\", xlab = \"DTI Ratio\")\n\nhist(training(validation_split)$payment_to_income_ratio, \n     main = \"Payment to Income Ratio\", \n     xlab = \"Payment to Income Ratio\")\n\nhist(log(training(validation_split)$income), \n     main = \"Log Income Distribution\", xlab = \"Log Income\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\n# Create a recipe\ncredit_recipe &lt;- recipe(default ~ ., data = training(validation_split)) %&gt;%\n  # Remove ID column\n  step_rm(customer_id) %&gt;%\n  \n  # Convert categorical variables to factors\n  step_string2factor(home_ownership, loan_purpose) %&gt;%\n  \n  # Create additional domain-specific features\n  step_mutate(\n    # We already have payment_to_income_ratio from data generation\n    # Add more credit risk indicators\n    credit_utilization = revol_util / 100,\n    acc_to_age_ratio = total_acc / age,\n    delinq_per_acc = ifelse(total_acc &gt; 0, delinq_2yrs / total_acc, 0),\n    inq_rate = inq_last_6mths / (open_acc + 0.1),  # Inquiry rate relative to open accounts\n    term_factor = loan_term / 12,  # Term in years\n    log_income = log(income),  # Log transform income\n    log_loan = log(loan_amount),  # Log transform loan amount\n    payment_ratio = payment_amount / (income / 12),  # Monthly payment to monthly income\n    util_to_income = (revol_util / 100) * (dti_ratio / 100)  # Interaction term\n  ) %&gt;%\n  \n  # Handle categorical variables\n  step_dummy(all_nominal_predictors()) %&gt;%\n  \n  # Impute missing values (if any)\n  step_impute_median(all_numeric_predictors()) %&gt;%\n  \n  # Transform highly skewed variables\n  step_YeoJohnson(income, loan_amount, payment_amount) %&gt;%\n  \n  # Remove highly correlated predictors\n  step_corr(all_numeric_predictors(), threshold = 0.85) %&gt;%\n  \n  # Normalize numeric predictors\n  step_normalize(all_numeric_predictors()) %&gt;%\n  \n  # Remove zero-variance predictors\n  step_zv(all_predictors())\n\n# Prep the recipe to examine the steps\nprepped_recipe &lt;- prep(credit_recipe)\nprepped_recipe\n\n# Check the transformed data\nrecipe_data &lt;- bake(prepped_recipe, new_data = NULL)\nglimpse(recipe_data)\n\nRows: 6,000\nColumns: 27\n$ age                             &lt;dbl&gt; -0.58938716, 1.60129128, 0.05970275, 0…\n$ employment_length               &lt;dbl&gt; 1.01545119, 0.17764322, 2.35594395, -0…\n$ loan_amount                     &lt;dbl&gt; -0.30772032, -1.64018559, 0.01897785, …\n$ credit_score                    &lt;dbl&gt; -1.66355858, -1.60583467, 0.02197934, …\n$ dti_ratio                       &lt;dbl&gt; -1.10745919, -0.21368748, -1.26746777,…\n$ delinq_2yrs                     &lt;dbl&gt; -0.6423758, -0.6423758, -0.6423758, -0…\n$ inq_last_6mths                  &lt;dbl&gt; -0.8324634, 1.5511111, -0.8324634, -0.…\n$ open_acc                        &lt;dbl&gt; -0.516984456, -1.282635289, -1.2826352…\n$ pub_rec                         &lt;dbl&gt; -0.3429372, -0.3429372, 2.7652549, -0.…\n$ total_acc                       &lt;dbl&gt; 0.39969129, 0.54625046, -0.47966374, 0…\n$ interest_rate                   &lt;dbl&gt; 1.47394527, 1.51546694, -0.11980960, 0…\n$ default                         &lt;fct&gt; no, no, no, no, no, no, no, no, yes, n…\n$ credit_utilization              &lt;dbl&gt; -1.841097091, 2.786415973, -1.09378697…\n$ acc_to_age_ratio                &lt;dbl&gt; 0.50174733, -0.54849606, -0.52980631, …\n$ delinq_per_acc                  &lt;dbl&gt; -0.44991964, -0.44991964, -0.44991964,…\n$ inq_rate                        &lt;dbl&gt; -0.520358012, 1.759991048, -0.52035801…\n$ term_factor                     &lt;dbl&gt; 0.3219163, -0.6274559, 0.3219163, -0.6…\n$ log_income                      &lt;dbl&gt; 2.44464243, 0.95097048, -0.59583257, 0…\n$ payment_ratio                   &lt;dbl&gt; -0.70055763, -0.66316187, -0.18762635,…\n$ util_to_income                  &lt;dbl&gt; -1.4271447, 1.7533431, -1.1717989, -1.…\n$ home_ownership_OWN              &lt;dbl&gt; -0.4238857, -0.4238857, -0.4238857, -0…\n$ home_ownership_RENT             &lt;dbl&gt; -0.8977787, -0.8977787, 1.1136746, 1.1…\n$ loan_purpose_debt_consolidation &lt;dbl&gt; 1.1204596, -0.8923421, -0.8923421, -0.…\n$ loan_purpose_home_improvement   &lt;dbl&gt; -0.3277222, -0.3277222, 3.0508567, -0.…\n$ loan_purpose_major_purchase     &lt;dbl&gt; -0.2968579, -0.2968579, -0.2968579, -0…\n$ loan_purpose_medical            &lt;dbl&gt; -0.299178, -0.299178, -0.299178, -0.29…\n$ loan_purpose_other              &lt;dbl&gt; -0.2208157, -0.2208157, -0.2208157, -0…\n\n# Verify\ntable(recipe_data$default)\n\n\n  no  yes \n5661  339"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#model-specification-and-tuning",
    "href": "posts/modeling-with-tidymodels.html#model-specification-and-tuning",
    "title": "Tutorial: Comprehensive Credit Scoring Model Development Using tidymodels",
    "section": "Model Specification and Tuning",
    "text": "Model Specification and Tuning\nFor credit scoring, we’ll compare two models: a logistic regression model (commonly used in the financial industry for its interpretability) and an XGBoost model (for its predictive power). We’ll tune both models using cross-validation.\n\n# Define the logistic regression model\nlog_reg_spec &lt;- logistic_reg(penalty = tune(), mixture = tune()) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"classification\")\n\n# Create a logistic regression workflow\nlog_reg_workflow &lt;- workflow() %&gt;%\n  add_recipe(credit_recipe) %&gt;%\n  add_model(log_reg_spec)\n\n# Define the tuning grid for logistic regression\nlog_reg_grid &lt;- grid_regular(\n  penalty(range = c(-5, 0), trans = log10_trans()),\n  mixture(range = c(0, 1)),\n  levels = c(10, 5)\n)\n\n# Define the XGBoost model with tunable parameters\nxgb_spec &lt;- boost_tree(\n  trees = tune(),\n  tree_depth = tune(),\n  min_n = tune(),\n  loss_reduction = tune(),\n  mtry = tune(),\n  learn_rate = tune()\n) %&gt;%\n  set_engine(\"xgboost\", objective = \"binary:logistic\", scale_pos_weight = 5) %&gt;%\n  set_mode(\"classification\")\n\n# Create an XGBoost workflow\nxgb_workflow &lt;- workflow() %&gt;%\n  add_recipe(credit_recipe) %&gt;%\n  add_model(xgb_spec)\n\n# Define the tuning grid for XGBoost\nxgb_grid &lt;- grid_latin_hypercube(\n  trees(range = c(100, 500)),\n  tree_depth(range = c(3, 10)),\n  min_n(range = c(2, 20)),\n  loss_reduction(range = c(0.001, 1.0)),\n  mtry(range = c(5, 20)),\n  learn_rate(range = c(-4, -1), trans = log10_trans()),\n  size = 15\n)\n\n# Create cross-validation folds with stratification\nset.seed(234)\ncv_folds &lt;- vfold_cv(training(validation_split), v = 3, strata = default)\n\n# Define the metrics to evaluate\nclassification_metrics &lt;- metric_set(\n  roc_auc,  # Area under the ROC curve\n  pr_auc,    # Area under the precision-recall curve\n)\n\n# Tune the logistic regression model\nset.seed(345)\nlog_reg_tuned &lt;- tune_grid(\n  log_reg_workflow,\n  resamples = cv_folds,\n  grid = log_reg_grid,\n  metrics = classification_metrics,\n  control = control_grid(save_pred = TRUE, verbose = TRUE)\n)\n\n# Tune the XGBoost model\nset.seed(456)\nxgb_tuned &lt;- tune_grid(\n  xgb_workflow,\n  resamples = cv_folds,\n  grid = xgb_grid,\n  metrics = classification_metrics,\n  control = control_grid(save_pred = TRUE, verbose = TRUE)\n)\n\n# Collect and visualize logistic regression tuning results\nlog_reg_results &lt;- log_reg_tuned %&gt;% collect_metrics()\nlog_reg_results %&gt;% filter(.metric == \"roc_auc\") %&gt;% arrange(desc(mean)) %&gt;% head()\n\n# A tibble: 6 × 8\n   penalty mixture .metric .estimator  mean     n std_err .config              \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 0.00167     1    roc_auc binary     0.853     3 0.00841 Preprocessor1_Model45\n2 0.00167     0.75 roc_auc binary     0.853     3 0.00841 Preprocessor1_Model35\n3 0.00599     0.25 roc_auc binary     0.853     3 0.00869 Preprocessor1_Model16\n4 0.00167     0.5  roc_auc binary     0.853     3 0.00836 Preprocessor1_Model25\n5 0.000464    1    roc_auc binary     0.852     3 0.00817 Preprocessor1_Model44\n6 0.00167     0.25 roc_auc binary     0.852     3 0.00837 Preprocessor1_Model15\n\n\nThe results above show how both models perform across different hyperparameter settings. XGBoost typically achieves higher predictive performance, while logistic regression offers better interpretability. For credit scoring applications, both aspects are important."
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#finalizing-and-evaluating-the-models",
    "href": "posts/modeling-with-tidymodels.html#finalizing-and-evaluating-the-models",
    "title": "Tutorial: Comprehensive Credit Scoring Model Development Using tidymodels",
    "section": "Finalizing and Evaluating the Models",
    "text": "Finalizing and Evaluating the Models\nWe’ll finalize both models using their best hyperparameters and evaluate them on the validation set.\n\n# Select best hyperparameters based on ROC AUC\nbest_log_reg_params &lt;- select_best(log_reg_tuned, metric = \"roc_auc\")\nbest_xgb_params &lt;- select_best(xgb_tuned, metric = \"roc_auc\")\n\n# Finalize workflows with best parameters\nfinal_log_reg_workflow &lt;- log_reg_workflow %&gt;%\n  finalize_workflow(best_log_reg_params)\n\nfinal_xgb_workflow &lt;- xgb_workflow %&gt;%\n  finalize_workflow(best_xgb_params)\n\n# Fit the final models on the full training data\nfinal_log_reg_model &lt;- final_log_reg_workflow %&gt;%\n  fit(data = training(validation_split))\n\nfinal_xgb_model &lt;- final_xgb_workflow %&gt;%\n  fit(data = training(validation_split))\n\n# Make predictions on the validation set with both models\nlog_reg_val_results &lt;- final_log_reg_model %&gt;%\n  predict(testing(validation_split)) %&gt;%\n  bind_cols(predict(final_log_reg_model, testing(validation_split), type = \"prob\")) %&gt;%\n  bind_cols(testing(validation_split) %&gt;% select(default, customer_id))\n\nxgb_val_results &lt;- final_xgb_model %&gt;%\n  predict(testing(validation_split)) %&gt;%\n  bind_cols(predict(final_xgb_model, testing(validation_split), type = \"prob\")) %&gt;%\n  bind_cols(testing(validation_split) %&gt;% select(default, customer_id))\n\n# Evaluate model performance on validation set\nlog_reg_val_metrics &lt;- log_reg_val_results %&gt;%\n  metrics(truth = default, estimate = .pred_class, .pred_yes)\n\nxgb_val_metrics &lt;- xgb_val_results %&gt;%\n  metrics(truth = default, estimate = .pred_class, .pred_yes)\n\ncat(\"Logistic Regression Validation Metrics:\\n\")\n\nLogistic Regression Validation Metrics:\n\nprint(log_reg_val_metrics)\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.950\n2 kap         binary         0.145\n3 mn_log_loss binary         3.64 \n4 roc_auc     binary         0.157\n\ncat(\"\\nXGBoost Validation Metrics:\\n\")\n\n\nXGBoost Validation Metrics:\n\nprint(xgb_val_metrics)\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary        0.948 \n2 kap         binary        0.0339\n3 mn_log_loss binary        4.82  \n4 roc_auc     binary        0.158"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#feature-importance-and-model-interpretation",
    "href": "posts/modeling-with-tidymodels.html#feature-importance-and-model-interpretation",
    "title": "Tutorial: Comprehensive Credit Scoring Model Development Using tidymodels",
    "section": "Feature Importance and Model Interpretation",
    "text": "Feature Importance and Model Interpretation\nUnderstanding which features drive the predictions is crucial for credit scoring models.\n#| label: feature-importance #| fig-width: 10 #| fig-height: 12"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#final-evaluation-on-test-set",
    "href": "posts/modeling-with-tidymodels.html#final-evaluation-on-test-set",
    "title": "Tutorial: Comprehensive Credit Scoring Model Development Using tidymodels",
    "section": "Final Evaluation on Test Set",
    "text": "Final Evaluation on Test Set\nNow let’s evaluate our best model (XGBoost) on the held-out test set for an unbiased assessment of its performance.\n#| label: final-evaluation"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#conclusion",
    "href": "posts/modeling-with-tidymodels.html#conclusion",
    "title": "Tutorial: Comprehensive Credit Scoring Model Development Using tidymodels",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we’ve demonstrated how to build a credit scoring model using the tidymodels framework. We covered:\n\nCreating a realistic credit dataset with domain-specific features\nImplementing a comprehensive feature engineering pipeline\nTraining and tuning both traditional (logistic regression) and modern (XGBoost) models\nEvaluating model performance using industry-standard metrics\nCreating interpretable visualizations of model results\n\nThe tidymodels framework provides a consistent and modular approach to building machine learning models, making it easier to experiment with different algorithms and preprocessing steps while maintaining good statistical practices."
  },
  {
    "objectID": "posts/simple-neural-net-with-torch.html",
    "href": "posts/simple-neural-net-with-torch.html",
    "title": "Building a Simple Neural Network in R with torch",
    "section": "",
    "text": "The torch package brings deep learning to R by providing bindings to the popular PyTorch library. This comprehensive tutorial demonstrates how to build and train a simple neural network using torch in R."
  },
  {
    "objectID": "posts/simple-neural-net-with-torch.html#installation",
    "href": "posts/simple-neural-net-with-torch.html#installation",
    "title": "Building a Simple Neural Network in R with torch",
    "section": "Installation",
    "text": "Installation\n\n# install.packages(\"torch\")\nlibrary(torch)\n# torch::install_torch()"
  },
  {
    "objectID": "posts/simple-neural-net-with-torch.html#creating-a-simple-neural-network",
    "href": "posts/simple-neural-net-with-torch.html#creating-a-simple-neural-network",
    "title": "Building a Simple Neural Network in R with torch",
    "section": "Creating a Simple Neural Network",
    "text": "Creating a Simple Neural Network\nLet’s create a neural network to perform regression on a simple dataset (predicting y from x).\n\n1. Generate Sample Data\nWe’ll start by creating some synthetic data with a linear relationship plus some noise:\n\n# Set seed for reproducibility\nset.seed(42)\n\n# Generate training data: y = 3x + 2 + noise\nx &lt;- torch_randn(100, 1)\ny &lt;- 3 * x + 2 + torch_randn(100, 1) * 0.3\n\n# Display the first few data points\nhead(data.frame(\n  x = as.numeric(x$squeeze()),\n  y = as.numeric(y$squeeze())\n))\n\n           x           y\n1 -0.7739413 -0.62908769\n2 -1.3591517 -1.72024834\n3 -0.6743400  0.05231112\n4  0.9649413  5.17009163\n5  1.1073095  5.11210012\n6 -0.1294657  1.98031366\n\n\n\n\n2. Define the Neural Network Module\nNow we’ll define our neural network architecture using torch’s module system:\n\n# Define a simple feedforward neural network\nnet &lt;- nn_module(\n  initialize = function() {\n    # Define layers\n    self$fc1 &lt;- nn_linear(1, 8)  # Input layer to hidden layer (1 -&gt; 8 neurons)\n    self$fc2 &lt;- nn_linear(8, 1)  # Hidden layer to output layer (8 -&gt; 1 neuron)\n  },\n  forward = function(x) {\n    # Define forward pass\n    x %&gt;% \n      self$fc1() %&gt;%     # First linear transformation\n      nnf_relu() %&gt;%     # ReLU activation function\n      self$fc2()         # Second linear transformation\n  }\n)\n\n# Instantiate the model\nmodel &lt;- net()\n\n# Display model structure\nprint(model)\n\nAn `nn_module` containing 25 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────\n• fc1: &lt;nn_linear&gt; #16 parameters\n• fc2: &lt;nn_linear&gt; #9 parameters\n\n\n\n\n3. Set Up the Optimizer and Loss Function\nWe need to define how our model will learn from the data:\n\n# Set up optimizer (Adam optimizer with learning rate 0.01)\noptimizer &lt;- optim_adam(model$parameters, lr = 0.01)\n\n# Define loss function (Mean Squared Error for regression)\nloss_fn &lt;- nnf_mse_loss\n\ncat(\"Optimizer:\", class(optimizer)[1], \"\\n\")\n\nOptimizer: optim_adam \n\ncat(\"Loss function:\", \"Mean Squared Error\\n\")\n\nLoss function: Mean Squared Error\n\ncat(\"Learning rate:\", 0.01, \"\\n\")\n\nLearning rate: 0.01 \n\n\n\n\n4. Training Loop\nNow we’ll train our neural network:\n\n# Store loss values for plotting\nloss_history &lt;- numeric(300)\n\n# Training loop\nfor(epoch in 1:300) {\n  # Set model to training mode\n  model$train()\n  \n  # Reset gradients\n  optimizer$zero_grad()\n  \n  # Forward pass\n  y_pred &lt;- model(x)\n  \n  # Calculate loss\n  loss &lt;- loss_fn(y_pred, y)\n  \n  # Backward pass\n  loss$backward()\n  \n  # Update parameters\n  optimizer$step()\n  \n  # Store loss for plotting\n  loss_history[epoch] &lt;- loss$item()\n  \n  # Print progress every 50 epochs\n  if(epoch %% 50 == 0) {\n    cat(sprintf(\"Epoch %d, Loss: %.6f\\n\", epoch, loss$item()))\n  }\n}\n\nEpoch 50, Loss: 1.833089\nEpoch 100, Loss: 0.088461\nEpoch 150, Loss: 0.079984\nEpoch 200, Loss: 0.079119\nEpoch 250, Loss: 0.078439\nEpoch 300, Loss: 0.077913\n\n\n\n\n5. Visualize the Training Progress\nLet’s see how the loss decreased during training:\n\n# Create a data frame for plotting\ntraining_df &lt;- data.frame(\n  epoch = 1:300,\n  loss = loss_history\n)\n\n# Plot training loss\nggplot(training_df, aes(x = epoch, y = loss)) +\n  geom_line(color = \"#2c3e50\", size = 1) +\n  labs(\n    title = \"Training Loss Over Time\",\n    subtitle = \"Neural Network Learning Progress\",\n    x = \"Epoch\",\n    y = \"Mean Squared Error Loss\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12, color = \"gray60\")\n  )\n\n\n\n\n\n\n\n\n\n\n6. Visualize the Results\nNow let’s see how well our trained model performs:\n\n# Set model to evaluation mode\nmodel$eval()\n\n# Generate predictions\nwith_no_grad({\n  y_pred &lt;- model(x)\n})\n\n# Convert to R vectors for plotting\nx_np &lt;- as.numeric(x$squeeze())\ny_np &lt;- as.numeric(y$squeeze())\ny_pred_np &lt;- as.numeric(y_pred$squeeze())\n\n# Create data frame for ggplot\nplot_df &lt;- data.frame(\n  x = x_np,\n  y_actual = y_np,\n  y_predicted = y_pred_np\n)\n\n# Create the plot\nggplot(plot_df, aes(x = x)) +\n  geom_point(aes(y = y_actual, color = \"Actual\"), alpha = 0.7, size = 2) +\n  geom_point(aes(y = y_predicted, color = \"Predicted\"), alpha = 0.7, size = 2) +\n  geom_smooth(aes(y = y_predicted), method = \"loess\", se = FALSE, \n              color = \"#e74c3c\", linetype = \"dashed\") +\n  labs(\n    title = \"Neural Network Regression Results\",\n    subtitle = \"Comparing actual vs predicted values\",\n    x = \"Input (x)\",\n    y = \"Output (y)\",\n    color = \"Data Type\"\n  ) +\n  scale_color_manual(values = c(\"Actual\" = \"#3498db\", \"Predicted\" = \"#e74c3c\")) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12, color = \"gray60\"),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\n\n\n\n7. Model Performance Analysis\nLet’s analyze how well our model learned the underlying pattern:\n\n# Calculate performance metrics\nmse &lt;- mean((y_pred_np - y_np)^2)\nrmse &lt;- sqrt(mse)\nmae &lt;- mean(abs(y_pred_np - y_np))\nr_squared &lt;- cor(y_pred_np, y_np)^2\n\n# Create performance summary\nperformance_summary &lt;- data.frame(\n  Metric = c(\"Mean Squared Error\", \"Root Mean Squared Error\", \n             \"Mean Absolute Error\", \"R-squared\"),\n  Value = c(mse, rmse, mae, r_squared)\n)\n\nprint(performance_summary)\n\n                   Metric      Value\n1      Mean Squared Error 0.07790354\n2 Root Mean Squared Error 0.27911206\n3     Mean Absolute Error 0.22832826\n4               R-squared 0.99026918\n\n# Compare with true relationship (y = 3x + 2)\n# Generate predictions on a grid for comparison\nx_grid &lt;- torch_linspace(-3, 3, 100)$unsqueeze(2)\nwith_no_grad({\n  y_grid_pred &lt;- model(x_grid)\n})\n\nx_grid_np &lt;- as.numeric(x_grid$squeeze())\ny_grid_pred_np &lt;- as.numeric(y_grid_pred$squeeze())\ny_grid_true &lt;- 3 * x_grid_np + 2\n\n# Plot comparison\ncomparison_df &lt;- data.frame(\n  x = x_grid_np,\n  y_true = y_grid_true,\n  y_predicted = y_grid_pred_np\n)\n\nggplot(comparison_df, aes(x = x)) +\n  geom_line(aes(y = y_true, color = \"True Function\"), size = 2) +\n  geom_line(aes(y = y_predicted, color = \"Neural Network\"), size = 2, linetype = \"dashed\") +\n  geom_point(data = plot_df, aes(y = y_actual), alpha = 0.3, color = \"gray50\") +\n  labs(\n    title = \"Neural Network vs True Function\",\n    subtitle = \"How well did our model learn the underlying pattern?\",\n    x = \"Input (x)\",\n    y = \"Output (y)\",\n    color = \"Function Type\"\n  ) +\n  scale_color_manual(values = c(\"True Function\" = \"#2c3e50\", \"Neural Network\" = \"#e74c3c\")) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12, color = \"gray60\"),\n    legend.position = \"top\"\n  )"
  },
  {
    "objectID": "posts/simple-neural-net-with-torch.html#understanding-the-neural-network",
    "href": "posts/simple-neural-net-with-torch.html#understanding-the-neural-network",
    "title": "Building a Simple Neural Network in R with torch",
    "section": "Understanding the Neural Network",
    "text": "Understanding the Neural Network\nThe following examination reveals what the network learned by analyzing its parameters:\n\n# Extract learned parameters\nlayer1_weight &lt;- as.matrix(model$layer1$weight$detach())\nlayer1_bias &lt;- as.numeric(model$layer1$bias$detach())\nlayer2_weight &lt;- as.matrix(model$layer2$weight$detach())\nlayer2_bias &lt;- as.numeric(model$layer1$bias$detach())\n\ncat(\"First layer (fc1) parameters:\\n\")\n\nFirst layer (fc1) parameters:\n\ncat(\"Weight matrix shape:\", dim(layer1_weight), \"\\n\")\n\nWeight matrix shape: 8 1 \n\ncat(\"Bias vector length:\", length(layer1_bias), \"\\n\\n\")\n\nBias vector length: 8 \n\ncat(\"Second layer (fc2) parameters:\\n\")\n\nSecond layer (fc2) parameters:\n\ncat(\"Weight matrix shape:\", dim(layer2_weight), \"\\n\")\n\nWeight matrix shape: 1 8 \n\ncat(\"Bias value:\", layer2_bias, \"\\n\\n\")\n\nBias value: 1.656915 1.261944 -0.8543047 1.698726 1.390176 1.139801 0.9438867 -0.9184557 \n\n# Display first layer weights and biases\ncat(\"First layer weights:\\n\")\n\nFirst layer weights:\n\nprint(round(layer1_weight, 4))\n\n        [,1]\n[1,]  0.5999\n[2,] -0.6173\n[3,] -1.0954\n[4,] -0.8322\n[5,]  1.5211\n[6,]  0.0378\n[7,]  0.8064\n[8,]  0.0171\n\ncat(\"\\nFirst layer biases:\\n\")\n\n\nFirst layer biases:\n\nprint(round(layer2_bias, 4))\n\n[1]  1.6569  1.2619 -0.8543  1.6987  1.3902  1.1398  0.9439 -0.9185"
  },
  {
    "objectID": "posts/simple-neural-net-with-torch.html#advanced-experimenting-with-different-architectures",
    "href": "posts/simple-neural-net-with-torch.html#advanced-experimenting-with-different-architectures",
    "title": "Building a Simple Neural Network in R with torch",
    "section": "Advanced: Experimenting with Different Architectures",
    "text": "Advanced: Experimenting with Different Architectures\nLet’s compare our simple network with different architectures:\n\n# Define different network architectures\ncreate_network &lt;- function(hidden_sizes) {\n  nn_module(\n    initialize = function(hidden_sizes) {\n      self$layers &lt;- nn_module_list()\n      \n      # Input layer\n      prev_size &lt;- 1\n      for(i in seq_along(hidden_sizes)) {\n        self$layers$append(nn_linear(prev_size, hidden_sizes[i]))\n        prev_size &lt;- hidden_sizes[i]\n      }\n      # Output layer\n      self$layers$append(nn_linear(prev_size, 1))\n    },\n    forward = function(x) {\n      for(i in 1:(length(self$layers) - 1)) {\n        x &lt;- nnf_relu(self$layers[[i]](x))\n      }\n      # No activation on output layer\n      self$layers[[length(self$layers)]](x)\n    }\n  )\n}\n\n# Train different architectures\narchitectures &lt;- list(\n  \"Simple (8)\" = c(8),\n  \"Deep (16-8)\" = c(16, 8),\n  \"Wide (32)\" = c(32),\n  \"Very Deep (16-16-8)\" = c(16, 16, 8)\n)\n\nresults &lt;- list()\n\nfor(arch_name in names(architectures)) {\n  cat(\"Training\", arch_name, \"architecture...\\n\")\n  \n  # Create and train model\n  net_class &lt;- create_network(architectures[[arch_name]])\n  model_temp &lt;- net_class(architectures[[arch_name]])\n  optimizer_temp &lt;- optim_adam(model_temp$parameters, lr = 0.01)\n  \n  # Quick training (fewer epochs for comparison)\n  for(epoch in 1:200) {\n    model_temp$train()\n    optimizer_temp$zero_grad()\n    y_pred_temp &lt;- model_temp(x)\n    loss_temp &lt;- loss_fn(y_pred_temp, y)\n    loss_temp$backward()\n    optimizer_temp$step()\n  }\n  \n  # Generate predictions\n  model_temp$eval()\n  with_no_grad({\n    y_pred_arch &lt;- model_temp(x_grid)\n  })\n  \n  results[[arch_name]] &lt;- data.frame(\n    x = x_grid_np,\n    y_pred = as.numeric(y_pred_arch$squeeze()),\n    architecture = arch_name\n  )\n}\n\nTraining Simple (8) architecture...\nTraining Deep (16-8) architecture...\nTraining Wide (32) architecture...\nTraining Very Deep (16-16-8) architecture...\n\n# Combine results\nall_results &lt;- do.call(rbind, results)\n\n# Plot comparison\nggplot(all_results, aes(x = x, y = y_pred, color = architecture)) +\n  geom_line(size = 1.2) +\n  geom_line(data = comparison_df, aes(y = y_true, color = \"True Function\"), \n            size = 2, linetype = \"solid\") +\n  geom_point(data = plot_df, aes(x = x, y = y_actual), \n             color = \"gray50\", alpha = 0.3, inherit.aes = FALSE) +\n  labs(\n    title = \"Comparison of Different Neural Network Architectures\",\n    subtitle = \"How network depth and width affect learning\",\n    x = \"Input (x)\",\n    y = \"Output (y)\",\n    color = \"Architecture\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12, color = \"gray60\"),\n    legend.position = \"top\"\n  )"
  },
  {
    "objectID": "posts/simple-neural-net-with-torch.html#key-takeaways",
    "href": "posts/simple-neural-net-with-torch.html#key-takeaways",
    "title": "Building a Simple Neural Network in R with torch",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nThis neural network implementation demonstrates several important concepts in development with torch:\n\nSimple Architecture: Even a simple 2-layer network can learn complex patterns effectively\nTraining Process: The importance of proper training loops with gradient computation\nVisualization: Effective methods for visualizing both training progress and results\nModel Evaluation: Understanding model performance through multiple metrics\nArchitecture Comparison: How different network structures affect learning capabilities\n\nThe torch package provides a straightforward approach to building and experimenting with neural networks in R, bringing the power of deep learning to the R ecosystem. This approach can be extended to more complex datasets and deeper architectures as needed."
  },
  {
    "objectID": "posts/simple-neural-net-with-torch.html#next-steps",
    "href": "posts/simple-neural-net-with-torch.html#next-steps",
    "title": "Building a Simple Neural Network in R with torch",
    "section": "Next Steps",
    "text": "Next Steps\nTo further explore neural networks with torch, practitioners can consider:\n\nExperimenting with different activation functions (sigmoid, tanh, etc.)\nAdding regularization techniques (dropout, weight decay)\nWorking with real-world datasets\nImplementing convolutional or recurrent neural networks\nUsing GPU acceleration for larger models"
  },
  {
    "objectID": "posts/multi-task-learning-with-torch.html",
    "href": "posts/multi-task-learning-with-torch.html",
    "title": "Multi-Task Learning with torch in R",
    "section": "",
    "text": "Multi-task learning (MTL) is an approach where a single neural network model is trained to perform multiple related tasks simultaneously. This methodology can improve model generalization, reduce overfitting, and leverage shared information across tasks. This post explores how to implement a multi-task learning model using the torch package in R."
  },
  {
    "objectID": "posts/multi-task-learning-with-torch.html#introduction-to-multi-task-learning",
    "href": "posts/multi-task-learning-with-torch.html#introduction-to-multi-task-learning",
    "title": "Multi-Task Learning with torch in R",
    "section": "",
    "text": "Multi-task learning operates by sharing representations between related tasks, enabling models to generalize more effectively. Instead of training separate models for each task, this approach develops a single model with:\n\nShared layers that learn common features across tasks\nTask-specific layers that specialize for each individual task\n\nMultiple loss functions, one for each task\n\nThis approach proves particularly valuable when dealing with related prediction problems that can benefit from shared feature representations."
  },
  {
    "objectID": "posts/multi-task-learning-with-torch.html#installation-and-setup",
    "href": "posts/multi-task-learning-with-torch.html#installation-and-setup",
    "title": "Multi-Task Learning with torch in R",
    "section": "Installation and Setup",
    "text": "Installation and Setup\nFirst, let’s install and load the required packages:\n\n# install.packages(c(\"torch\", \"tidyverse\", \"corrplot\"))\nlibrary(torch)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/multi-task-learning-with-torch.html#creating-a-multi-task-learning-model",
    "href": "posts/multi-task-learning-with-torch.html#creating-a-multi-task-learning-model",
    "title": "Multi-Task Learning with torch in R",
    "section": "Creating a Multi-Task Learning Model",
    "text": "Creating a Multi-Task Learning Model\nThe implementation will construct a model that simultaneously performs two related tasks:\n\nRegression: Predicting a continuous value\nClassification: Predicting a binary outcome\n\n\n1. Generate Sample Data for Multiple Tasks\nThe first step involves creating a synthetic dataset that contains features relevant to both tasks:\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Number of samples\nn &lt;- 1000\n\n# Create a dataset with 5 features\nx &lt;- torch_randn(n, 5)\n\n# Task 1 (Regression): Predict continuous value\n# Create a target that's a function of the input features plus some noise\ny_regression &lt;- x[, 1] * 0.7 + x[, 2] * 0.3 - x[, 3] * 0.5 + torch_randn(n) * 0.2\n\n# Task 2 (Classification): Predict binary outcome\n# Create a classification target based on a nonlinear combination of features\nlogits &lt;- x[, 1] * 0.8 - x[, 4] * 0.4 + x[, 5] * 0.6\ny_classification &lt;- (logits &gt; 0)$to(torch_float())\n\n# Split into training (70%) and testing (30%) sets\ntrain_idx &lt;- 1:round(0.7 * n)\ntest_idx &lt;- (round(0.7 * n) + 1):n\n\n# Training data\nx_train &lt;- x[train_idx, ]\ny_reg_train &lt;- y_regression[train_idx]\ny_cls_train &lt;- y_classification[train_idx]\n\n# Testing data\nx_test &lt;- x[test_idx, ]\ny_reg_test &lt;- y_regression[test_idx]\ny_cls_test &lt;- y_classification[test_idx]\n\n\n\n2. Exploratory Data Analysis\nThe next step examines the relationships between features and targets:\n\n# Convert to R matrices for analysis\nx_r &lt;- as.matrix(x)\ny_reg_r &lt;- as.numeric(y_regression)\ny_cls_r &lt;- as.numeric(y_classification)\n\n# Create a comprehensive dataset for analysis\nanalysis_df &lt;- data.frame(\n  Feature1 = x_r[, 1],\n  Feature2 = x_r[, 2],\n  Feature3 = x_r[, 3],\n  Feature4 = x_r[, 4],\n  Feature5 = x_r[, 5],\n  Regression_Target = y_reg_r,\n  Classification_Target = y_cls_r\n)\n\n# Correlation analysis\ncor_matrix &lt;- cor(analysis_df)\ncorrplot(cor_matrix, method = \"color\", type = \"upper\", \n         tl.col = \"black\", tl.srt = 45, addCoef.col = \"black\",\n         title = \"Feature and Target Correlations\", mar = c(0,0,1,0))\n\n\n\n\n\n\n\n# Visualize feature distributions by classification target\nlong_features &lt;- analysis_df %&gt;%\n  select(-Regression_Target) %&gt;%\n  pivot_longer(cols = starts_with(\"Feature\"), \n               names_to = \"Feature\", values_to = \"Value\") %&gt;%\n  mutate(Class = factor(Classification_Target, labels = c(\"Class 0\", \"Class 1\")))\n\nggplot(long_features, aes(x = Value, fill = Class)) +\n  geom_histogram(alpha = 0.7, bins = 20, position = \"identity\") +\n  facet_wrap(~Feature, scales = \"free\") +\n  labs(title = \"Feature Distributions by Classification Target\",\n       subtitle = \"Understanding how features relate to the binary classification task\",\n       x = \"Feature Value\", y = \"Count\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\n\n3. Define the Multi-Task Neural Network\nThe architecture design creates a neural network with shared layers and task-specific branches:\n\n# Define the multi-task neural network\nmulti_task_net &lt;- nn_module(\n  \"MultiTaskNet\",\n  \n  initialize = function(input_size, \n                        hidden_size, \n                        reg_output_size = 1, \n                        cls_output_size = 1) {\n    \n    self$input_size &lt;- input_size\n    self$hidden_size &lt;- hidden_size\n    self$reg_output_size &lt;- reg_output_size\n    self$cls_output_size &lt;- cls_output_size\n    \n    # Shared layers - these learn representations useful for both tasks\n    self$shared_layer1 &lt;- nn_linear(input_size, hidden_size)\n    self$shared_layer2 &lt;- nn_linear(hidden_size, hidden_size)\n    self$dropout &lt;- nn_dropout(0.2)  # Add regularization\n    \n    # Task-specific layers\n    # Regression branch\n    self$regression_layer &lt;- nn_linear(hidden_size, reg_output_size)\n    \n    # Classification branch\n    self$classification_layer &lt;- nn_linear(hidden_size, cls_output_size)\n  },\n  \n  forward = function(x) {\n    # Shared feature extraction\n    shared_features &lt;- x %&gt;%\n      self$shared_layer1() %&gt;%\n      nnf_relu() %&gt;%\n      self$dropout() %&gt;%\n      self$shared_layer2() %&gt;%\n      nnf_relu() %&gt;%\n      self$dropout()\n    \n    # Task-specific predictions\n    regression_output &lt;- self$regression_layer(shared_features)\n    classification_logits &lt;- self$classification_layer(shared_features)\n    \n    list(\n      regression = regression_output,\n      classification = classification_logits\n    )\n  }\n)\n\n# Create model instance\nmodel &lt;- multi_task_net(\n  input_size = 5,\n  hidden_size = 30\n)\n\n# Print model architecture\nprint(model)\n\nAn `nn_module` containing 1,172 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────\n• shared_layer1: &lt;nn_linear&gt; #180 parameters\n• shared_layer2: &lt;nn_linear&gt; #930 parameters\n• dropout: &lt;nn_dropout&gt; #0 parameters\n• regression_layer: &lt;nn_linear&gt; #31 parameters\n• classification_layer: &lt;nn_linear&gt; #31 parameters\n\n# Count parameters\ntotal_params &lt;- sum(sapply(model$parameters, function(p) prod(p$shape)))\ncat(\"\\nTotal parameters:\", total_params, \"\\n\")\n\n\nTotal parameters: 1172 \n\n\n\n\n4. Define Loss Functions and Optimizer\nMulti-task learning requires separate loss functions for each task. To address potential overfitting, particularly in the classification task, several regularization strategies will be implemented:\n\n# Loss functions\nregression_loss_fn &lt;- nnf_mse_loss  # Mean squared error for regression\nclassification_loss_fn &lt;- nnf_binary_cross_entropy_with_logits  # Binary cross-entropy for classification\n\n# Optimizer with weight decay for L2 regularization\noptimizer &lt;- optim_adam(model$parameters, lr = 0.01, weight_decay = 1e-4)\n\n# Task weights - these control the relative importance of each task\ntask_weights &lt;- c(regression = 0.5, classification = 0.5)\n\n# Early stopping parameters\npatience &lt;- 15\nbest_val_loss &lt;- Inf\npatience_counter &lt;- 0\nbest_model_state &lt;- NULL\n\n# Validation split from training data\nval_size &lt;- round(0.2 * length(train_idx))\nval_indices &lt;- sample(train_idx, val_size)\ntrain_indices &lt;- setdiff(train_idx, val_indices)\n\n# Create validation sets\nx_val &lt;- x[val_indices, ]\ny_reg_val &lt;- y_regression[val_indices]\ny_cls_val &lt;- y_classification[val_indices]\n\n# Update training sets\nx_train &lt;- x[train_indices, ]\ny_reg_train &lt;- y_regression[train_indices]\ny_cls_train &lt;- y_classification[train_indices]\n\ncat(\"Updated split - Training:\", length(train_indices), \n    \"Validation:\", length(val_indices), \n    \"Testing:\", length(test_idx), \"\\n\")\n\nUpdated split - Training: 560 Validation: 140 Testing: 300 \n\n\n\n\n5. Enhanced Training Loop with Overfitting Prevention\nThe training process now incorporates validation monitoring and early stopping to prevent overfitting:\n\n# Hyperparameters\nepochs &lt;- 200  # Increased epochs since we have early stopping\n\n# Enhanced training history tracking\ntraining_history &lt;- data.frame(\n  epoch = integer(),\n  train_reg_loss = numeric(),\n  train_cls_loss = numeric(),\n  train_total_loss = numeric(),\n  val_reg_loss = numeric(),\n  val_cls_loss = numeric(),\n  val_total_loss = numeric(),\n  val_accuracy = numeric()\n)\n\ncat(\"Starting enhanced training with overfitting prevention...\\n\")\n\nStarting enhanced training with overfitting prevention...\n\nfor (epoch in 1:epochs) {\n  # Training phase\n  model$train()\n  optimizer$zero_grad()\n  \n  # Forward pass on training data\n  outputs &lt;- model(x_train)\n  \n  # Calculate training loss for each task\n  train_reg_loss &lt;- regression_loss_fn(\n    outputs$regression$squeeze(), \n    y_reg_train\n  )\n  \n  train_cls_loss &lt;- classification_loss_fn(\n    outputs$classification$squeeze(), \n    y_cls_train\n  )\n  \n  # Weighted combined training loss\n  train_total_loss &lt;- task_weights[\"regression\"] * train_reg_loss + \n                     task_weights[\"classification\"] * train_cls_loss\n    # Backward pass and optimize\n  train_total_loss$backward()\n  \n  # Gradient clipping to prevent exploding gradients\n  nn_utils_clip_grad_norm_(model$parameters, max_norm = 1.0)\n  \n  optimizer$step()\n  \n  # Validation phase\n  model$eval()\n  with_no_grad({\n    val_outputs &lt;- model(x_val)\n    \n    # Calculate validation losses\n    val_reg_loss &lt;- regression_loss_fn(\n      val_outputs$regression$squeeze(), \n      y_reg_val\n    )\n    \n    val_cls_loss &lt;- classification_loss_fn(\n      val_outputs$classification$squeeze(), \n      y_cls_val\n    )\n    \n    val_total_loss &lt;- task_weights[\"regression\"] * val_reg_loss + \n                     task_weights[\"classification\"] * val_cls_loss\n    \n    # Calculate validation accuracy\n    val_cls_probs &lt;- nnf_sigmoid(val_outputs$classification$squeeze())\n    val_cls_preds &lt;- (val_cls_probs &gt; 0.5)$to(torch_int())\n    val_accuracy &lt;- (val_cls_preds == y_cls_val$to(torch_int()))$sum()$item() / length(val_indices)\n  })\n  \n  # Record history\n  training_history &lt;- rbind(\n    training_history,\n    data.frame(\n      epoch = epoch,\n      train_reg_loss = as.numeric(train_reg_loss$item()),\n      train_cls_loss = as.numeric(train_cls_loss$item()),\n      train_total_loss = as.numeric(train_total_loss$item()),\n      val_reg_loss = as.numeric(val_reg_loss$item()),\n      val_cls_loss = as.numeric(val_cls_loss$item()),\n      val_total_loss = as.numeric(val_total_loss$item()),\n      val_accuracy = val_accuracy\n    )\n  )\n  \n  # Early stopping logic\n  current_val_loss &lt;- as.numeric(val_total_loss$item())\n  if (current_val_loss &lt; best_val_loss) {\n    best_val_loss &lt;- current_val_loss\n    patience_counter &lt;- 0\n    # Save best model state\n    best_model_state &lt;- model$state_dict()\n  } else {\n    patience_counter &lt;- patience_counter + 1\n  }\n  \n  # Print progress every 25 epochs\n  if (epoch %% 25 == 0 || epoch == 1) {\n    cat(sprintf(\"Epoch %d - Train Loss: %.4f, Val Loss: %.4f, Val Acc: %.3f, Patience: %d/%d\\n\", \n                epoch, train_total_loss$item(), val_total_loss$item(), \n                val_accuracy, patience_counter, patience))\n  }\n  \n  # Early stopping\n  if (patience_counter &gt;= patience) {\n    cat(sprintf(\"\\nEarly stopping at epoch %d. Best validation loss: %.4f\\n\", epoch, best_val_loss))\n    break\n  }\n}\n\nEpoch 1 - Train Loss: 0.8349, Val Loss: 0.6657, Val Acc: 0.507, Patience: 0/15\nEpoch 25 - Train Loss: 0.2110, Val Loss: 0.1678, Val Acc: 0.950, Patience: 0/15\nEpoch 50 - Train Loss: 0.0987, Val Loss: 0.0626, Val Acc: 0.979, Patience: 2/15\nEpoch 75 - Train Loss: 0.0788, Val Loss: 0.0484, Val Acc: 0.993, Patience: 0/15\n\nEarly stopping at epoch 95. Best validation loss: 0.0424\n\n# Load best model state\nif (!is.null(best_model_state)) {\n  model$load_state_dict(best_model_state)\n  cat(\"Loaded best model from validation\\n\")\n}\n\nLoaded best model from validation\n\ncat(\"Training completed!\\n\")\n\nTraining completed!\n\n\n\n\n6. Model Evaluation\nThe evaluation process assesses the model’s performance on both tasks:\n\n# Set model to evaluation mode\nmodel$eval()\n\n# Make predictions on test set\nwith_no_grad({\n  outputs &lt;- model(x_test)\n  \n  # Regression evaluation\n  reg_preds &lt;- outputs$regression$squeeze()\n  reg_test_loss &lt;- regression_loss_fn(reg_preds, y_reg_test)\n  \n  # Classification evaluation\n  cls_preds &lt;- outputs$classification$squeeze()\n  cls_probs &lt;- nnf_sigmoid(cls_preds)\n  cls_test_loss &lt;- classification_loss_fn(cls_preds, y_cls_test)\n  \n  # Convert predictions to binary (threshold = 0.5)\n  cls_pred_labels &lt;- (cls_probs &gt; 0.5)$to(torch_int())\n  \n  # Calculate accuracy\n  accuracy &lt;- (cls_pred_labels == y_cls_test$to(torch_int()))$sum()$item() / length(test_idx)\n})\n\n# Calculate additional metrics\nreg_preds_r &lt;- as.numeric(reg_preds)\ny_reg_test_r &lt;- as.numeric(y_reg_test)\ncls_probs_r &lt;- as.numeric(cls_probs)\ny_cls_test_r &lt;- as.numeric(y_cls_test)\n\n# Regression metrics\nrmse &lt;- sqrt(mean((reg_preds_r - y_reg_test_r)^2))\nmae &lt;- mean(abs(reg_preds_r - y_reg_test_r))\nr_squared &lt;- cor(reg_preds_r, y_reg_test_r)^2\n\n# Classification metrics\nauc &lt;- try({\n  if(require(pROC, quietly = TRUE)) {\n    pROC::auc(pROC::roc(y_cls_test_r, cls_probs_r, quiet = TRUE))\n  } else {\n    NA\n  }\n}, silent = TRUE)\n\n# Display results\nperformance_results &lt;- data.frame(\n  Task = c(\"Regression\", \"Regression\", \"Regression\", \"Classification\", \"Classification\", \"Classification\"),\n  Metric = c(\"Test Loss (MSE)\", \"RMSE\", \"R-squared\", \"Test Loss (BCE)\", \"Accuracy\", \"AUC\"),\n  Value = c(\n    round(reg_test_loss$item(), 4),\n    round(rmse, 4),\n    round(r_squared, 4),\n    round(cls_test_loss$item(), 4),\n    round(accuracy * 100, 2), \n    round(auc * 100, 2)\n  )\n)\n\nprint(performance_results)\n\n            Task          Metric   Value\n1     Regression Test Loss (MSE)  0.0562\n2     Regression            RMSE  0.2370\n3     Regression       R-squared  0.9411\n4 Classification Test Loss (BCE)  0.0440\n5 Classification        Accuracy 98.6700\n6 Classification             AUC 99.9600\n\ncat(\"\\nClassification AUC:\", round(as.numeric(auc), 4), \"\\n\")\n\n\nClassification AUC: 0.9996 \n\n\n\n\n7. Enhanced Visualization with Overfitting Analysis\nComprehensive visualizations demonstrate the model’s performance and overfitting prevention:\n\n# Plot enhanced training history with overfitting detection\np1 &lt;- training_history %&gt;%\n  select(epoch, train_total_loss, val_total_loss) %&gt;%\n  pivot_longer(cols = c(train_total_loss, val_total_loss), \n               names_to = \"split\", values_to = \"loss\") %&gt;%\n  mutate(split = case_when(\n    split == \"train_total_loss\" ~ \"Training\",\n    split == \"val_total_loss\" ~ \"Validation\"\n  )) %&gt;%\n  ggplot(aes(x = epoch, y = loss, color = split)) +\n  geom_line(size = 1) +\n  geom_vline(xintercept = which.min(training_history$val_total_loss), \n             linetype = \"dashed\", color = \"red\", alpha = 0.7) +\n  labs(title = \"Training vs Validation Loss\",\n       subtitle = \"Red line shows optimal stopping point\",\n       x = \"Epoch\", y = \"Total Loss\", color = \"Dataset\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n# Separate task losses\np2 &lt;- training_history %&gt;%\n  select(epoch, train_reg_loss, val_reg_loss, train_cls_loss, val_cls_loss) %&gt;%\n  pivot_longer(cols = -epoch, names_to = \"metric\", values_to = \"loss\") %&gt;%\n  separate(metric, into = c(\"split\", \"task\", \"loss_type\"), sep = \"_\") %&gt;%\n  mutate(\n    split = ifelse(split == \"train\", \"Training\", \"Validation\"),\n    task = ifelse(task == \"reg\", \"Regression\", \"Classification\"),\n    metric_name = paste(split, task)\n  ) %&gt;%\n  ggplot(aes(x = epoch, y = loss, color = metric_name)) +\n  geom_line(size = 1) +\n  facet_wrap(~task, scales = \"free_y\") +\n  labs(title = \"Task-Specific Loss Curves\",\n       subtitle = \"Monitoring overfitting in individual tasks\",\n       x = \"Epoch\", y = \"Loss\", color = \"Split & Task\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set2\")\n\n# Validation accuracy progression\np3 &lt;- ggplot(training_history, aes(x = epoch, y = val_accuracy)) +\n  geom_line(color = \"#2c3e50\", size = 1) +\n  geom_hline(yintercept = max(training_history$val_accuracy), \n             linetype = \"dashed\", color = \"red\", alpha = 0.7) +\n  labs(title = \"Validation Accuracy Progression\",\n       subtitle = paste(\"Peak accuracy:\", round(max(training_history$val_accuracy), 3)),\n       x = \"Epoch\", y = \"Validation Accuracy\") +\n  theme_minimal()\n\n# Overfitting analysis\ntraining_history$overfitting_gap &lt;- training_history$train_total_loss - training_history$val_total_loss\n\np4 &lt;- ggplot(training_history, aes(x = epoch, y = overfitting_gap)) +\n  geom_line(color = \"#e74c3c\", size = 1) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  labs(title = \"Overfitting Gap Analysis\",\n       subtitle = \"Difference between training and validation loss\",\n       x = \"Epoch\", y = \"Training Loss - Validation Loss\") +\n  theme_minimal()\n\n# Regression predictions vs actual values\nregression_results &lt;- data.frame(\n  Actual = y_reg_test_r,\n  Predicted = reg_preds_r\n)\n\np5 &lt;- ggplot(regression_results, aes(x = Actual, y = Predicted)) +\n  geom_point(alpha = 0.6, color = \"#2c3e50\") +\n  geom_abline(slope = 1, intercept = 0, color = \"#e74c3c\", linetype = \"dashed\", size = 1) +\n  geom_smooth(method = \"lm\", color = \"#3498db\", se = TRUE) +\n  labs(title = \"Regression Task: Actual vs Predicted Values\",\n       subtitle = paste(\"R² =\", round(r_squared, 3), \", RMSE =\", round(rmse, 3)),\n       x = \"Actual Values\", y = \"Predicted Values\") +\n  theme_minimal()\n\n# Classification probability distribution\ncls_results &lt;- data.frame(\n  Probability = cls_probs_r,\n  Actual_Class = factor(y_cls_test_r, labels = c(\"Class 0\", \"Class 1\"))\n)\n\np6 &lt;- ggplot(cls_results, aes(x = Probability, fill = Actual_Class)) +\n  geom_histogram(alpha = 0.7, bins = 20, position = \"identity\") +\n  geom_vline(xintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Classification Task: Predicted Probabilities\",\n       subtitle = paste(\"Accuracy =\", round(accuracy * 100, 1), \"%\"),\n       x = \"Predicted Probability\", y = \"Count\", fill = \"Actual Class\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\")\n\n# Combine plots\nlibrary(patchwork)\n(p1 | p3) / (p2) / (p4) / (p5 | p6)\n\n\n\n\n\n\n\n\n\n\n8. Feature Importance Analysis\nAnalyse what the shared layers learned:\n\n# Extract weights from the first shared layer\nshared_weights &lt;- as.matrix(model$shared_layer1$weight$detach())\n\n# Create feature importance visualization\nfeature_importance_df &lt;- data.frame(\n  Feature = rep(paste0(\"Feature_\", 1:5), each = 30),\n  Neuron = rep(1:30, times = 5),\n  Weight = as.vector(t(shared_weights))\n)\n\n# Calculate average absolute importance per feature\nfeature_avg_importance &lt;- feature_importance_df %&gt;%\n  group_by(Feature) %&gt;%\n  summarise(\n    Avg_Abs_Weight = mean(abs(Weight)),\n    Std_Weight = sd(Weight)\n  ) %&gt;%\n  arrange(desc(Avg_Abs_Weight))\n\n# Plot feature importance\np5 &lt;- ggplot(feature_avg_importance, aes(x = reorder(Feature, Avg_Abs_Weight), \n                                        y = Avg_Abs_Weight)) +\n  geom_col(fill = \"#3498db\", alpha = 0.8) +\n  geom_errorbar(aes(ymin = Avg_Abs_Weight - Std_Weight, \n                    ymax = Avg_Abs_Weight + Std_Weight),\n                width = 0.2, color = \"#2c3e50\") +\n  coord_flip() +\n  labs(title = \"Feature Importance in Shared Layers\",\n       subtitle = \"Average absolute weights from first shared layer\",\n       x = \"Features\", y = \"Average Absolute Weight\") +\n  theme_minimal()\n\n# Weight distribution heatmap\np6 &lt;- ggplot(feature_importance_df, aes(x = Neuron, y = Feature, fill = Weight)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"#e74c3c\", mid = \"white\", high = \"#2c3e50\", \n                       midpoint = 0, name = \"Weight\") +\n  labs(title = \"Shared Layer Weight Distribution\",\n       subtitle = \"How each feature connects to shared neurons\",\n       x = \"Neuron Index\", y = \"Input Features\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank())\n\np5 | p6"
  },
  {
    "objectID": "posts/multi-task-learning-with-torch.html#comparing-multi-task-vs-single-task-performance",
    "href": "posts/multi-task-learning-with-torch.html#comparing-multi-task-vs-single-task-performance",
    "title": "Multi-Task Learning with torch in R",
    "section": "Comparing Multi-Task vs Single-Task Performance",
    "text": "Comparing Multi-Task vs Single-Task Performance\nThe following comparison analyzes the multi-task model against individual single-task models:\n\n# Define single-task networks\nsingle_task_regression &lt;- nn_module(\n  \"SingleTaskRegression\",\n  initialize = function(input_size, hidden_size) {\n    self$layer1 &lt;- nn_linear(input_size, hidden_size)\n    self$layer2 &lt;- nn_linear(hidden_size, hidden_size)\n    self$output &lt;- nn_linear(hidden_size, 1)\n    self$dropout &lt;- nn_dropout(0.2)\n  },\n  forward = function(x) {\n    x %&gt;%\n      self$layer1() %&gt;%\n      nnf_relu() %&gt;%\n      self$dropout() %&gt;%\n      self$layer2() %&gt;%\n      nnf_relu() %&gt;%\n      self$dropout() %&gt;%\n      self$output()\n  }\n)\n\nsingle_task_classification &lt;- nn_module(\n  \"SingleTaskClassification\", \n  initialize = function(input_size, hidden_size) {\n    self$layer1 &lt;- nn_linear(input_size, hidden_size)\n    self$layer2 &lt;- nn_linear(hidden_size, hidden_size)\n    self$output &lt;- nn_linear(hidden_size, 1)\n    self$dropout &lt;- nn_dropout(0.2)\n  },\n  forward = function(x) {\n    x %&gt;%\n      self$layer1() %&gt;%\n      nnf_relu() %&gt;%\n      self$dropout() %&gt;%\n      self$layer2() %&gt;%\n      nnf_relu() %&gt;%\n      self$dropout() %&gt;%\n      self$output()\n  }\n)\n\n# Train single-task models quickly for comparison\ntrain_single_task &lt;- function(model, loss_fn, target, task_type = \"regression\", epochs = 100) {\n  optimizer &lt;- optim_adam(model$parameters, lr = 0.01)\n  \n  for(epoch in 1:epochs) {\n    model$train()\n    optimizer$zero_grad()\n    output &lt;- model(x_train)\n    loss &lt;- loss_fn(output$squeeze(), target)\n    loss$backward()\n    optimizer$step()\n  }\n  \n  # Evaluate\n  model$eval()\n  with_no_grad({\n    test_output &lt;- model(x_test)\n    if(task_type == \"regression\") {\n      test_loss &lt;- loss_fn(test_output$squeeze(), y_reg_test)\n      return(list(loss = test_loss$item(), type = \"regression\"))\n    } else {\n      test_loss &lt;- loss_fn(test_output$squeeze(), y_cls_test)\n      probs &lt;- nnf_sigmoid(test_output$squeeze())\n      preds &lt;- (probs &gt; 0.5)$to(torch_int())\n      accuracy &lt;- (preds == y_cls_test$to(torch_int()))$sum()$item() / length(test_idx)\n      return(list(loss = test_loss$item(), accuracy = accuracy, type = \"classification\"))\n    }\n  })\n}\n\n# Train single-task models\ncat(\"Training single-task models for comparison...\\n\")\n\nTraining single-task models for comparison...\n\nreg_model &lt;- single_task_regression(5, 64)\ncls_model &lt;- single_task_classification(5, 64)\n\nreg_results &lt;- train_single_task(reg_model, nnf_mse_loss, y_reg_train, \"regression\")\ncls_results &lt;- train_single_task(cls_model, nnf_binary_cross_entropy_with_logits, y_cls_train, \"classification\")\n\n# Create comparison\ncomparison_df &lt;- data.frame(\n  Model = c(\"Multi-Task\", \"Single-Task\", \"Multi-Task\", \"Single-Task\"),\n  Task = c(\"Regression\", \"Regression\", \"Classification\", \"Classification\"),\n  Metric = c(\"MSE Loss\", \"MSE Loss\", \"Accuracy (%)\", \"Accuracy (%)\"),\n  Value = c(\n    reg_test_loss$item(),\n    reg_results$loss,\n    accuracy * 100,\n    cls_results$accuracy * 100\n  )\n)\n\nprint(comparison_df)\n\n        Model           Task       Metric       Value\n1  Multi-Task     Regression     MSE Loss  0.04065187\n2 Single-Task     Regression     MSE Loss  0.03902529\n3  Multi-Task Classification Accuracy (%) 99.33333333\n4 Single-Task Classification Accuracy (%) 98.66666667\n\n# Visualize comparison\nggplot(comparison_df, aes(x = Task, y = Value, fill = Model)) +\n  geom_col(position = \"dodge\", alpha = 0.8) +\n  facet_wrap(~Metric, scales = \"free_y\") +\n  labs(title = \"Multi-Task vs Single-Task Performance Comparison\",\n       subtitle = \"Lower loss and higher accuracy are better\",\n       x = \"Task\", y = \"Performance Value\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\")"
  },
  {
    "objectID": "posts/multi-task-learning-with-torch.html#benefits-of-multi-task-learning",
    "href": "posts/multi-task-learning-with-torch.html#benefits-of-multi-task-learning",
    "title": "Multi-Task Learning with torch in R",
    "section": "Benefits of Multi-Task Learning",
    "text": "Benefits of Multi-Task Learning\nBased on our implementation, we can see several key benefits:\n\n# Calculate parameter efficiency\nmulti_task_params &lt;- sum(sapply(model$parameters, function(p) prod(p$shape)))\nsingle_task_params &lt;- sum(sapply(reg_model$parameters, function(p) prod(p$shape))) + \n                     sum(sapply(cls_model$parameters, function(p) prod(p$shape)))\n\nefficiency_gain &lt;- (single_task_params - multi_task_params) / single_task_params * 100\n\ncat(\"Benefits of Multi-Task Learning:\\n\")\n\nBenefits of Multi-Task Learning:\n\ncat(\"================================\\n\\n\")\n\n================================\n\ncat(\"1. Parameter Efficiency:\\n\")\n\n1. Parameter Efficiency:\n\ncat(\"   - Multi-task model parameters:\", multi_task_params, \"\\n\")\n\n   - Multi-task model parameters: 4674 \n\ncat(\"   - Combined single-task parameters:\", single_task_params, \"\\n\")\n\n   - Combined single-task parameters: 9218 \n\ncat(\"   - Parameter reduction:\", round(efficiency_gain, 1), \"%\\n\\n\")\n\n   - Parameter reduction: 49.3 %\n\ncat(\"2. Improved Generalization:\\n\")\n\n2. Improved Generalization:\n\ncat(\"   - Shared representations benefit both tasks\\n\")\n\n   - Shared representations benefit both tasks\n\ncat(\"   - Regularization effect from multiple objectives\\n\\n\")\n\n   - Regularization effect from multiple objectives\n\ncat(\"3. Training Efficiency:\\n\")\n\n3. Training Efficiency:\n\ncat(\"   - Single training loop for multiple tasks\\n\")\n\n   - Single training loop for multiple tasks\n\ncat(\"   - Shared gradient updates across tasks\\n\\n\")\n\n   - Shared gradient updates across tasks\n\ncat(\"4. Knowledge Transfer:\\n\")\n\n4. Knowledge Transfer:\n\ncat(\"   - Features learned for one task help the other\\n\")\n\n   - Features learned for one task help the other\n\ncat(\"   - Especially valuable with limited data\\n\")\n\n   - Especially valuable with limited data"
  },
  {
    "objectID": "posts/multi-task-learning-with-torch.html#extensions-and-advanced-techniques",
    "href": "posts/multi-task-learning-with-torch.html#extensions-and-advanced-techniques",
    "title": "Multi-Task Learning with torch in R",
    "section": "Extensions and Advanced Techniques",
    "text": "Extensions and Advanced Techniques\nHere are some advanced techniques you can explore:\n\n# 1. Dynamic Task Weighting\n# Adjust task weights during training based on performance\ndynamic_weight_scheduler &lt;- function(epoch, reg_loss, cls_loss) {\n  # Example: Increase weight for task with higher loss\n  total_loss &lt;- reg_loss + cls_loss\n  reg_weight &lt;- cls_loss / total_loss\n  cls_weight &lt;- reg_loss / total_loss\n  return(c(regression = reg_weight, classification = cls_weight))\n}\n\n# 2. Task-Specific Learning Rates\n# Different learning rates for different parts of the network\nshared_optimizer &lt;- optim_adam(model$shared_layer1$parameters, lr = 0.01)\nreg_optimizer &lt;- optim_adam(model$regression_layer$parameters, lr = 0.005)\ncls_optimizer &lt;- optim_adam(model$classification_layer$parameters, lr = 0.02)\n\n# 3. Uncertainty-Based Weighting\n# Weight tasks based on prediction uncertainty\nuncertainty_weight &lt;- function(predictions, targets) {\n  # Calculate prediction uncertainty and adjust weights accordingly\n  # Lower uncertainty → higher confidence → lower weight\n  variance &lt;- torch_var(predictions - targets)\n  return(1 / (1 + variance))\n}\n\n# 4. Gradient Balancing\n# Ensure gradients from different tasks are properly balanced\ngradient_clipping &lt;- function(model, max_norm = 1.0) {\n  torch_nn_utils_clip_grad_norm_(model$parameters, max_norm)\n}\n\ncat(\"Advanced techniques to explore:\\n\")\ncat(\"- Dynamic task weighting based on performance\\n\")\ncat(\"- Task-specific learning rates\\n\") \ncat(\"- Uncertainty-based loss weighting\\n\")\ncat(\"- Gradient clipping and balancing\\n\")\ncat(\"- Adding more tasks (e.g., ranking, clustering)\\n\")\ncat(\"- Hard parameter sharing vs soft parameter sharing\\n\")"
  },
  {
    "objectID": "posts/multi-task-learning-with-torch.html#key-takeaways",
    "href": "posts/multi-task-learning-with-torch.html#key-takeaways",
    "title": "Multi-Task Learning with torch in R",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nArchitecture Design: The shared-private paradigm enables models to learn both common and task-specific representations\nLoss Combination: Properly weighting multiple loss functions proves crucial for balanced learning across tasks\nEvaluation Strategy: Each task requires appropriate metrics, and overall model success depends on performance across all tasks\nParameter Efficiency: Multi-task models can achieve comparable performance with fewer total parameters when properly regularized\nKnowledge Transfer: Related tasks can benefit from shared feature learning, especially when data is limited"
  },
  {
    "objectID": "posts/multi-task-learning-with-torch.html#next-steps",
    "href": "posts/multi-task-learning-with-torch.html#next-steps",
    "title": "Multi-Task Learning with torch in R",
    "section": "Next Steps",
    "text": "Next Steps\nTo further explore multi-task learning with torch, consider:\n\nExperimenting with different task combinations (regression + classification + ranking)\nImplementing attention mechanisms to dynamically weight task contributions\nUsing more sophisticated architectures (ResNet-style connections, attention layers)\nApplying multi-task learning to real-world datasets\nExploring meta-learning approaches for task adaptation\nImplementing cross-stitch networks for more flexible parameter sharing\n\nMulti-task learning represents a powerful paradigm in machine learning that can lead to more efficient and generalizable models, especially when dealing with related prediction problems."
  },
  {
    "objectID": "posts/multi-task-learning-with-torch.html#try-it-yourself",
    "href": "posts/multi-task-learning-with-torch.html#try-it-yourself",
    "title": "Multi-Task Learning with torch in R",
    "section": "Try It Yourself",
    "text": "Try It Yourself\nWant to experiment with this code interactively? Try it out in your browser:\n\nInteractive R with WebR - Run R code directly in your browser\nWebR Playground - Advanced interactive R environment with code editing"
  },
  {
    "objectID": "posts/multi-task-learning-with-torch.html#additional-resources",
    "href": "posts/multi-task-learning-with-torch.html#additional-resources",
    "title": "Multi-Task Learning with torch in R",
    "section": "Additional Resources",
    "text": "Additional Resources\nFor more R programming tutorials and advanced techniques:\n\nR’tichoke Home - Main site with all tutorials\nAll Posts - Browse our complete collection\nGetting Started with R - Beginner-friendly tutorials\nInstallation Guide - Set up your R environment"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "R’tichoke is a dedicated repository of R programming content, tutorials, and resources.\nFeel free to reach out on LinkedIn!"
  },
  {
    "objectID": "about.html#about-rtichoke",
    "href": "about.html#about-rtichoke",
    "title": "About",
    "section": "",
    "text": "R’tichoke is a dedicated repository of R programming content, tutorials, and resources.\nFeel free to reach out on LinkedIn!"
  },
  {
    "objectID": "about.html#how-this-site-works",
    "href": "about.html#how-this-site-works",
    "title": "About",
    "section": "How This Site Works",
    "text": "How This Site Works\nAll content on this site is created using Quarto, a next-generation version of R Markdown. Quarto is great for:\n\nWriting content in .qmd files (Quarto Markdown)\nIncluding executable R code chunks\nGenerating beautiful, interactive web pages\nCreating a consistent, organized structure"
  },
  {
    "objectID": "about.html#r-resources",
    "href": "about.html#r-resources",
    "title": "About",
    "section": "R Resources",
    "text": "R Resources\nHere are some additional resources for learning R:\n\nR Project for Statistical Computing\nRStudio\nTidyverse\nR for Data Science\n\nDisclaimer: Parts of this website (including posts) have been generated using GenAI."
  },
  {
    "objectID": "installation.html#verification",
    "href": "installation.html#verification",
    "title": "Installing R and RStudio",
    "section": "Verification",
    "text": "Verification\nTo verify installation:\n\nOpen RStudio\nIn the Console, type:\nR.version\nPress Enter to display R installation information"
  },
  {
    "objectID": "installation.html#package-installation",
    "href": "installation.html#package-installation",
    "title": "Installing R and RStudio",
    "section": "Package Installation",
    "text": "Package Installation\nInstall packages using:\ninstall.packages(\"packagename\")\nExample - tidyverse installation:\ninstall.packages(\"tidyverse\")\n\nEssential Packages\nRecommended packages for beginners:\ninstall.packages(\"dplyr\")      # Data manipulation and visualization\ninstall.packages(\"rmarkdown\")  # Dynamic documents\ninstall.packages(\"knitr\")      # Report generation\ninstall.packages(\"shiny\")      # Interactive web applications"
  },
  {
    "objectID": "get-started/base-graphics.html",
    "href": "get-started/base-graphics.html",
    "title": "Introduction to Base Graphics in R",
    "section": "",
    "text": "R comes with a built-in graphics system known as “base graphics.” These functions provide a straightforward way to create plots without requiring additional packages.\n\nBasic Plot Types\n\nScatter Plots\n\n# Basic scatter plot\nplot(mtcars$wt, mtcars$mpg, \n     main = \"Car Weight vs. Fuel Efficiency\",\n     xlab = \"Weight (1000 lbs)\",\n     ylab = \"Miles Per Gallon\",\n     pch = 19,  # Solid circle point type\n     col = \"blue\")\n\n# Add a grid\ngrid()\n\n# Add a trend line\nabline(lm(mpg ~ wt, data = mtcars), col = \"red\", lwd = 2)\n\n# Add a legend\nlegend(\"topright\", \n       legend = \"Trend Line\", \n       col = \"red\", \n       lwd = 2)\n\n\n\n\n\n\n\n\n\n\nLine Plots\n\n# Create some data\nx &lt;- 1:10\ny &lt;- x^2\n\n# Basic line plot\nplot(x, y, \n     type = \"l\",  # 'l' for line\n     main = \"Line Plot Example\",\n     xlab = \"X Values\",\n     ylab = \"Y Values\",\n     col = \"darkgreen\",\n     lwd = 2)\n\n# Add points to the line\npoints(x, y, pch = 19, col = \"darkgreen\")\n\n# Add another line\nlines(x, 2*x, col = \"blue\", lwd = 2)\n\n# Add a legend\nlegend(\"topleft\", \n       legend = c(\"y = x^2\", \"y = 2x\"), \n       col = c(\"darkgreen\", \"blue\"), \n       lwd = 2)\n\n\n\n\n\n\n\n\n\n\nBar Plots\n\n# Create a simple frequency table\ncylinders &lt;- table(mtcars$cyl)\n\n# Basic bar plot\nbarplot(cylinders,\n        main = \"Car Cylinders Distribution\",\n        xlab = \"Number of Cylinders\",\n        ylab = \"Frequency\",\n        col = c(\"lightblue\", \"skyblue\", \"steelblue\"),\n        border = \"white\")\n\n\n\n\n\n\n\n# Add text labels on top of bars\ntext(x = barplot(cylinders), \n     y = cylinders + 1, \n     labels = cylinders, \n     col = \"black\")\n\n\n\n\n\n\n\n\n\n\nHistograms\n\n# Basic histogram\nhist(mtcars$mpg,\n     main = \"Distribution of Fuel Efficiency\",\n     xlab = \"Miles Per Gallon\",\n     col = \"lightgreen\",\n     border = \"white\",\n     breaks = 10)  # Number of bins\n\n\n\n\n\n\n\n# Add a density curve\nhist(mtcars$mpg,\n     main = \"Distribution with Density Curve\",\n     xlab = \"Miles Per Gallon\",\n     col = \"lightgreen\",\n     border = \"white\",\n     freq = FALSE)  # Show density instead of frequency\n\n# Add a normal density curve\ncurve(dnorm(x, mean = mean(mtcars$mpg), sd = sd(mtcars$mpg)),\n      add = TRUE, col = \"darkred\", lwd = 2)\n\n# Add a legend\nlegend(\"topright\", \n       legend = \"Normal Distribution\", \n       col = \"darkred\", \n       lwd = 2)\n\n\n\n\n\n\n\n\n\n\nBox Plots\n\n# Basic box plot\nboxplot(mpg ~ cyl, data = mtcars,\n        main = \"Fuel Efficiency by Cylinder Count\",\n        xlab = \"Number of Cylinders\",\n        ylab = \"Miles Per Gallon\",\n        col = c(\"lightpink\", \"lightblue\", \"lightgreen\"))\n\n# Add a title to the plot\ntitle(\"Comparison of MPG Distribution\")\n\n\n\n\n\n\n\n\n\n\n\nCustomizing Plots\n\nPlot Parameters\n\n# Create a customized scatter plot\nplot(mtcars$wt, mtcars$mpg,\n     main = \"Customized Scatter Plot\",\n     xlab = \"Weight (1000 lbs)\",\n     ylab = \"Miles Per Gallon\",\n     pch = 16,       # Point type\n     col = \"purple\", # Point color\n     cex = 1.5,      # Point size\n     type = \"p\",     # Plot type ('p' for points)\n     lwd = 2,        # Line width\n     bty = \"l\",      # Box type ('l' for L-shaped)\n     xlim = c(1, 6), # X-axis limits\n     ylim = c(10, 35) # Y-axis limits\n)\n\n\n\n\n\n\n\n\n\n\nColors and Point Types\n\n# Create a plot showing different point types and colors\nplot(1:20, 1:20, \n     type = \"n\",  # 'n' for no plotting\n     main = \"Point Types (pch) in R\",\n     xlab = \"Point Type (pch value)\",\n     ylab = \"\")\n\n# Add points with different pch values\nfor (i in 1:20) {\n  points(i, 10, pch = i, cex = 2)\n  text(i, 12, labels = i)\n}\n\n\n\n\n\n\n\n# Show different colors\nplot(1:8, rep(1, 8), \n     type = \"n\",\n     main = \"Basic Colors in R\",\n     xlab = \"\", ylab = \"\",\n     xlim = c(0.5, 8.5), ylim = c(0, 2),\n     axes = FALSE)\n\ncolors &lt;- c(\"black\", \"red\", \"green\", \"blue\", \"cyan\", \"magenta\", \"yellow\", \"gray\")\nfor (i in 1:8) {\n  points(i, 1, pch = 19, col = colors[i], cex = 3)\n  text(i, 0.7, labels = colors[i])\n}\n\n\n\n\n\n\n\n\n\n\n\nMultiple Plots in One Figure\n\n# Set up a 2x2 plotting area\npar(mfrow = c(2, 2))\n\n# Plot 1: Scatter plot\nplot(mtcars$wt, mtcars$mpg, main = \"Weight vs MPG\", pch = 19)\n\n# Plot 2: Histogram\nhist(mtcars$mpg, main = \"MPG Distribution\", col = \"lightblue\")\n\n# Plot 3: Box plot\nboxplot(mpg ~ cyl, data = mtcars, main = \"MPG by Cylinders\", col = \"lightgreen\")\n\n# Plot 4: Bar plot\nbarplot(table(mtcars$gear), main = \"Gear Count\", col = \"salmon\")\n\n\n\n\n\n\n\n# Reset to 1x1 plotting area\npar(mfrow = c(1, 1))\n\n\n\nAdding Elements to Plots\n\n# Create a basic plot\nplot(mtcars$wt, mtcars$mpg, \n     type = \"n\",  # Start with an empty plot\n     main = \"Car Weight vs. Fuel Efficiency\",\n     xlab = \"Weight (1000 lbs)\",\n     ylab = \"Miles Per Gallon\")\n\n# Add points with different colors by cylinder\ncyl_colors &lt;- c(\"red\", \"green\", \"blue\")\nfor (i in unique(mtcars$cyl)) {\n  subset_idx &lt;- mtcars$cyl == i\n  points(mtcars$wt[subset_idx], mtcars$mpg[subset_idx], \n         col = cyl_colors[i/4],  # 4,6,8 cylinders mapped to colors\n         pch = 19)\n}\n\n# Add a legend\nlegend(\"topright\", \n       legend = c(\"4 cylinders\", \"6 cylinders\", \"8 cylinders\"), \n       col = cyl_colors, \n       pch = 19)\n\n# Add text annotations\ntext(mtcars$wt[mtcars$mpg &gt; 30], mtcars$mpg[mtcars$mpg &gt; 30], \n     labels = rownames(mtcars)[mtcars$mpg &gt; 30],\n     pos = 4)  # Position 4 is to the right\n\n# Add a horizontal line at mean MPG\nabline(h = mean(mtcars$mpg), lty = 2, col = \"darkgray\")\ntext(5, mean(mtcars$mpg) + 1, \"Mean MPG\", col = \"darkgray\")\n\n\n\n\n\n\n\n\n\n\nSaving Plots\n\n# Example of how to save a plot (not run)\n# png(\"my_plot.png\", width = 800, height = 600)\nplot(mtcars$wt, mtcars$mpg, main = \"Plot to Save\", pch = 19, col = \"blue\")\n\n\n\n\n\n\n\n# dev.off()  # Close the device to save the file\n\n# Other formats\n# pdf(\"my_plot.pdf\", width = 8, height = 6)\n# jpeg(\"my_plot.jpg\", width = 800, height = 600, quality = 100)\n# svg(\"my_plot.svg\", width = 8, height = 6)\n\nBase graphics in R provide a foundation for creating plots. While newer packages like ggplot2 offer more sophisticated options, base graphics remain valuable for quick visualizations and understanding plotting fundamentals in R."
  },
  {
    "objectID": "get-started/base-graphics.html#introduction-to-base-graphics-in-r",
    "href": "get-started/base-graphics.html#introduction-to-base-graphics-in-r",
    "title": "Introduction to Base Graphics in R",
    "section": "",
    "text": "R comes with a built-in graphics system known as “base graphics.” These functions provide a straightforward way to create plots without requiring additional packages.\n\n\n\n\n\n# Basic scatter plot\nplot(mtcars$wt, mtcars$mpg, \n     main = \"Car Weight vs. Fuel Efficiency\",\n     xlab = \"Weight (1000 lbs)\",\n     ylab = \"Miles Per Gallon\",\n     pch = 19,  # Solid circle point type\n     col = \"blue\")\n\n# Add a grid\ngrid()\n\n# Add a trend line\nabline(lm(mpg ~ wt, data = mtcars), col = \"red\", lwd = 2)\n\n# Add a legend\nlegend(\"topright\", \n       legend = \"Trend Line\", \n       col = \"red\", \n       lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n# Create some data\nx &lt;- 1:10\ny &lt;- x^2\n\n# Basic line plot\nplot(x, y, \n     type = \"l\",  # 'l' for line\n     main = \"Line Plot Example\",\n     xlab = \"X Values\",\n     ylab = \"Y Values\",\n     col = \"darkgreen\",\n     lwd = 2)\n\n# Add points to the line\npoints(x, y, pch = 19, col = \"darkgreen\")\n\n# Add another line\nlines(x, 2*x, col = \"blue\", lwd = 2)\n\n# Add a legend\nlegend(\"topleft\", \n       legend = c(\"y = x^2\", \"y = 2x\"), \n       col = c(\"darkgreen\", \"blue\"), \n       lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n# Create a simple frequency table\ncylinders &lt;- table(mtcars$cyl)\n\n# Basic bar plot\nbarplot(cylinders,\n        main = \"Car Cylinders Distribution\",\n        xlab = \"Number of Cylinders\",\n        ylab = \"Frequency\",\n        col = c(\"lightblue\", \"skyblue\", \"steelblue\"),\n        border = \"white\")\n\n\n\n\n\n\n\n# Add text labels on top of bars\ntext(x = barplot(cylinders), \n     y = cylinders + 1, \n     labels = cylinders, \n     col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\n# Basic histogram\nhist(mtcars$mpg,\n     main = \"Distribution of Fuel Efficiency\",\n     xlab = \"Miles Per Gallon\",\n     col = \"lightgreen\",\n     border = \"white\",\n     breaks = 10)  # Number of bins\n\n\n\n\n\n\n\n# Add a density curve\nhist(mtcars$mpg,\n     main = \"Distribution with Density Curve\",\n     xlab = \"Miles Per Gallon\",\n     col = \"lightgreen\",\n     border = \"white\",\n     freq = FALSE)  # Show density instead of frequency\n\n# Add a normal density curve\ncurve(dnorm(x, mean = mean(mtcars$mpg), sd = sd(mtcars$mpg)),\n      add = TRUE, col = \"darkred\", lwd = 2)\n\n# Add a legend\nlegend(\"topright\", \n       legend = \"Normal Distribution\", \n       col = \"darkred\", \n       lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n# Basic box plot\nboxplot(mpg ~ cyl, data = mtcars,\n        main = \"Fuel Efficiency by Cylinder Count\",\n        xlab = \"Number of Cylinders\",\n        ylab = \"Miles Per Gallon\",\n        col = c(\"lightpink\", \"lightblue\", \"lightgreen\"))\n\n# Add a title to the plot\ntitle(\"Comparison of MPG Distribution\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Create a customized scatter plot\nplot(mtcars$wt, mtcars$mpg,\n     main = \"Customized Scatter Plot\",\n     xlab = \"Weight (1000 lbs)\",\n     ylab = \"Miles Per Gallon\",\n     pch = 16,       # Point type\n     col = \"purple\", # Point color\n     cex = 1.5,      # Point size\n     type = \"p\",     # Plot type ('p' for points)\n     lwd = 2,        # Line width\n     bty = \"l\",      # Box type ('l' for L-shaped)\n     xlim = c(1, 6), # X-axis limits\n     ylim = c(10, 35) # Y-axis limits\n)\n\n\n\n\n\n\n\n\n\n\n\n\n# Create a plot showing different point types and colors\nplot(1:20, 1:20, \n     type = \"n\",  # 'n' for no plotting\n     main = \"Point Types (pch) in R\",\n     xlab = \"Point Type (pch value)\",\n     ylab = \"\")\n\n# Add points with different pch values\nfor (i in 1:20) {\n  points(i, 10, pch = i, cex = 2)\n  text(i, 12, labels = i)\n}\n\n\n\n\n\n\n\n# Show different colors\nplot(1:8, rep(1, 8), \n     type = \"n\",\n     main = \"Basic Colors in R\",\n     xlab = \"\", ylab = \"\",\n     xlim = c(0.5, 8.5), ylim = c(0, 2),\n     axes = FALSE)\n\ncolors &lt;- c(\"black\", \"red\", \"green\", \"blue\", \"cyan\", \"magenta\", \"yellow\", \"gray\")\nfor (i in 1:8) {\n  points(i, 1, pch = 19, col = colors[i], cex = 3)\n  text(i, 0.7, labels = colors[i])\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Set up a 2x2 plotting area\npar(mfrow = c(2, 2))\n\n# Plot 1: Scatter plot\nplot(mtcars$wt, mtcars$mpg, main = \"Weight vs MPG\", pch = 19)\n\n# Plot 2: Histogram\nhist(mtcars$mpg, main = \"MPG Distribution\", col = \"lightblue\")\n\n# Plot 3: Box plot\nboxplot(mpg ~ cyl, data = mtcars, main = \"MPG by Cylinders\", col = \"lightgreen\")\n\n# Plot 4: Bar plot\nbarplot(table(mtcars$gear), main = \"Gear Count\", col = \"salmon\")\n\n\n\n\n\n\n\n# Reset to 1x1 plotting area\npar(mfrow = c(1, 1))\n\n\n\n\n\n# Create a basic plot\nplot(mtcars$wt, mtcars$mpg, \n     type = \"n\",  # Start with an empty plot\n     main = \"Car Weight vs. Fuel Efficiency\",\n     xlab = \"Weight (1000 lbs)\",\n     ylab = \"Miles Per Gallon\")\n\n# Add points with different colors by cylinder\ncyl_colors &lt;- c(\"red\", \"green\", \"blue\")\nfor (i in unique(mtcars$cyl)) {\n  subset_idx &lt;- mtcars$cyl == i\n  points(mtcars$wt[subset_idx], mtcars$mpg[subset_idx], \n         col = cyl_colors[i/4],  # 4,6,8 cylinders mapped to colors\n         pch = 19)\n}\n\n# Add a legend\nlegend(\"topright\", \n       legend = c(\"4 cylinders\", \"6 cylinders\", \"8 cylinders\"), \n       col = cyl_colors, \n       pch = 19)\n\n# Add text annotations\ntext(mtcars$wt[mtcars$mpg &gt; 30], mtcars$mpg[mtcars$mpg &gt; 30], \n     labels = rownames(mtcars)[mtcars$mpg &gt; 30],\n     pos = 4)  # Position 4 is to the right\n\n# Add a horizontal line at mean MPG\nabline(h = mean(mtcars$mpg), lty = 2, col = \"darkgray\")\ntext(5, mean(mtcars$mpg) + 1, \"Mean MPG\", col = \"darkgray\")\n\n\n\n\n\n\n\n\n\n\n\n\n# Example of how to save a plot (not run)\n# png(\"my_plot.png\", width = 800, height = 600)\nplot(mtcars$wt, mtcars$mpg, main = \"Plot to Save\", pch = 19, col = \"blue\")\n\n\n\n\n\n\n\n# dev.off()  # Close the device to save the file\n\n# Other formats\n# pdf(\"my_plot.pdf\", width = 8, height = 6)\n# jpeg(\"my_plot.jpg\", width = 800, height = 600, quality = 100)\n# svg(\"my_plot.svg\", width = 8, height = 6)\n\nBase graphics in R provide a foundation for creating plots. While newer packages like ggplot2 offer more sophisticated options, base graphics remain valuable for quick visualizations and understanding plotting fundamentals in R."
  },
  {
    "objectID": "get-started/data-wrangling-with-dplyr.html",
    "href": "get-started/data-wrangling-with-dplyr.html",
    "title": "Data Wrangling with dplyr",
    "section": "",
    "text": "The dplyr package is part of the tidyverse and provides a grammar for data manipulation in R. This post demonstrates essential data wrangling techniques using built-in datasets."
  },
  {
    "objectID": "get-started/data-wrangling-with-dplyr.html#setup",
    "href": "get-started/data-wrangling-with-dplyr.html#setup",
    "title": "Data Wrangling with dplyr",
    "section": "Setup",
    "text": "Setup\nFirst, load the necessary packages:\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(knitr)"
  },
  {
    "objectID": "get-started/data-wrangling-with-dplyr.html#working-with-the-mtcars-dataset",
    "href": "get-started/data-wrangling-with-dplyr.html#working-with-the-mtcars-dataset",
    "title": "Data Wrangling with dplyr",
    "section": "Working with the mtcars Dataset",
    "text": "Working with the mtcars Dataset\nThe built-in mtcars dataset is used for examples:\n\n# Look at the mtcars data\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…"
  },
  {
    "objectID": "get-started/data-wrangling-with-dplyr.html#basic-dplyr-functions",
    "href": "get-started/data-wrangling-with-dplyr.html#basic-dplyr-functions",
    "title": "Data Wrangling with dplyr",
    "section": "Basic dplyr Functions",
    "text": "Basic dplyr Functions\n\nFiltering Rows\n\n# Find all cars with 6 cylinders\nsix_cyl &lt;- mtcars %&gt;% \n  filter(cyl == 6)\n\n# Show the first few rows\nhead(six_cyl) %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nValiant\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\nMerc 280\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\nMerc 280C\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n\n\n\n\n\nSelecting Columns\n\n# Select only specific columns\ncar_data &lt;- mtcars %&gt;% \n  select(mpg, cyl, hp, wt)\n\nhead(car_data) %&gt;%\n  kable()\n\n\n\n\n\nmpg\ncyl\nhp\nwt\n\n\n\n\nMazda RX4\n21.0\n6\n110\n2.620\n\n\nMazda RX4 Wag\n21.0\n6\n110\n2.875\n\n\nDatsun 710\n22.8\n4\n93\n2.320\n\n\nHornet 4 Drive\n21.4\n6\n110\n3.215\n\n\nHornet Sportabout\n18.7\n8\n175\n3.440\n\n\nValiant\n18.1\n6\n105\n3.460\n\n\n\n\n\n\n\nArranging Rows\n\n# Find the cars with best fuel efficiency\nmost_efficient &lt;- mtcars %&gt;% \n  arrange(desc(mpg)) %&gt;%\n  select(mpg, cyl, hp, wt)\n\nhead(most_efficient) %&gt;%\n  kable()\n\n\n\n\n\nmpg\ncyl\nhp\nwt\n\n\n\n\nToyota Corolla\n33.9\n4\n65\n1.835\n\n\nFiat 128\n32.4\n4\n66\n2.200\n\n\nHonda Civic\n30.4\n4\n52\n1.615\n\n\nLotus Europa\n30.4\n4\n113\n1.513\n\n\nFiat X1-9\n27.3\n4\n66\n1.935\n\n\nPorsche 914-2\n26.0\n4\n91\n2.140\n\n\n\n\n\n\n\nCreating New Variables\n\n# Calculate power-to-weight ratio\ncar_stats &lt;- mtcars %&gt;% \n  mutate(\n    power_to_weight = hp / wt,\n    efficiency_score = mpg * (1/wt)\n  ) %&gt;%\n  select(mpg, hp, wt, power_to_weight, efficiency_score)\n\nhead(car_stats) %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\nhp\nwt\npower_to_weight\nefficiency_score\n\n\n\n\nMazda RX4\n21.0\n110\n2.620\n41.98473\n8.015267\n\n\nMazda RX4 Wag\n21.0\n110\n2.875\n38.26087\n7.304348\n\n\nDatsun 710\n22.8\n93\n2.320\n40.08621\n9.827586\n\n\nHornet 4 Drive\n21.4\n110\n3.215\n34.21462\n6.656299\n\n\nHornet Sportabout\n18.7\n175\n3.440\n50.87209\n5.436046\n\n\nValiant\n18.1\n105\n3.460\n30.34682\n5.231214\n\n\n\n\n\n\n\nSummarizing Data\n\n# Calculate average stats by cylinder count\ncyl_stats &lt;- mtcars %&gt;% \n  group_by(cyl) %&gt;%\n  summarize(\n    avg_mpg = mean(mpg),\n    avg_hp = mean(hp),\n    count = n()\n  ) %&gt;%\n  arrange(cyl)\n\ncyl_stats %&gt;%\n  kable()\n\n\n\n\ncyl\navg_mpg\navg_hp\ncount\n\n\n\n\n4\n26.66364\n82.63636\n11\n\n\n6\n19.74286\n122.28571\n7\n\n\n8\n15.10000\n209.21429\n14"
  },
  {
    "objectID": "get-started/data-wrangling-with-dplyr.html#visualizing-the-results",
    "href": "get-started/data-wrangling-with-dplyr.html#visualizing-the-results",
    "title": "Data Wrangling with dplyr",
    "section": "Visualizing the Results",
    "text": "Visualizing the Results\n\n# Plot average mpg by cylinder count\nggplot(cyl_stats, aes(x = factor(cyl), y = avg_mpg)) +\n  geom_col(aes(fill = avg_hp)) +\n  geom_text(aes(label = round(avg_mpg, 1)), vjust = -0.5) +\n  scale_fill_viridis_c() +\n  labs(\n    title = \"Average Fuel Efficiency by Cylinder Count\",\n    subtitle = \"Color indicates average horsepower\",\n    x = \"Number of Cylinders\",\n    y = \"Average MPG\",\n    fill = \"Avg. Horsepower\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "get-started/data-wrangling-with-dplyr.html#working-with-the-iris-dataset",
    "href": "get-started/data-wrangling-with-dplyr.html#working-with-the-iris-dataset",
    "title": "Data Wrangling with dplyr",
    "section": "Working with the iris Dataset",
    "text": "Working with the iris Dataset\nThe following example explores another built-in dataset, iris:\n\n# Look at the iris data\nglimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\n\nFiltering and Grouping\n\n# Calculate average measurements by species\niris_stats &lt;- iris %&gt;%\n  group_by(Species) %&gt;%\n  summarize(\n    avg_sepal_length = mean(Sepal.Length),\n    avg_sepal_width = mean(Sepal.Width),\n    avg_petal_length = mean(Petal.Length),\n    avg_petal_width = mean(Petal.Width),\n    count = n()\n  )\n\niris_stats %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies\navg_sepal_length\navg_sepal_width\navg_petal_length\navg_petal_width\ncount\n\n\n\n\nsetosa\n5.006\n3.428\n1.462\n0.246\n50\n\n\nversicolor\n5.936\n2.770\n4.260\n1.326\n50\n\n\nvirginica\n6.588\n2.974\n5.552\n2.026\n50\n\n\n\n\n\n\n\nVisualizing Iris Data\n\n# Create a scatter plot with multiple dimensions\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Iris Dataset: Sepal Dimensions by Species\",\n    x = \"Sepal Length (cm)\",\n    y = \"Sepal Width (cm)\"\n  ) +\n  theme_minimal() +\n  facet_wrap(~Species)"
  },
  {
    "objectID": "get-started/data-wrangling-with-dplyr.html#conclusion",
    "href": "get-started/data-wrangling-with-dplyr.html#conclusion",
    "title": "Data Wrangling with dplyr",
    "section": "Conclusion",
    "text": "Conclusion\nThe dplyr package provides a consistent and intuitive way to manipulate data in R. These basic functions can easily be used to develop more complex workflows!"
  },
  {
    "objectID": "get-started/ggplot2.html",
    "href": "get-started/ggplot2.html",
    "title": "Introduction to ggplot2 Graphics",
    "section": "",
    "text": "The ggplot2 package, part of the tidyverse, implements the Grammar of Graphics to create elegant and complex plots with consistent syntax. It is one of the most popular visualization packages in R.\n\n\nFirst, install and load the package:\n\n# Install if needed (uncomment to run)\n# install.packages(\"ggplot2\")\n\n# Load the package\nlibrary(ggplot2)\n\n# We'll use the built-in mtcars dataset\ndata(mtcars)\n\n\n\n\nggplot2 is based on the idea that any plot can be built from the same components:\n\nData: The dataset you want to visualize\nAesthetics: Mapping of variables to visual properties\nGeometries: Visual elements representing data points\nFacets: For creating small multiples\nStatistics: Statistical transformations of the data\nCoordinates: The coordinate system\nThemes: Controlling the visual style\n\n\n\n\nEvery ggplot2 plot starts with the ggplot() function and builds with layers:\n\n# Basic scatter plot\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Enhanced scatter plot\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl), size = hp)) +\n  geom_point(alpha = 0.7) +\n  labs(\n    title = \"Car Weight vs. Fuel Efficiency\",\n    subtitle = \"Colored by cylinder count, sized by horsepower\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles Per Gallon\",\n    color = \"Cylinders\",\n    size = \"Horsepower\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n# Create sample time series data\ntime_data &lt;- data.frame(\n  time = 1:20,\n  value = cumsum(rnorm(20))\n)\n\n# Line plot\nggplot(time_data, aes(x = time, y = value)) +\n  geom_line(color = \"steelblue\", size = 1) +\n  geom_point(color = \"steelblue\", size = 2) +\n  labs(title = \"Time Series Plot\", x = \"Time\", y = \"Value\")\n\n\n\n\n\n\n\n\n\n\n\n\n# Count of cars by cylinder\nggplot(mtcars, aes(x = factor(cyl))) +\n  geom_bar(fill = \"steelblue\") +\n  labs(title = \"Count of Cars by Cylinder\", x = \"Cylinders\", y = \"Count\")\n\n\n\n\n\n\n\n# Bar chart with values\ncyl_summary &lt;- as.data.frame(table(mtcars$cyl))\nnames(cyl_summary) &lt;- c(\"cyl\", \"count\")\n\nggplot(cyl_summary, aes(x = cyl, y = count)) +\n  geom_col(fill = \"steelblue\") +\n  geom_text(aes(label = count), vjust = -0.5) +\n  labs(title = \"Count of Cars by Cylinder\", x = \"Cylinders\", y = \"Count\")\n\n\n\n\n\n\n\n\n\n\n\n\n# Histogram\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(bins = 10, fill = \"steelblue\", color = \"white\") +\n  labs(title = \"Distribution of Fuel Efficiency\", x = \"Miles Per Gallon\", y = \"Count\")\n\n\n\n\n\n\n\n# Density plot\nggplot(mtcars, aes(x = mpg)) +\n  geom_density(fill = \"steelblue\", alpha = 0.5) +\n  labs(title = \"Density of Fuel Efficiency\", x = \"Miles Per Gallon\", y = \"Density\")\n\n\n\n\n\n\n\n# Combined histogram and density\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(aes(y = ..density..), bins = 10, fill = \"lightblue\", color = \"white\") +\n  geom_density(color = \"darkblue\", size = 1) +\n  labs(title = \"Distribution of Fuel Efficiency\", x = \"Miles Per Gallon\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\n\n# Box plot\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot(fill = \"lightblue\") +\n  labs(title = \"Fuel Efficiency by Cylinder Count\", x = \"Cylinders\", y = \"Miles Per Gallon\")\n\n\n\n\n\n\n\n# Box plot with points\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot(fill = \"lightblue\", outlier.shape = NA) +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  labs(title = \"Fuel Efficiency by Cylinder Count\", x = \"Cylinders\", y = \"Miles Per Gallon\")\n\n\n\n\n\n\n\n\n\n\n\n\nYou can map variables to various aesthetic properties:\n\n# Multiple aesthetics\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl), shape = factor(am), size = hp)) +\n  geom_point(alpha = 0.7) +\n  labs(\n    title = \"Car Weight vs. Fuel Efficiency\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles Per Gallon\",\n    color = \"Cylinders\",\n    shape = \"Transmission\",\n    size = \"Horsepower\"\n  ) +\n  scale_shape_discrete(labels = c(\"Automatic\", \"Manual\"))\n\n\n\n\n\n\n\n\n\n\n\nFaceting creates separate plots for subsets of data:\n\n# Facet by transmission type\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point() +\n  facet_wrap(~am, labeller = labeller(am = c(\"0\" = \"Automatic\", \"1\" = \"Manual\"))) +\n  labs(title = \"Weight vs. MPG by Transmission Type\", x = \"Weight\", y = \"MPG\", color = \"Cylinders\")\n\n\n\n\n\n\n\n# Facet grid with two variables\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  facet_grid(vs ~ gear, labeller = labeller(\n    vs = c(\"0\" = \"V-Engine\", \"1\" = \"Straight Engine\"),\n    gear = c(\"3\" = \"3 Gears\", \"4\" = \"4 Gears\", \"5\" = \"5 Gears\")\n  )) +\n  labs(title = \"Weight vs. MPG by Engine Type and Gear Count\")\n\n\n\n\n\n\n\n\n\n\n\nggplot2 can add statistical summaries to plots:\n\n# Scatter plot with linear regression line\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(title = \"Weight vs. MPG with Linear Trend\", x = \"Weight\", y = \"MPG\")\n\n\n\n\n\n\n\n# Scatter plot with different smoothing methods by cylinder\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  labs(title = \"Weight vs. MPG by Cylinder\", x = \"Weight\", y = \"MPG\", color = \"Cylinders\")\n\n\n\n\n\n\n\n\n\n\n\nChange how the data is mapped to the plotting area:\n\n# Flip coordinates\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot(fill = \"lightblue\") +\n  coord_flip() +\n  labs(title = \"Fuel Efficiency by Cylinder Count\", x = \"Cylinders\", y = \"Miles Per Gallon\")\n\n\n\n\n\n\n\n# Polar coordinates for a pie chart\nggplot(cyl_summary, aes(x = \"\", y = count, fill = cyl)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(\"y\", start = 0) +\n  labs(title = \"Cars by Cylinder Count\", fill = \"Cylinders\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nThemes control the non-data elements of the plot:\n\n# Default theme\np &lt;- ggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point() +\n  labs(title = \"Weight vs. MPG by Cylinder Count\", x = \"Weight\", y = \"MPG\", color = \"Cylinders\")\n\n# Different built-in themes\np + theme_minimal()\n\n\n\n\n\n\n\np + theme_classic()\n\n\n\n\n\n\n\np + theme_dark()\n\n\n\n\n\n\n\np + theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nYou can customize specific theme elements:\n\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Custom Themed Plot\",\n    subtitle = \"Weight vs. MPG by Cylinder Count\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles Per Gallon\",\n    color = \"Cylinders\"\n  ) +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    plot.subtitle = element_text(size = 12, color = \"gray50\"),\n    axis.title = element_text(size = 12, face = \"bold\"),\n    legend.position = \"top\",\n    legend.background = element_rect(fill = \"lightyellow\", color = \"gray\"),\n    panel.background = element_rect(fill = \"white\"),\n    panel.grid.major = element_line(color = \"gray90\"),\n    panel.grid.minor = element_line(color = \"gray95\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nThe patchwork package makes it easy to combine multiple ggplots:\n\n# Create three different plots\nif (requireNamespace(\"patchwork\", quietly = TRUE)) {\n  library(patchwork)\n  \n  p1 &lt;- ggplot(mtcars, aes(x = wt, y = mpg)) +\n    geom_point() +\n    labs(title = \"Weight vs. MPG\")\n  \n  p2 &lt;- ggplot(mtcars, aes(x = hp, y = mpg)) +\n    geom_point(color = \"red\") +\n    labs(title = \"Horsepower vs. MPG\")\n  \n  p3 &lt;- ggplot(mtcars, aes(x = factor(cyl))) +\n    geom_bar(fill = \"steelblue\") +\n    labs(title = \"Count by Cylinders\")\n  \n  # Combine plots\n  p1 + p2 + p3 + plot_layout(ncol = 2)\n} else {\n  message(\"The patchwork package is not installed. Install with: install.packages('patchwork')\")\n}\n\n\n\n\n\n\n\n\n\n\n\n\n# Create a plot to save\np &lt;- ggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(title = \"Weight vs. MPG\", x = \"Weight\", y = \"MPG\", color = \"Cylinders\") +\n  theme_minimal()\n\n# Example of how to save (not run)\n# ggsave(\"my_ggplot.png\", plot = p, width = 8, height = 6, dpi = 300)\n# ggsave(\"my_ggplot.pdf\", plot = p, width = 8, height = 6)\n\nggplot2 offers a powerful and flexible system for creating visualizations in R. Its consistent syntax and layered approach make it possible to create both simple and complex plots with the same basic structure."
  },
  {
    "objectID": "get-started/ggplot2.html#introduction-to-ggplot2-graphics",
    "href": "get-started/ggplot2.html#introduction-to-ggplot2-graphics",
    "title": "Introduction to ggplot2 Graphics",
    "section": "",
    "text": "The ggplot2 package, part of the tidyverse, implements the Grammar of Graphics to create elegant and complex plots with consistent syntax. It is one of the most popular visualization packages in R.\n\n\nFirst, install and load the package:\n\n# Install if needed (uncomment to run)\n# install.packages(\"ggplot2\")\n\n# Load the package\nlibrary(ggplot2)\n\n# We'll use the built-in mtcars dataset\ndata(mtcars)\n\n\n\n\nggplot2 is based on the idea that any plot can be built from the same components:\n\nData: The dataset you want to visualize\nAesthetics: Mapping of variables to visual properties\nGeometries: Visual elements representing data points\nFacets: For creating small multiples\nStatistics: Statistical transformations of the data\nCoordinates: The coordinate system\nThemes: Controlling the visual style\n\n\n\n\nEvery ggplot2 plot starts with the ggplot() function and builds with layers:\n\n# Basic scatter plot\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Enhanced scatter plot\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl), size = hp)) +\n  geom_point(alpha = 0.7) +\n  labs(\n    title = \"Car Weight vs. Fuel Efficiency\",\n    subtitle = \"Colored by cylinder count, sized by horsepower\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles Per Gallon\",\n    color = \"Cylinders\",\n    size = \"Horsepower\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n# Create sample time series data\ntime_data &lt;- data.frame(\n  time = 1:20,\n  value = cumsum(rnorm(20))\n)\n\n# Line plot\nggplot(time_data, aes(x = time, y = value)) +\n  geom_line(color = \"steelblue\", size = 1) +\n  geom_point(color = \"steelblue\", size = 2) +\n  labs(title = \"Time Series Plot\", x = \"Time\", y = \"Value\")\n\n\n\n\n\n\n\n\n\n\n\n\n# Count of cars by cylinder\nggplot(mtcars, aes(x = factor(cyl))) +\n  geom_bar(fill = \"steelblue\") +\n  labs(title = \"Count of Cars by Cylinder\", x = \"Cylinders\", y = \"Count\")\n\n\n\n\n\n\n\n# Bar chart with values\ncyl_summary &lt;- as.data.frame(table(mtcars$cyl))\nnames(cyl_summary) &lt;- c(\"cyl\", \"count\")\n\nggplot(cyl_summary, aes(x = cyl, y = count)) +\n  geom_col(fill = \"steelblue\") +\n  geom_text(aes(label = count), vjust = -0.5) +\n  labs(title = \"Count of Cars by Cylinder\", x = \"Cylinders\", y = \"Count\")\n\n\n\n\n\n\n\n\n\n\n\n\n# Histogram\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(bins = 10, fill = \"steelblue\", color = \"white\") +\n  labs(title = \"Distribution of Fuel Efficiency\", x = \"Miles Per Gallon\", y = \"Count\")\n\n\n\n\n\n\n\n# Density plot\nggplot(mtcars, aes(x = mpg)) +\n  geom_density(fill = \"steelblue\", alpha = 0.5) +\n  labs(title = \"Density of Fuel Efficiency\", x = \"Miles Per Gallon\", y = \"Density\")\n\n\n\n\n\n\n\n# Combined histogram and density\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(aes(y = ..density..), bins = 10, fill = \"lightblue\", color = \"white\") +\n  geom_density(color = \"darkblue\", size = 1) +\n  labs(title = \"Distribution of Fuel Efficiency\", x = \"Miles Per Gallon\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\n\n# Box plot\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot(fill = \"lightblue\") +\n  labs(title = \"Fuel Efficiency by Cylinder Count\", x = \"Cylinders\", y = \"Miles Per Gallon\")\n\n\n\n\n\n\n\n# Box plot with points\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot(fill = \"lightblue\", outlier.shape = NA) +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  labs(title = \"Fuel Efficiency by Cylinder Count\", x = \"Cylinders\", y = \"Miles Per Gallon\")\n\n\n\n\n\n\n\n\n\n\n\n\nYou can map variables to various aesthetic properties:\n\n# Multiple aesthetics\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl), shape = factor(am), size = hp)) +\n  geom_point(alpha = 0.7) +\n  labs(\n    title = \"Car Weight vs. Fuel Efficiency\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles Per Gallon\",\n    color = \"Cylinders\",\n    shape = \"Transmission\",\n    size = \"Horsepower\"\n  ) +\n  scale_shape_discrete(labels = c(\"Automatic\", \"Manual\"))\n\n\n\n\n\n\n\n\n\n\n\nFaceting creates separate plots for subsets of data:\n\n# Facet by transmission type\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point() +\n  facet_wrap(~am, labeller = labeller(am = c(\"0\" = \"Automatic\", \"1\" = \"Manual\"))) +\n  labs(title = \"Weight vs. MPG by Transmission Type\", x = \"Weight\", y = \"MPG\", color = \"Cylinders\")\n\n\n\n\n\n\n\n# Facet grid with two variables\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  facet_grid(vs ~ gear, labeller = labeller(\n    vs = c(\"0\" = \"V-Engine\", \"1\" = \"Straight Engine\"),\n    gear = c(\"3\" = \"3 Gears\", \"4\" = \"4 Gears\", \"5\" = \"5 Gears\")\n  )) +\n  labs(title = \"Weight vs. MPG by Engine Type and Gear Count\")\n\n\n\n\n\n\n\n\n\n\n\nggplot2 can add statistical summaries to plots:\n\n# Scatter plot with linear regression line\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(title = \"Weight vs. MPG with Linear Trend\", x = \"Weight\", y = \"MPG\")\n\n\n\n\n\n\n\n# Scatter plot with different smoothing methods by cylinder\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  labs(title = \"Weight vs. MPG by Cylinder\", x = \"Weight\", y = \"MPG\", color = \"Cylinders\")\n\n\n\n\n\n\n\n\n\n\n\nChange how the data is mapped to the plotting area:\n\n# Flip coordinates\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot(fill = \"lightblue\") +\n  coord_flip() +\n  labs(title = \"Fuel Efficiency by Cylinder Count\", x = \"Cylinders\", y = \"Miles Per Gallon\")\n\n\n\n\n\n\n\n# Polar coordinates for a pie chart\nggplot(cyl_summary, aes(x = \"\", y = count, fill = cyl)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(\"y\", start = 0) +\n  labs(title = \"Cars by Cylinder Count\", fill = \"Cylinders\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nThemes control the non-data elements of the plot:\n\n# Default theme\np &lt;- ggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point() +\n  labs(title = \"Weight vs. MPG by Cylinder Count\", x = \"Weight\", y = \"MPG\", color = \"Cylinders\")\n\n# Different built-in themes\np + theme_minimal()\n\n\n\n\n\n\n\np + theme_classic()\n\n\n\n\n\n\n\np + theme_dark()\n\n\n\n\n\n\n\np + theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nYou can customize specific theme elements:\n\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Custom Themed Plot\",\n    subtitle = \"Weight vs. MPG by Cylinder Count\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles Per Gallon\",\n    color = \"Cylinders\"\n  ) +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    plot.subtitle = element_text(size = 12, color = \"gray50\"),\n    axis.title = element_text(size = 12, face = \"bold\"),\n    legend.position = \"top\",\n    legend.background = element_rect(fill = \"lightyellow\", color = \"gray\"),\n    panel.background = element_rect(fill = \"white\"),\n    panel.grid.major = element_line(color = \"gray90\"),\n    panel.grid.minor = element_line(color = \"gray95\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nThe patchwork package makes it easy to combine multiple ggplots:\n\n# Create three different plots\nif (requireNamespace(\"patchwork\", quietly = TRUE)) {\n  library(patchwork)\n  \n  p1 &lt;- ggplot(mtcars, aes(x = wt, y = mpg)) +\n    geom_point() +\n    labs(title = \"Weight vs. MPG\")\n  \n  p2 &lt;- ggplot(mtcars, aes(x = hp, y = mpg)) +\n    geom_point(color = \"red\") +\n    labs(title = \"Horsepower vs. MPG\")\n  \n  p3 &lt;- ggplot(mtcars, aes(x = factor(cyl))) +\n    geom_bar(fill = \"steelblue\") +\n    labs(title = \"Count by Cylinders\")\n  \n  # Combine plots\n  p1 + p2 + p3 + plot_layout(ncol = 2)\n} else {\n  message(\"The patchwork package is not installed. Install with: install.packages('patchwork')\")\n}\n\n\n\n\n\n\n\n\n\n\n\n\n# Create a plot to save\np &lt;- ggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(title = \"Weight vs. MPG\", x = \"Weight\", y = \"MPG\", color = \"Cylinders\") +\n  theme_minimal()\n\n# Example of how to save (not run)\n# ggsave(\"my_ggplot.png\", plot = p, width = 8, height = 6, dpi = 300)\n# ggsave(\"my_ggplot.pdf\", plot = p, width = 8, height = 6)\n\nggplot2 offers a powerful and flexible system for creating visualizations in R. Its consistent syntax and layered approach make it possible to create both simple and complex plots with the same basic structure."
  },
  {
    "objectID": "get-started/indexing.html",
    "href": "get-started/indexing.html",
    "title": "Indexing Arrays in R",
    "section": "",
    "text": "R provides flexible ways to access and manipulate elements in data structures like vectors, matrices, and arrays.\n\n\nVectors are one-dimensional arrays and the most basic data structure in R:\n\n# Create a vector\nx &lt;- c(10, 20, 30, 40, 50)\n\n# Access by position (indexing starts at 1, not 0)\nx[1]        # First element\n\n[1] 10\n\nx[3]        # Third element\n\n[1] 30\n\nx[length(x)] # Last element\n\n[1] 50\n\n# Access multiple elements\nx[c(1, 3, 5)]  # First, third, and fifth elements\n\n[1] 10 30 50\n\nx[1:3]         # First three elements\n\n[1] 10 20 30\n\n# Negative indices exclude elements\nx[-2]          # All elements except the second\n\n[1] 10 30 40 50\n\nx[-(3:5)]      # All elements except third through fifth\n\n[1] 10 20\n\n\n\n\n\nLogical vectors can be used to filter elements:\n\n# Create a vector\nages &lt;- c(25, 18, 45, 32, 16, 50)\n\n# Filter using logical conditions\nages[ages &gt; 30]         # Elements greater than 30\n\n[1] 45 32 50\n\nages[ages &gt;= 18 & ages &lt;= 40]  # Elements between 18 and 40\n\n[1] 25 18 32\n\n# Named logical operations\nadults &lt;- ages &gt;= 18\nages[adults]            # Only adult ages\n\n[1] 25 18 45 32 50\n\n\n\n\n\nMatrices are two-dimensional arrays:\n\n# Create a matrix\nmat &lt;- matrix(1:12, nrow = 3, ncol = 4)\nprint(mat)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n# Access by row and column indices\nmat[1, 2]      # Element at first row, second column\n\n[1] 4\n\nmat[2, ]       # Entire second row\n\n[1]  2  5  8 11\n\nmat[, 3]       # Entire third column\n\n[1] 7 8 9\n\nmat[1:2, 3:4]  # Submatrix (rows 1-2, columns 3-4)\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n\n# Logical indexing in matrices\nmat[mat &gt; 6]   # All elements greater than 6\n\n[1]  7  8  9 10 11 12\n\n\n\n\n\nArrays can have more than two dimensions:\n\n# Create a 3D array (2x3x2)\narr &lt;- array(1:12, dim = c(2, 3, 2))\nprint(arr)\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    7    9   11\n[2,]    8   10   12\n\n# Access elements\narr[1, 2, 1]   # Element at position [1,2,1]\n\n[1] 3\n\narr[, , 1]     # First \"layer\" of the array\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\narr[1, , ]     # All elements in first row across all layers\n\n     [,1] [,2]\n[1,]    1    7\n[2,]    3    9\n[3,]    5   11\n\n\n\n\n\nData frames combine features of matrices and lists:\n\n# Create a data frame\ndf &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\"),\n  age = c(25, 30, 35, 40),\n  score = c(88, 92, 79, 94)\n)\nprint(df)\n\n     name age score\n1   Alice  25    88\n2     Bob  30    92\n3 Charlie  35    79\n4   David  40    94\n\n# Access by row and column indices (like matrices)\ndf[1, 2]       # First row, second column\n\n[1] 25\n\ndf[2:3, ]      # Second and third rows\n\n     name age score\n2     Bob  30    92\n3 Charlie  35    79\n\n# Access by column name\ndf$name        # Name column\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\" \"David\"  \n\ndf[, \"age\"]    # Age column\n\n[1] 25 30 35 40\n\ndf[[\"score\"]]  # Score column\n\n[1] 88 92 79 94\n\n# Filter rows by condition\ndf[df$age &gt; 30, ]  # Rows where age is greater than 30\n\n     name age score\n3 Charlie  35    79\n4   David  40    94\n\n\n\n\n\nLists can contain elements of different types:\n\n# Create a list\nmy_list &lt;- list(\n  name = \"John\",\n  numbers = c(1, 2, 3),\n  matrix = matrix(1:4, nrow = 2)\n)\nprint(my_list)\n\n$name\n[1] \"John\"\n\n$numbers\n[1] 1 2 3\n\n$matrix\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n# Access list elements\nmy_list[[1]]       # First element (by position)\n\n[1] \"John\"\n\nmy_list[[\"name\"]]  # Element by name\n\n[1] \"John\"\n\nmy_list$numbers    # Element by name using $ notation\n\n[1] 1 2 3\n\n# Access nested elements\nmy_list$numbers[2]  # Second element of the numbers vector\n\n[1] 2\n\nmy_list$matrix[1,2] # Element at row 1, column 2 of the matrix\n\n[1] 3\n\n\n\n\n\n\n# Using which() for positional indexing from logical conditions\nx &lt;- c(5, 10, 15, 20, 25)\nwhich(x &gt; 15)  # Returns positions where condition is TRUE\n\n[1] 4 5\n\n# Using %in% for membership tests\nfruits &lt;- c(\"apple\", \"banana\", \"cherry\", \"date\")\nfruits %in% c(\"banana\", \"date\", \"fig\")  # Tests which elements are in the second vector\n\n[1] FALSE  TRUE FALSE  TRUE\n\nfruits[fruits %in% c(\"banana\", \"date\", \"fig\")]  # Select matching elements\n\n[1] \"banana\" \"date\"  \n\n\nRemember that R indexing starts at 1, not 0 as in many other programming languages."
  },
  {
    "objectID": "get-started/indexing.html#indexing-arrays-in-r",
    "href": "get-started/indexing.html#indexing-arrays-in-r",
    "title": "Indexing Arrays in R",
    "section": "",
    "text": "R provides flexible ways to access and manipulate elements in data structures like vectors, matrices, and arrays.\n\n\nVectors are one-dimensional arrays and the most basic data structure in R:\n\n# Create a vector\nx &lt;- c(10, 20, 30, 40, 50)\n\n# Access by position (indexing starts at 1, not 0)\nx[1]        # First element\n\n[1] 10\n\nx[3]        # Third element\n\n[1] 30\n\nx[length(x)] # Last element\n\n[1] 50\n\n# Access multiple elements\nx[c(1, 3, 5)]  # First, third, and fifth elements\n\n[1] 10 30 50\n\nx[1:3]         # First three elements\n\n[1] 10 20 30\n\n# Negative indices exclude elements\nx[-2]          # All elements except the second\n\n[1] 10 30 40 50\n\nx[-(3:5)]      # All elements except third through fifth\n\n[1] 10 20\n\n\n\n\n\nLogical vectors can be used to filter elements:\n\n# Create a vector\nages &lt;- c(25, 18, 45, 32, 16, 50)\n\n# Filter using logical conditions\nages[ages &gt; 30]         # Elements greater than 30\n\n[1] 45 32 50\n\nages[ages &gt;= 18 & ages &lt;= 40]  # Elements between 18 and 40\n\n[1] 25 18 32\n\n# Named logical operations\nadults &lt;- ages &gt;= 18\nages[adults]            # Only adult ages\n\n[1] 25 18 45 32 50\n\n\n\n\n\nMatrices are two-dimensional arrays:\n\n# Create a matrix\nmat &lt;- matrix(1:12, nrow = 3, ncol = 4)\nprint(mat)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n# Access by row and column indices\nmat[1, 2]      # Element at first row, second column\n\n[1] 4\n\nmat[2, ]       # Entire second row\n\n[1]  2  5  8 11\n\nmat[, 3]       # Entire third column\n\n[1] 7 8 9\n\nmat[1:2, 3:4]  # Submatrix (rows 1-2, columns 3-4)\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n\n# Logical indexing in matrices\nmat[mat &gt; 6]   # All elements greater than 6\n\n[1]  7  8  9 10 11 12\n\n\n\n\n\nArrays can have more than two dimensions:\n\n# Create a 3D array (2x3x2)\narr &lt;- array(1:12, dim = c(2, 3, 2))\nprint(arr)\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    7    9   11\n[2,]    8   10   12\n\n# Access elements\narr[1, 2, 1]   # Element at position [1,2,1]\n\n[1] 3\n\narr[, , 1]     # First \"layer\" of the array\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\narr[1, , ]     # All elements in first row across all layers\n\n     [,1] [,2]\n[1,]    1    7\n[2,]    3    9\n[3,]    5   11\n\n\n\n\n\nData frames combine features of matrices and lists:\n\n# Create a data frame\ndf &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\"),\n  age = c(25, 30, 35, 40),\n  score = c(88, 92, 79, 94)\n)\nprint(df)\n\n     name age score\n1   Alice  25    88\n2     Bob  30    92\n3 Charlie  35    79\n4   David  40    94\n\n# Access by row and column indices (like matrices)\ndf[1, 2]       # First row, second column\n\n[1] 25\n\ndf[2:3, ]      # Second and third rows\n\n     name age score\n2     Bob  30    92\n3 Charlie  35    79\n\n# Access by column name\ndf$name        # Name column\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\" \"David\"  \n\ndf[, \"age\"]    # Age column\n\n[1] 25 30 35 40\n\ndf[[\"score\"]]  # Score column\n\n[1] 88 92 79 94\n\n# Filter rows by condition\ndf[df$age &gt; 30, ]  # Rows where age is greater than 30\n\n     name age score\n3 Charlie  35    79\n4   David  40    94\n\n\n\n\n\nLists can contain elements of different types:\n\n# Create a list\nmy_list &lt;- list(\n  name = \"John\",\n  numbers = c(1, 2, 3),\n  matrix = matrix(1:4, nrow = 2)\n)\nprint(my_list)\n\n$name\n[1] \"John\"\n\n$numbers\n[1] 1 2 3\n\n$matrix\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n# Access list elements\nmy_list[[1]]       # First element (by position)\n\n[1] \"John\"\n\nmy_list[[\"name\"]]  # Element by name\n\n[1] \"John\"\n\nmy_list$numbers    # Element by name using $ notation\n\n[1] 1 2 3\n\n# Access nested elements\nmy_list$numbers[2]  # Second element of the numbers vector\n\n[1] 2\n\nmy_list$matrix[1,2] # Element at row 1, column 2 of the matrix\n\n[1] 3\n\n\n\n\n\n\n# Using which() for positional indexing from logical conditions\nx &lt;- c(5, 10, 15, 20, 25)\nwhich(x &gt; 15)  # Returns positions where condition is TRUE\n\n[1] 4 5\n\n# Using %in% for membership tests\nfruits &lt;- c(\"apple\", \"banana\", \"cherry\", \"date\")\nfruits %in% c(\"banana\", \"date\", \"fig\")  # Tests which elements are in the second vector\n\n[1] FALSE  TRUE FALSE  TRUE\n\nfruits[fruits %in% c(\"banana\", \"date\", \"fig\")]  # Select matching elements\n\n[1] \"banana\" \"date\"  \n\n\nRemember that R indexing starts at 1, not 0 as in many other programming languages."
  },
  {
    "objectID": "get-started/loops.html",
    "href": "get-started/loops.html",
    "title": "Loops and Apply Functions in R",
    "section": "",
    "text": "R offers several methods to perform repetitive tasks through loops and the more efficient apply family of functions.\n\n\nThe for loop iterates over elements in a sequence:\n\n# Basic for loop\nfor (i in 1:5) {\n  print(paste(\"Iteration:\", i))\n}\n\n[1] \"Iteration: 1\"\n[1] \"Iteration: 2\"\n[1] \"Iteration: 3\"\n[1] \"Iteration: 4\"\n[1] \"Iteration: 5\"\n\n# Looping through a vector\nfruits &lt;- c(\"apple\", \"banana\", \"cherry\")\nfor (fruit in fruits) {\n  print(paste(\"I like\", fruit))\n}\n\n[1] \"I like apple\"\n[1] \"I like banana\"\n[1] \"I like cherry\"\n\n\n\n\n\nThe while loop continues until a condition becomes false:\n\n# Basic while loop\ncounter &lt;- 1\nwhile (counter &lt;= 5) {\n  print(paste(\"Count:\", counter))\n  counter &lt;- counter + 1\n}\n\n[1] \"Count: 1\"\n[1] \"Count: 2\"\n[1] \"Count: 3\"\n[1] \"Count: 4\"\n[1] \"Count: 5\"\n\n\n\n\n\nThe repeat loop runs indefinitely until a break statement:\n\n# Repeat loop with break\ncounter &lt;- 1\nrepeat {\n  print(paste(\"Count:\", counter))\n  counter &lt;- counter + 1\n  if (counter &gt; 5) {\n    break\n  }\n}\n\n[1] \"Count: 1\"\n[1] \"Count: 2\"\n[1] \"Count: 3\"\n[1] \"Count: 4\"\n[1] \"Count: 5\"\n\n\n\n\n\nUse break to exit a loop and next to skip to the next iteration:\n\n# Using next to skip iterations\nfor (i in 1:10) {\n  if (i %% 2 == 0) {  # Skip even numbers\n    next\n  }\n  print(paste(\"Odd number:\", i))\n}\n\n[1] \"Odd number: 1\"\n[1] \"Odd number: 3\"\n[1] \"Odd number: 5\"\n[1] \"Odd number: 7\"\n[1] \"Odd number: 9\""
  },
  {
    "objectID": "get-started/loops.html#loops-and-apply-functions-in-r",
    "href": "get-started/loops.html#loops-and-apply-functions-in-r",
    "title": "Loops and Apply Functions in R",
    "section": "",
    "text": "R offers several methods to perform repetitive tasks through loops and the more efficient apply family of functions.\n\n\nThe for loop iterates over elements in a sequence:\n\n# Basic for loop\nfor (i in 1:5) {\n  print(paste(\"Iteration:\", i))\n}\n\n[1] \"Iteration: 1\"\n[1] \"Iteration: 2\"\n[1] \"Iteration: 3\"\n[1] \"Iteration: 4\"\n[1] \"Iteration: 5\"\n\n# Looping through a vector\nfruits &lt;- c(\"apple\", \"banana\", \"cherry\")\nfor (fruit in fruits) {\n  print(paste(\"I like\", fruit))\n}\n\n[1] \"I like apple\"\n[1] \"I like banana\"\n[1] \"I like cherry\"\n\n\n\n\n\nThe while loop continues until a condition becomes false:\n\n# Basic while loop\ncounter &lt;- 1\nwhile (counter &lt;= 5) {\n  print(paste(\"Count:\", counter))\n  counter &lt;- counter + 1\n}\n\n[1] \"Count: 1\"\n[1] \"Count: 2\"\n[1] \"Count: 3\"\n[1] \"Count: 4\"\n[1] \"Count: 5\"\n\n\n\n\n\nThe repeat loop runs indefinitely until a break statement:\n\n# Repeat loop with break\ncounter &lt;- 1\nrepeat {\n  print(paste(\"Count:\", counter))\n  counter &lt;- counter + 1\n  if (counter &gt; 5) {\n    break\n  }\n}\n\n[1] \"Count: 1\"\n[1] \"Count: 2\"\n[1] \"Count: 3\"\n[1] \"Count: 4\"\n[1] \"Count: 5\"\n\n\n\n\n\nUse break to exit a loop and next to skip to the next iteration:\n\n# Using next to skip iterations\nfor (i in 1:10) {\n  if (i %% 2 == 0) {  # Skip even numbers\n    next\n  }\n  print(paste(\"Odd number:\", i))\n}\n\n[1] \"Odd number: 1\"\n[1] \"Odd number: 3\"\n[1] \"Odd number: 5\"\n[1] \"Odd number: 7\"\n[1] \"Odd number: 9\""
  },
  {
    "objectID": "get-started/loops.html#apply-functions",
    "href": "get-started/loops.html#apply-functions",
    "title": "Loops and Apply Functions in R",
    "section": "Apply Functions",
    "text": "Apply Functions\nThe apply family of functions offers a more efficient and concise way to perform iterations in R.\n\napply() - For matrices and arrays\n\n# Create a matrix\nmat &lt;- matrix(1:9, nrow = 3)\nprint(mat)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n# Apply sum function to each row\napply(mat, 1, sum)  # MARGIN=1 for rows\n\n[1] 12 15 18\n\n# Apply mean function to each column\napply(mat, 2, mean)  # MARGIN=2 for columns\n\n[1] 2 5 8\n\n\n\n\nlapply() - For lists, returns a list\n\n# List of vectors\nmy_list &lt;- list(a = 1:3, b = 4:6, c = 7:9)\n\n# Apply function to each element\nlapply(my_list, sum)\n\n$a\n[1] 6\n\n$b\n[1] 15\n\n$c\n[1] 24\n\n\n\n\nsapply() - Simplified apply, returns vector or matrix\n\n# Same as above but with simplified output\nsapply(my_list, sum)\n\n a  b  c \n 6 15 24 \n\n# Using with a custom function\nsapply(my_list, function(x) x * 2)\n\n     a  b  c\n[1,] 2  8 14\n[2,] 4 10 16\n[3,] 6 12 18\n\n\n\n\nvapply() - Like sapply but with pre-specified output type\n\n# Specify the output type for safety\nvapply(my_list, sum, FUN.VALUE = numeric(1))\n\n a  b  c \n 6 15 24 \n\n\n\n\ntapply() - Apply function to subsets of a vector\n\n# Vector and grouping factor\nvalues &lt;- c(1, 2, 3, 4, 5, 6, 7, 8)\ngroups &lt;- c(\"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\")\n\n# Calculate mean by group\ntapply(values, groups, mean)\n\nA B \n4 5 \n\n\n\n\nmapply() - Multivariate version of sapply\n\n# Apply function to multiple lists in parallel\nmapply(sum, list(1:3), list(4:6), list(7:9))\n\n[1] 45\n\n# Create strings with multiple inputs\nmapply(paste, \"X\", 1:5, \"Y\", SIMPLIFY = TRUE)\n\n      X    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt; \n\"X 1 Y\" \"X 2 Y\" \"X 3 Y\" \"X 4 Y\" \"X 5 Y\" \n\n\nThe apply family of functions is generally preferred over loops in R because they are: 1. More concise and readable 2. Often faster for large datasets 3. Aligned with R’s vectorized approach to data processing"
  },
  {
    "objectID": "get-started/data-table.html",
    "href": "get-started/data-table.html",
    "title": "Introduction to data.table",
    "section": "",
    "text": "The data.table package is a high-performance extension of R’s data.frame that provides concise syntax for data manipulation. It is particularly efficient for large datasets.\n\nGetting Started with data.table\nFirst, install and load the package:\n\n# Install if needed (uncomment to run)\n# install.packages(\"data.table\")\n\n# Load the package\nlibrary(data.table)\n\n# Convert the built-in mtcars dataset to a data.table\ndt_cars &lt;- as.data.table(mtcars, keep.rownames = TRUE)\nsetnames(dt_cars, \"rn\", \"model\")  # Rename the rownames column\nhead(dt_cars)\n\n               model   mpg   cyl  disp    hp  drat    wt  qsec    vs    am\n              &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:         Mazda RX4  21.0     6   160   110  3.90 2.620 16.46     0     1\n2:     Mazda RX4 Wag  21.0     6   160   110  3.90 2.875 17.02     0     1\n3:        Datsun 710  22.8     4   108    93  3.85 2.320 18.61     1     1\n4:    Hornet 4 Drive  21.4     6   258   110  3.08 3.215 19.44     1     0\n5: Hornet Sportabout  18.7     8   360   175  3.15 3.440 17.02     0     0\n6:           Valiant  18.1     6   225   105  2.76 3.460 20.22     1     0\n    gear  carb\n   &lt;num&gt; &lt;num&gt;\n1:     4     4\n2:     4     4\n3:     4     1\n4:     3     1\n5:     3     2\n6:     3     1\n\n\n\n\nBasic Syntax: [i, j, by]\ndata.table uses a concise syntax based on [i, j, by]: - i: Subset rows (WHERE) - j: Compute on columns (SELECT) - by: Group by columns (GROUP BY)\n\nSubsetting Rows (i)\n\n# Select cars with 6 cylinders\ndt_cars[cyl == 6]\n\n            model   mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear\n           &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:      Mazda RX4  21.0     6 160.0   110  3.90 2.620 16.46     0     1     4\n2:  Mazda RX4 Wag  21.0     6 160.0   110  3.90 2.875 17.02     0     1     4\n3: Hornet 4 Drive  21.4     6 258.0   110  3.08 3.215 19.44     1     0     3\n4:        Valiant  18.1     6 225.0   105  2.76 3.460 20.22     1     0     3\n5:       Merc 280  19.2     6 167.6   123  3.92 3.440 18.30     1     0     4\n6:      Merc 280C  17.8     6 167.6   123  3.92 3.440 18.90     1     0     4\n7:   Ferrari Dino  19.7     6 145.0   175  3.62 2.770 15.50     0     1     5\n    carb\n   &lt;num&gt;\n1:     4\n2:     4\n3:     1\n4:     1\n5:     4\n6:     4\n7:     6\n\n# Multiple conditions\ndt_cars[cyl == 6 & mpg &gt; 20]\n\n            model   mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear\n           &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:      Mazda RX4  21.0     6   160   110  3.90 2.620 16.46     0     1     4\n2:  Mazda RX4 Wag  21.0     6   160   110  3.90 2.875 17.02     0     1     4\n3: Hornet 4 Drive  21.4     6   258   110  3.08 3.215 19.44     1     0     3\n    carb\n   &lt;num&gt;\n1:     4\n2:     4\n3:     1\n\n# Select specific rows by position\ndt_cars[1:5]\n\n               model   mpg   cyl  disp    hp  drat    wt  qsec    vs    am\n              &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:         Mazda RX4  21.0     6   160   110  3.90 2.620 16.46     0     1\n2:     Mazda RX4 Wag  21.0     6   160   110  3.90 2.875 17.02     0     1\n3:        Datsun 710  22.8     4   108    93  3.85 2.320 18.61     1     1\n4:    Hornet 4 Drive  21.4     6   258   110  3.08 3.215 19.44     1     0\n5: Hornet Sportabout  18.7     8   360   175  3.15 3.440 17.02     0     0\n    gear  carb\n   &lt;num&gt; &lt;num&gt;\n1:     4     4\n2:     4     4\n3:     4     1\n4:     3     1\n5:     3     2\n\n\n\n\nSelecting and Computing on Columns (j)\n\n# Select specific columns\ndt_cars[1:10, .(mpg, hp, cyl)]\n\n      mpg    hp   cyl\n    &lt;num&gt; &lt;num&gt; &lt;num&gt;\n 1:  21.0   110     6\n 2:  21.0   110     6\n 3:  22.8    93     4\n 4:  21.4   110     6\n 5:  18.7   175     8\n 6:  18.1   105     6\n 7:  14.3   245     8\n 8:  24.4    62     4\n 9:  22.8    95     4\n10:  19.2   123     6\n\n# Compute new values\ndt_cars[1:10, .(kpl = mpg * 0.425)]\n\n        kpl\n      &lt;num&gt;\n 1:  8.9250\n 2:  8.9250\n 3:  9.6900\n 4:  9.0950\n 5:  7.9475\n 6:  7.6925\n 7:  6.0775\n 8: 10.3700\n 9:  9.6900\n10:  8.1600\n\n# Select and compute multiple columns\ndt_cars[1:10, .(model, kpl = mpg * 0.425, hp_per_cyl = hp/cyl)]\n\n                model     kpl hp_per_cyl\n               &lt;char&gt;   &lt;num&gt;      &lt;num&gt;\n 1:         Mazda RX4  8.9250   18.33333\n 2:     Mazda RX4 Wag  8.9250   18.33333\n 3:        Datsun 710  9.6900   23.25000\n 4:    Hornet 4 Drive  9.0950   18.33333\n 5: Hornet Sportabout  7.9475   21.87500\n 6:           Valiant  7.6925   17.50000\n 7:        Duster 360  6.0775   30.62500\n 8:         Merc 240D 10.3700   15.50000\n 9:          Merc 230  9.6900   23.75000\n10:          Merc 280  8.1600   20.50000\n\n# Apply functions\ndt_cars[1:10, .(avg_mpg = mean(mpg), max_hp = max(hp))]\n\n   avg_mpg max_hp\n     &lt;num&gt;  &lt;num&gt;\n1:   20.37    245\n\n\n\n\nGrouping (by)\n\n# Group by cylinder and calculate statistics\ndt_cars[, .(count = .N, avg_mpg = mean(mpg)), by = cyl]\n\n     cyl count  avg_mpg\n   &lt;num&gt; &lt;int&gt;    &lt;num&gt;\n1:     6     7 19.74286\n2:     4    11 26.66364\n3:     8    14 15.10000\n\n# Multiple grouping variables\ndt_cars[, .(count = .N, avg_mpg = mean(mpg)), by = .(cyl, gear)]\n\n     cyl  gear count avg_mpg\n   &lt;num&gt; &lt;num&gt; &lt;int&gt;   &lt;num&gt;\n1:     6     4     4  19.750\n2:     4     4     8  26.925\n3:     6     3     2  19.750\n4:     8     3    12  15.050\n5:     4     3     1  21.500\n6:     4     5     2  28.200\n7:     8     5     2  15.400\n8:     6     5     1  19.700\n\n# Grouping with expressions\ndt_cars[, .(count = .N), by = .(cyl, high_mpg = mpg &gt; 20)]\n\n     cyl high_mpg count\n   &lt;num&gt;   &lt;lgcl&gt; &lt;int&gt;\n1:     6     TRUE     3\n2:     4     TRUE    11\n3:     8    FALSE    14\n4:     6    FALSE     4\n\n\n\n\n\nSpecial Symbols in data.table\ndata.table provides special symbols for common operations:\n\n# .N: number of rows\ndt_cars[, .N]\n\n[1] 32\n\ndt_cars[, .N, by = cyl]\n\n     cyl     N\n   &lt;num&gt; &lt;int&gt;\n1:     6     7\n2:     4    11\n3:     8    14\n\n# .SD: Subset of Data\ndt_cars[, lapply(.SD, mean), by = cyl, .SDcols = c(\"mpg\", \"hp\", \"wt\")]\n\n     cyl      mpg        hp       wt\n   &lt;num&gt;    &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1:     6 19.74286 122.28571 3.117143\n2:     4 26.66364  82.63636 2.285727\n3:     8 15.10000 209.21429 3.999214\n\n# .I: Row numbers\ndt_cars[, .I[1:2], by = cyl]  # First two row numbers for each cyl group\n\n     cyl    V1\n   &lt;num&gt; &lt;int&gt;\n1:     6     1\n2:     6     2\n3:     4     3\n4:     4     8\n5:     8     5\n6:     8     7\n\n\n\n\nModifying Data\ndata.table allows efficient in-place modifications:\n\n# Create a copy to avoid modifying the original\ndt_copy &lt;- copy(dt_cars)\n\n# Add a new column\ndt_copy[, efficiency := mpg/wt]\nhead(dt_copy)\n\n               model   mpg   cyl  disp    hp  drat    wt  qsec    vs    am\n              &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:         Mazda RX4  21.0     6   160   110  3.90 2.620 16.46     0     1\n2:     Mazda RX4 Wag  21.0     6   160   110  3.90 2.875 17.02     0     1\n3:        Datsun 710  22.8     4   108    93  3.85 2.320 18.61     1     1\n4:    Hornet 4 Drive  21.4     6   258   110  3.08 3.215 19.44     1     0\n5: Hornet Sportabout  18.7     8   360   175  3.15 3.440 17.02     0     0\n6:           Valiant  18.1     6   225   105  2.76 3.460 20.22     1     0\n    gear  carb efficiency\n   &lt;num&gt; &lt;num&gt;      &lt;num&gt;\n1:     4     4   8.015267\n2:     4     4   7.304348\n3:     4     1   9.827586\n4:     3     1   6.656299\n5:     3     2   5.436047\n6:     3     1   5.231214\n\n# Update existing values\ndt_copy[cyl == 4, mpg := mpg * 1.1]  # Increase mpg by 10% for 4-cylinder cars\nhead(dt_copy)\n\n               model   mpg   cyl  disp    hp  drat    wt  qsec    vs    am\n              &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:         Mazda RX4 21.00     6   160   110  3.90 2.620 16.46     0     1\n2:     Mazda RX4 Wag 21.00     6   160   110  3.90 2.875 17.02     0     1\n3:        Datsun 710 25.08     4   108    93  3.85 2.320 18.61     1     1\n4:    Hornet 4 Drive 21.40     6   258   110  3.08 3.215 19.44     1     0\n5: Hornet Sportabout 18.70     8   360   175  3.15 3.440 17.02     0     0\n6:           Valiant 18.10     6   225   105  2.76 3.460 20.22     1     0\n    gear  carb efficiency\n   &lt;num&gt; &lt;num&gt;      &lt;num&gt;\n1:     4     4   8.015267\n2:     4     4   7.304348\n3:     4     1   9.827586\n4:     3     1   6.656299\n5:     3     2   5.436047\n6:     3     1   5.231214\n\n# Delete columns\ndt_copy[, c(\"carb\", \"vs\") := NULL]\nhead(dt_copy)\n\n               model   mpg   cyl  disp    hp  drat    wt  qsec    am  gear\n              &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:         Mazda RX4 21.00     6   160   110  3.90 2.620 16.46     1     4\n2:     Mazda RX4 Wag 21.00     6   160   110  3.90 2.875 17.02     1     4\n3:        Datsun 710 25.08     4   108    93  3.85 2.320 18.61     1     4\n4:    Hornet 4 Drive 21.40     6   258   110  3.08 3.215 19.44     0     3\n5: Hornet Sportabout 18.70     8   360   175  3.15 3.440 17.02     0     3\n6:           Valiant 18.10     6   225   105  2.76 3.460 20.22     0     3\n   efficiency\n        &lt;num&gt;\n1:   8.015267\n2:   7.304348\n3:   9.827586\n4:   6.656299\n5:   5.436047\n6:   5.231214\n\n\n\n\nKeys and Indexing\nSetting keys enables fast subsetting and joins:\n\n# Set a key\nsetkey(dt_copy, cyl)\ndt_copy\n\nKey: &lt;cyl&gt;\n                  model   mpg   cyl  disp    hp  drat    wt  qsec    am  gear\n                 &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n 1:          Datsun 710 25.08     4 108.0    93  3.85 2.320 18.61     1     4\n 2:           Merc 240D 26.84     4 146.7    62  3.69 3.190 20.00     0     4\n 3:            Merc 230 25.08     4 140.8    95  3.92 3.150 22.90     0     4\n 4:            Fiat 128 35.64     4  78.7    66  4.08 2.200 19.47     1     4\n 5:         Honda Civic 33.44     4  75.7    52  4.93 1.615 18.52     1     4\n 6:      Toyota Corolla 37.29     4  71.1    65  4.22 1.835 19.90     1     4\n 7:       Toyota Corona 23.65     4 120.1    97  3.70 2.465 20.01     0     3\n 8:           Fiat X1-9 30.03     4  79.0    66  4.08 1.935 18.90     1     4\n 9:       Porsche 914-2 28.60     4 120.3    91  4.43 2.140 16.70     1     5\n10:        Lotus Europa 33.44     4  95.1   113  3.77 1.513 16.90     1     5\n11:          Volvo 142E 23.54     4 121.0   109  4.11 2.780 18.60     1     4\n12:           Mazda RX4 21.00     6 160.0   110  3.90 2.620 16.46     1     4\n13:       Mazda RX4 Wag 21.00     6 160.0   110  3.90 2.875 17.02     1     4\n14:      Hornet 4 Drive 21.40     6 258.0   110  3.08 3.215 19.44     0     3\n15:             Valiant 18.10     6 225.0   105  2.76 3.460 20.22     0     3\n16:            Merc 280 19.20     6 167.6   123  3.92 3.440 18.30     0     4\n17:           Merc 280C 17.80     6 167.6   123  3.92 3.440 18.90     0     4\n18:        Ferrari Dino 19.70     6 145.0   175  3.62 2.770 15.50     1     5\n19:   Hornet Sportabout 18.70     8 360.0   175  3.15 3.440 17.02     0     3\n20:          Duster 360 14.30     8 360.0   245  3.21 3.570 15.84     0     3\n21:          Merc 450SE 16.40     8 275.8   180  3.07 4.070 17.40     0     3\n22:          Merc 450SL 17.30     8 275.8   180  3.07 3.730 17.60     0     3\n23:         Merc 450SLC 15.20     8 275.8   180  3.07 3.780 18.00     0     3\n24:  Cadillac Fleetwood 10.40     8 472.0   205  2.93 5.250 17.98     0     3\n25: Lincoln Continental 10.40     8 460.0   215  3.00 5.424 17.82     0     3\n26:   Chrysler Imperial 14.70     8 440.0   230  3.23 5.345 17.42     0     3\n27:    Dodge Challenger 15.50     8 318.0   150  2.76 3.520 16.87     0     3\n28:         AMC Javelin 15.20     8 304.0   150  3.15 3.435 17.30     0     3\n29:          Camaro Z28 13.30     8 350.0   245  3.73 3.840 15.41     0     3\n30:    Pontiac Firebird 19.20     8 400.0   175  3.08 3.845 17.05     0     3\n31:      Ford Pantera L 15.80     8 351.0   264  4.22 3.170 14.50     1     5\n32:       Maserati Bora 15.00     8 301.0   335  3.54 3.570 14.60     1     5\n                  model   mpg   cyl  disp    hp  drat    wt  qsec    am  gear\n    efficiency\n         &lt;num&gt;\n 1:   9.827586\n 2:   7.648903\n 3:   7.238095\n 4:  14.727273\n 5:  18.823529\n 6:  18.474114\n 7:   8.722110\n 8:  14.108527\n 9:  12.149533\n10:  20.092531\n11:   7.697842\n12:   8.015267\n13:   7.304348\n14:   6.656299\n15:   5.231214\n16:   5.581395\n17:   5.174419\n18:   7.111913\n19:   5.436047\n20:   4.005602\n21:   4.029484\n22:   4.638070\n23:   4.021164\n24:   1.980952\n25:   1.917404\n26:   2.750234\n27:   4.403409\n28:   4.425036\n29:   3.463542\n30:   4.993498\n31:   4.984227\n32:   4.201681\n    efficiency\n\n# Fast subsetting using key\ndt_copy[.(6)]  # All rows where cyl == 6\n\nKey: &lt;cyl&gt;\n            model   mpg   cyl  disp    hp  drat    wt  qsec    am  gear\n           &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:      Mazda RX4  21.0     6 160.0   110  3.90 2.620 16.46     1     4\n2:  Mazda RX4 Wag  21.0     6 160.0   110  3.90 2.875 17.02     1     4\n3: Hornet 4 Drive  21.4     6 258.0   110  3.08 3.215 19.44     0     3\n4:        Valiant  18.1     6 225.0   105  2.76 3.460 20.22     0     3\n5:       Merc 280  19.2     6 167.6   123  3.92 3.440 18.30     0     4\n6:      Merc 280C  17.8     6 167.6   123  3.92 3.440 18.90     0     4\n7:   Ferrari Dino  19.7     6 145.0   175  3.62 2.770 15.50     1     5\n   efficiency\n        &lt;num&gt;\n1:   8.015267\n2:   7.304348\n3:   6.656299\n4:   5.231214\n5:   5.581395\n6:   5.174419\n7:   7.111913\n\n# Multiple keys\nsetkey(dt_copy, cyl, gear)\ndt_copy[.(6, 4)]  # All rows where cyl == 6 and gear == 4\n\nKey: &lt;cyl, gear&gt;\n           model   mpg   cyl  disp    hp  drat    wt  qsec    am  gear\n          &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:     Mazda RX4  21.0     6 160.0   110  3.90 2.620 16.46     1     4\n2: Mazda RX4 Wag  21.0     6 160.0   110  3.90 2.875 17.02     1     4\n3:      Merc 280  19.2     6 167.6   123  3.92 3.440 18.30     0     4\n4:     Merc 280C  17.8     6 167.6   123  3.92 3.440 18.90     0     4\n   efficiency\n        &lt;num&gt;\n1:   8.015267\n2:   7.304348\n3:   5.581395\n4:   5.174419\n\n\n\n\nJoins in data.table\ndata.table provides efficient joins using keys:\n\n# Create sample data.tables\nmanufacturers &lt;- data.table(\n  make = c(\"Honda\", \"Toyota\", \"Ford\", \"BMW\", \"Mercedes\"),\n  country = c(\"Japan\", \"Japan\", \"USA\", \"Germany\", \"Germany\")\n)\n\ncars &lt;- data.table(\n  model = c(\"Civic\", \"Corolla\", \"Focus\", \"3 Series\", \"Fiesta\"),\n  make = c(\"Honda\", \"Toyota\", \"Ford\", \"BMW\", \"Ford\")\n)\n\n# Set keys for joining\nsetkey(manufacturers, make)\nsetkey(cars, make)\n\n# Inner join\ncars[manufacturers]\n\nKey: &lt;make&gt;\n      model     make country\n     &lt;char&gt;   &lt;char&gt;  &lt;char&gt;\n1: 3 Series      BMW Germany\n2:    Focus     Ford     USA\n3:   Fiesta     Ford     USA\n4:    Civic    Honda   Japan\n5:     &lt;NA&gt; Mercedes Germany\n6:  Corolla   Toyota   Japan\n\n# Left join\nmanufacturers[cars, nomatch=NA]\n\nKey: &lt;make&gt;\n     make country    model\n   &lt;char&gt;  &lt;char&gt;   &lt;char&gt;\n1:    BMW Germany 3 Series\n2:   Ford     USA    Focus\n3:   Ford     USA   Fiesta\n4:  Honda   Japan    Civic\n5: Toyota   Japan  Corolla\n\n# Non-equi joins\ndt_cars[dt_cars[, .(max_mpg = max(mpg)), by = cyl], on = .(mpg = max_mpg, cyl)]\n\n              model   mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear\n             &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:   Hornet 4 Drive  21.4     6 258.0   110  3.08 3.215 19.44     1     0     3\n2:   Toyota Corolla  33.9     4  71.1    65  4.22 1.835 19.90     1     1     4\n3: Pontiac Firebird  19.2     8 400.0   175  3.08 3.845 17.05     0     0     3\n    carb\n   &lt;num&gt;\n1:     1\n2:     1\n3:     2\n\n\n\n\nReshaping data\ndata.table provides functions for reshaping data:\n\n# Create a sample data.table\ndt &lt;- data.table(\n  id = rep(1:3, each = 2),\n  variable = rep(c(\"height\", \"weight\"), 3),\n  value = c(170, 68, 155, 52, 182, 75)\n)\ndt\n\n      id variable value\n   &lt;int&gt;   &lt;char&gt; &lt;num&gt;\n1:     1   height   170\n2:     1   weight    68\n3:     2   height   155\n4:     2   weight    52\n5:     3   height   182\n6:     3   weight    75\n\n# Wide to long\ndt_wide &lt;- dcast(dt, id ~ variable, value.var = \"value\")\ndt_wide\n\nKey: &lt;id&gt;\n      id height weight\n   &lt;int&gt;  &lt;num&gt;  &lt;num&gt;\n1:     1    170     68\n2:     2    155     52\n3:     3    182     75\n\n# Long to wide\ndt_long &lt;- melt(dt_wide, id.vars = \"id\", variable.name = \"measure\", value.name = \"value\")\ndt_long\n\n      id measure value\n   &lt;int&gt;  &lt;fctr&gt; &lt;num&gt;\n1:     1  height   170\n2:     2  height   155\n3:     3  height   182\n4:     1  weight    68\n5:     2  weight    52\n6:     3  weight    75\n\n\n\n\nPerformance Benefits\ndata.table is designed for performance:\n\n# Create a larger dataset for demonstration\nset.seed(123)\nn &lt;- 1e6\ndt_large &lt;- data.table(\n  id = 1:n,\n  x = sample(1:100, n, replace = TRUE),\n  y = sample(letters[1:5], n, replace = TRUE)\n)\n\n# Measure time for a grouped operation\nsystem.time(dt_large[, .(mean_x = mean(x)), by = y])\n\n   user  system elapsed \n   0.07    0.04    0.03 \n\n# Compare with equivalent dplyr operation (if dplyr is installed)\ndf_large &lt;- as.data.frame(dt_large)\nsystem.time(dplyr::summarise(dplyr::group_by(df_large, y), mean_x = mean(x)))\n\n   user  system elapsed \n   0.34    0.03    0.40 \n\n\ndata.table is particularly valuable when working with large datasets due to its efficient memory usage and optimized C implementation."
  },
  {
    "objectID": "get-started/data-table.html#introduction-to-data.table",
    "href": "get-started/data-table.html#introduction-to-data.table",
    "title": "Introduction to data.table",
    "section": "",
    "text": "The data.table package is a high-performance extension of R’s data.frame that provides concise syntax for data manipulation. It is particularly efficient for large datasets.\n\n\nFirst, install and load the package:\n\n# Install if needed (uncomment to run)\n# install.packages(\"data.table\")\n\n# Load the package\nlibrary(data.table)\n\n# Convert the built-in mtcars dataset to a data.table\ndt_cars &lt;- as.data.table(mtcars, keep.rownames = TRUE)\nsetnames(dt_cars, \"rn\", \"model\")  # Rename the rownames column\nhead(dt_cars)\n\n               model   mpg   cyl  disp    hp  drat    wt  qsec    vs    am\n              &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:         Mazda RX4  21.0     6   160   110  3.90 2.620 16.46     0     1\n2:     Mazda RX4 Wag  21.0     6   160   110  3.90 2.875 17.02     0     1\n3:        Datsun 710  22.8     4   108    93  3.85 2.320 18.61     1     1\n4:    Hornet 4 Drive  21.4     6   258   110  3.08 3.215 19.44     1     0\n5: Hornet Sportabout  18.7     8   360   175  3.15 3.440 17.02     0     0\n6:           Valiant  18.1     6   225   105  2.76 3.460 20.22     1     0\n    gear  carb\n   &lt;num&gt; &lt;num&gt;\n1:     4     4\n2:     4     4\n3:     4     1\n4:     3     1\n5:     3     2\n6:     3     1\n\n\n\n\n\ndata.table uses a concise syntax based on [i, j, by]: - i: Subset rows (WHERE) - j: Compute on columns (SELECT) - by: Group by columns (GROUP BY)\n\n\n\n# Select cars with 6 cylinders\ndt_cars[cyl == 6]\n\n            model   mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear\n           &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:      Mazda RX4  21.0     6 160.0   110  3.90 2.620 16.46     0     1     4\n2:  Mazda RX4 Wag  21.0     6 160.0   110  3.90 2.875 17.02     0     1     4\n3: Hornet 4 Drive  21.4     6 258.0   110  3.08 3.215 19.44     1     0     3\n4:        Valiant  18.1     6 225.0   105  2.76 3.460 20.22     1     0     3\n5:       Merc 280  19.2     6 167.6   123  3.92 3.440 18.30     1     0     4\n6:      Merc 280C  17.8     6 167.6   123  3.92 3.440 18.90     1     0     4\n7:   Ferrari Dino  19.7     6 145.0   175  3.62 2.770 15.50     0     1     5\n    carb\n   &lt;num&gt;\n1:     4\n2:     4\n3:     1\n4:     1\n5:     4\n6:     4\n7:     6\n\n# Multiple conditions\ndt_cars[cyl == 6 & mpg &gt; 20]\n\n            model   mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear\n           &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:      Mazda RX4  21.0     6   160   110  3.90 2.620 16.46     0     1     4\n2:  Mazda RX4 Wag  21.0     6   160   110  3.90 2.875 17.02     0     1     4\n3: Hornet 4 Drive  21.4     6   258   110  3.08 3.215 19.44     1     0     3\n    carb\n   &lt;num&gt;\n1:     4\n2:     4\n3:     1\n\n# Select specific rows by position\ndt_cars[1:5]\n\n               model   mpg   cyl  disp    hp  drat    wt  qsec    vs    am\n              &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:         Mazda RX4  21.0     6   160   110  3.90 2.620 16.46     0     1\n2:     Mazda RX4 Wag  21.0     6   160   110  3.90 2.875 17.02     0     1\n3:        Datsun 710  22.8     4   108    93  3.85 2.320 18.61     1     1\n4:    Hornet 4 Drive  21.4     6   258   110  3.08 3.215 19.44     1     0\n5: Hornet Sportabout  18.7     8   360   175  3.15 3.440 17.02     0     0\n    gear  carb\n   &lt;num&gt; &lt;num&gt;\n1:     4     4\n2:     4     4\n3:     4     1\n4:     3     1\n5:     3     2\n\n\n\n\n\n\n# Select specific columns\ndt_cars[1:10, .(mpg, hp, cyl)]\n\n      mpg    hp   cyl\n    &lt;num&gt; &lt;num&gt; &lt;num&gt;\n 1:  21.0   110     6\n 2:  21.0   110     6\n 3:  22.8    93     4\n 4:  21.4   110     6\n 5:  18.7   175     8\n 6:  18.1   105     6\n 7:  14.3   245     8\n 8:  24.4    62     4\n 9:  22.8    95     4\n10:  19.2   123     6\n\n# Compute new values\ndt_cars[1:10, .(kpl = mpg * 0.425)]\n\n        kpl\n      &lt;num&gt;\n 1:  8.9250\n 2:  8.9250\n 3:  9.6900\n 4:  9.0950\n 5:  7.9475\n 6:  7.6925\n 7:  6.0775\n 8: 10.3700\n 9:  9.6900\n10:  8.1600\n\n# Select and compute multiple columns\ndt_cars[1:10, .(model, kpl = mpg * 0.425, hp_per_cyl = hp/cyl)]\n\n                model     kpl hp_per_cyl\n               &lt;char&gt;   &lt;num&gt;      &lt;num&gt;\n 1:         Mazda RX4  8.9250   18.33333\n 2:     Mazda RX4 Wag  8.9250   18.33333\n 3:        Datsun 710  9.6900   23.25000\n 4:    Hornet 4 Drive  9.0950   18.33333\n 5: Hornet Sportabout  7.9475   21.87500\n 6:           Valiant  7.6925   17.50000\n 7:        Duster 360  6.0775   30.62500\n 8:         Merc 240D 10.3700   15.50000\n 9:          Merc 230  9.6900   23.75000\n10:          Merc 280  8.1600   20.50000\n\n# Apply functions\ndt_cars[1:10, .(avg_mpg = mean(mpg), max_hp = max(hp))]\n\n   avg_mpg max_hp\n     &lt;num&gt;  &lt;num&gt;\n1:   20.37    245\n\n\n\n\n\n\n# Group by cylinder and calculate statistics\ndt_cars[, .(count = .N, avg_mpg = mean(mpg)), by = cyl]\n\n     cyl count  avg_mpg\n   &lt;num&gt; &lt;int&gt;    &lt;num&gt;\n1:     6     7 19.74286\n2:     4    11 26.66364\n3:     8    14 15.10000\n\n# Multiple grouping variables\ndt_cars[, .(count = .N, avg_mpg = mean(mpg)), by = .(cyl, gear)]\n\n     cyl  gear count avg_mpg\n   &lt;num&gt; &lt;num&gt; &lt;int&gt;   &lt;num&gt;\n1:     6     4     4  19.750\n2:     4     4     8  26.925\n3:     6     3     2  19.750\n4:     8     3    12  15.050\n5:     4     3     1  21.500\n6:     4     5     2  28.200\n7:     8     5     2  15.400\n8:     6     5     1  19.700\n\n# Grouping with expressions\ndt_cars[, .(count = .N), by = .(cyl, high_mpg = mpg &gt; 20)]\n\n     cyl high_mpg count\n   &lt;num&gt;   &lt;lgcl&gt; &lt;int&gt;\n1:     6     TRUE     3\n2:     4     TRUE    11\n3:     8    FALSE    14\n4:     6    FALSE     4\n\n\n\n\n\n\ndata.table provides special symbols for common operations:\n\n# .N: number of rows\ndt_cars[, .N]\n\n[1] 32\n\ndt_cars[, .N, by = cyl]\n\n     cyl     N\n   &lt;num&gt; &lt;int&gt;\n1:     6     7\n2:     4    11\n3:     8    14\n\n# .SD: Subset of Data\ndt_cars[, lapply(.SD, mean), by = cyl, .SDcols = c(\"mpg\", \"hp\", \"wt\")]\n\n     cyl      mpg        hp       wt\n   &lt;num&gt;    &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1:     6 19.74286 122.28571 3.117143\n2:     4 26.66364  82.63636 2.285727\n3:     8 15.10000 209.21429 3.999214\n\n# .I: Row numbers\ndt_cars[, .I[1:2], by = cyl]  # First two row numbers for each cyl group\n\n     cyl    V1\n   &lt;num&gt; &lt;int&gt;\n1:     6     1\n2:     6     2\n3:     4     3\n4:     4     8\n5:     8     5\n6:     8     7\n\n\n\n\n\ndata.table allows efficient in-place modifications:\n\n# Create a copy to avoid modifying the original\ndt_copy &lt;- copy(dt_cars)\n\n# Add a new column\ndt_copy[, efficiency := mpg/wt]\nhead(dt_copy)\n\n               model   mpg   cyl  disp    hp  drat    wt  qsec    vs    am\n              &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:         Mazda RX4  21.0     6   160   110  3.90 2.620 16.46     0     1\n2:     Mazda RX4 Wag  21.0     6   160   110  3.90 2.875 17.02     0     1\n3:        Datsun 710  22.8     4   108    93  3.85 2.320 18.61     1     1\n4:    Hornet 4 Drive  21.4     6   258   110  3.08 3.215 19.44     1     0\n5: Hornet Sportabout  18.7     8   360   175  3.15 3.440 17.02     0     0\n6:           Valiant  18.1     6   225   105  2.76 3.460 20.22     1     0\n    gear  carb efficiency\n   &lt;num&gt; &lt;num&gt;      &lt;num&gt;\n1:     4     4   8.015267\n2:     4     4   7.304348\n3:     4     1   9.827586\n4:     3     1   6.656299\n5:     3     2   5.436047\n6:     3     1   5.231214\n\n# Update existing values\ndt_copy[cyl == 4, mpg := mpg * 1.1]  # Increase mpg by 10% for 4-cylinder cars\nhead(dt_copy)\n\n               model   mpg   cyl  disp    hp  drat    wt  qsec    vs    am\n              &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:         Mazda RX4 21.00     6   160   110  3.90 2.620 16.46     0     1\n2:     Mazda RX4 Wag 21.00     6   160   110  3.90 2.875 17.02     0     1\n3:        Datsun 710 25.08     4   108    93  3.85 2.320 18.61     1     1\n4:    Hornet 4 Drive 21.40     6   258   110  3.08 3.215 19.44     1     0\n5: Hornet Sportabout 18.70     8   360   175  3.15 3.440 17.02     0     0\n6:           Valiant 18.10     6   225   105  2.76 3.460 20.22     1     0\n    gear  carb efficiency\n   &lt;num&gt; &lt;num&gt;      &lt;num&gt;\n1:     4     4   8.015267\n2:     4     4   7.304348\n3:     4     1   9.827586\n4:     3     1   6.656299\n5:     3     2   5.436047\n6:     3     1   5.231214\n\n# Delete columns\ndt_copy[, c(\"carb\", \"vs\") := NULL]\nhead(dt_copy)\n\n               model   mpg   cyl  disp    hp  drat    wt  qsec    am  gear\n              &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:         Mazda RX4 21.00     6   160   110  3.90 2.620 16.46     1     4\n2:     Mazda RX4 Wag 21.00     6   160   110  3.90 2.875 17.02     1     4\n3:        Datsun 710 25.08     4   108    93  3.85 2.320 18.61     1     4\n4:    Hornet 4 Drive 21.40     6   258   110  3.08 3.215 19.44     0     3\n5: Hornet Sportabout 18.70     8   360   175  3.15 3.440 17.02     0     3\n6:           Valiant 18.10     6   225   105  2.76 3.460 20.22     0     3\n   efficiency\n        &lt;num&gt;\n1:   8.015267\n2:   7.304348\n3:   9.827586\n4:   6.656299\n5:   5.436047\n6:   5.231214\n\n\n\n\n\nSetting keys enables fast subsetting and joins:\n\n# Set a key\nsetkey(dt_copy, cyl)\ndt_copy\n\nKey: &lt;cyl&gt;\n                  model   mpg   cyl  disp    hp  drat    wt  qsec    am  gear\n                 &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n 1:          Datsun 710 25.08     4 108.0    93  3.85 2.320 18.61     1     4\n 2:           Merc 240D 26.84     4 146.7    62  3.69 3.190 20.00     0     4\n 3:            Merc 230 25.08     4 140.8    95  3.92 3.150 22.90     0     4\n 4:            Fiat 128 35.64     4  78.7    66  4.08 2.200 19.47     1     4\n 5:         Honda Civic 33.44     4  75.7    52  4.93 1.615 18.52     1     4\n 6:      Toyota Corolla 37.29     4  71.1    65  4.22 1.835 19.90     1     4\n 7:       Toyota Corona 23.65     4 120.1    97  3.70 2.465 20.01     0     3\n 8:           Fiat X1-9 30.03     4  79.0    66  4.08 1.935 18.90     1     4\n 9:       Porsche 914-2 28.60     4 120.3    91  4.43 2.140 16.70     1     5\n10:        Lotus Europa 33.44     4  95.1   113  3.77 1.513 16.90     1     5\n11:          Volvo 142E 23.54     4 121.0   109  4.11 2.780 18.60     1     4\n12:           Mazda RX4 21.00     6 160.0   110  3.90 2.620 16.46     1     4\n13:       Mazda RX4 Wag 21.00     6 160.0   110  3.90 2.875 17.02     1     4\n14:      Hornet 4 Drive 21.40     6 258.0   110  3.08 3.215 19.44     0     3\n15:             Valiant 18.10     6 225.0   105  2.76 3.460 20.22     0     3\n16:            Merc 280 19.20     6 167.6   123  3.92 3.440 18.30     0     4\n17:           Merc 280C 17.80     6 167.6   123  3.92 3.440 18.90     0     4\n18:        Ferrari Dino 19.70     6 145.0   175  3.62 2.770 15.50     1     5\n19:   Hornet Sportabout 18.70     8 360.0   175  3.15 3.440 17.02     0     3\n20:          Duster 360 14.30     8 360.0   245  3.21 3.570 15.84     0     3\n21:          Merc 450SE 16.40     8 275.8   180  3.07 4.070 17.40     0     3\n22:          Merc 450SL 17.30     8 275.8   180  3.07 3.730 17.60     0     3\n23:         Merc 450SLC 15.20     8 275.8   180  3.07 3.780 18.00     0     3\n24:  Cadillac Fleetwood 10.40     8 472.0   205  2.93 5.250 17.98     0     3\n25: Lincoln Continental 10.40     8 460.0   215  3.00 5.424 17.82     0     3\n26:   Chrysler Imperial 14.70     8 440.0   230  3.23 5.345 17.42     0     3\n27:    Dodge Challenger 15.50     8 318.0   150  2.76 3.520 16.87     0     3\n28:         AMC Javelin 15.20     8 304.0   150  3.15 3.435 17.30     0     3\n29:          Camaro Z28 13.30     8 350.0   245  3.73 3.840 15.41     0     3\n30:    Pontiac Firebird 19.20     8 400.0   175  3.08 3.845 17.05     0     3\n31:      Ford Pantera L 15.80     8 351.0   264  4.22 3.170 14.50     1     5\n32:       Maserati Bora 15.00     8 301.0   335  3.54 3.570 14.60     1     5\n                  model   mpg   cyl  disp    hp  drat    wt  qsec    am  gear\n    efficiency\n         &lt;num&gt;\n 1:   9.827586\n 2:   7.648903\n 3:   7.238095\n 4:  14.727273\n 5:  18.823529\n 6:  18.474114\n 7:   8.722110\n 8:  14.108527\n 9:  12.149533\n10:  20.092531\n11:   7.697842\n12:   8.015267\n13:   7.304348\n14:   6.656299\n15:   5.231214\n16:   5.581395\n17:   5.174419\n18:   7.111913\n19:   5.436047\n20:   4.005602\n21:   4.029484\n22:   4.638070\n23:   4.021164\n24:   1.980952\n25:   1.917404\n26:   2.750234\n27:   4.403409\n28:   4.425036\n29:   3.463542\n30:   4.993498\n31:   4.984227\n32:   4.201681\n    efficiency\n\n# Fast subsetting using key\ndt_copy[.(6)]  # All rows where cyl == 6\n\nKey: &lt;cyl&gt;\n            model   mpg   cyl  disp    hp  drat    wt  qsec    am  gear\n           &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:      Mazda RX4  21.0     6 160.0   110  3.90 2.620 16.46     1     4\n2:  Mazda RX4 Wag  21.0     6 160.0   110  3.90 2.875 17.02     1     4\n3: Hornet 4 Drive  21.4     6 258.0   110  3.08 3.215 19.44     0     3\n4:        Valiant  18.1     6 225.0   105  2.76 3.460 20.22     0     3\n5:       Merc 280  19.2     6 167.6   123  3.92 3.440 18.30     0     4\n6:      Merc 280C  17.8     6 167.6   123  3.92 3.440 18.90     0     4\n7:   Ferrari Dino  19.7     6 145.0   175  3.62 2.770 15.50     1     5\n   efficiency\n        &lt;num&gt;\n1:   8.015267\n2:   7.304348\n3:   6.656299\n4:   5.231214\n5:   5.581395\n6:   5.174419\n7:   7.111913\n\n# Multiple keys\nsetkey(dt_copy, cyl, gear)\ndt_copy[.(6, 4)]  # All rows where cyl == 6 and gear == 4\n\nKey: &lt;cyl, gear&gt;\n           model   mpg   cyl  disp    hp  drat    wt  qsec    am  gear\n          &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:     Mazda RX4  21.0     6 160.0   110  3.90 2.620 16.46     1     4\n2: Mazda RX4 Wag  21.0     6 160.0   110  3.90 2.875 17.02     1     4\n3:      Merc 280  19.2     6 167.6   123  3.92 3.440 18.30     0     4\n4:     Merc 280C  17.8     6 167.6   123  3.92 3.440 18.90     0     4\n   efficiency\n        &lt;num&gt;\n1:   8.015267\n2:   7.304348\n3:   5.581395\n4:   5.174419\n\n\n\n\n\ndata.table provides efficient joins using keys:\n\n# Create sample data.tables\nmanufacturers &lt;- data.table(\n  make = c(\"Honda\", \"Toyota\", \"Ford\", \"BMW\", \"Mercedes\"),\n  country = c(\"Japan\", \"Japan\", \"USA\", \"Germany\", \"Germany\")\n)\n\ncars &lt;- data.table(\n  model = c(\"Civic\", \"Corolla\", \"Focus\", \"3 Series\", \"Fiesta\"),\n  make = c(\"Honda\", \"Toyota\", \"Ford\", \"BMW\", \"Ford\")\n)\n\n# Set keys for joining\nsetkey(manufacturers, make)\nsetkey(cars, make)\n\n# Inner join\ncars[manufacturers]\n\nKey: &lt;make&gt;\n      model     make country\n     &lt;char&gt;   &lt;char&gt;  &lt;char&gt;\n1: 3 Series      BMW Germany\n2:    Focus     Ford     USA\n3:   Fiesta     Ford     USA\n4:    Civic    Honda   Japan\n5:     &lt;NA&gt; Mercedes Germany\n6:  Corolla   Toyota   Japan\n\n# Left join\nmanufacturers[cars, nomatch=NA]\n\nKey: &lt;make&gt;\n     make country    model\n   &lt;char&gt;  &lt;char&gt;   &lt;char&gt;\n1:    BMW Germany 3 Series\n2:   Ford     USA    Focus\n3:   Ford     USA   Fiesta\n4:  Honda   Japan    Civic\n5: Toyota   Japan  Corolla\n\n# Non-equi joins\ndt_cars[dt_cars[, .(max_mpg = max(mpg)), by = cyl], on = .(mpg = max_mpg, cyl)]\n\n              model   mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear\n             &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:   Hornet 4 Drive  21.4     6 258.0   110  3.08 3.215 19.44     1     0     3\n2:   Toyota Corolla  33.9     4  71.1    65  4.22 1.835 19.90     1     1     4\n3: Pontiac Firebird  19.2     8 400.0   175  3.08 3.845 17.05     0     0     3\n    carb\n   &lt;num&gt;\n1:     1\n2:     1\n3:     2\n\n\n\n\n\ndata.table provides functions for reshaping data:\n\n# Create a sample data.table\ndt &lt;- data.table(\n  id = rep(1:3, each = 2),\n  variable = rep(c(\"height\", \"weight\"), 3),\n  value = c(170, 68, 155, 52, 182, 75)\n)\ndt\n\n      id variable value\n   &lt;int&gt;   &lt;char&gt; &lt;num&gt;\n1:     1   height   170\n2:     1   weight    68\n3:     2   height   155\n4:     2   weight    52\n5:     3   height   182\n6:     3   weight    75\n\n# Wide to long\ndt_wide &lt;- dcast(dt, id ~ variable, value.var = \"value\")\ndt_wide\n\nKey: &lt;id&gt;\n      id height weight\n   &lt;int&gt;  &lt;num&gt;  &lt;num&gt;\n1:     1    170     68\n2:     2    155     52\n3:     3    182     75\n\n# Long to wide\ndt_long &lt;- melt(dt_wide, id.vars = \"id\", variable.name = \"measure\", value.name = \"value\")\ndt_long\n\n      id measure value\n   &lt;int&gt;  &lt;fctr&gt; &lt;num&gt;\n1:     1  height   170\n2:     2  height   155\n3:     3  height   182\n4:     1  weight    68\n5:     2  weight    52\n6:     3  weight    75\n\n\n\n\n\ndata.table is designed for performance:\n\n# Create a larger dataset for demonstration\nset.seed(123)\nn &lt;- 1e6\ndt_large &lt;- data.table(\n  id = 1:n,\n  x = sample(1:100, n, replace = TRUE),\n  y = sample(letters[1:5], n, replace = TRUE)\n)\n\n# Measure time for a grouped operation\nsystem.time(dt_large[, .(mean_x = mean(x)), by = y])\n\n   user  system elapsed \n   0.01    0.00    0.02 \n\n# Compare with equivalent dplyr operation (if dplyr is installed)\ndf_large &lt;- as.data.frame(dt_large)\nsystem.time(dplyr::summarise(dplyr::group_by(df_large, y), mean_x = mean(x)))\n\n   user  system elapsed \n   0.19    0.00    0.19 \n\n\ndata.table is particularly valuable when working with large datasets due to its efficient memory usage and optimized C implementation."
  },
  {
    "objectID": "get-started/dplyr.html",
    "href": "get-started/dplyr.html",
    "title": "Introduction to dplyr",
    "section": "",
    "text": "The dplyr package is part of the tidyverse and provides a grammar for data manipulation in R. It makes data transformation tasks more intuitive and readable.\n\n\nFirst, let’s install and load the package:\n\n# Install if needed (uncomment to run)\n# install.packages(\"dplyr\")\n# install.packages(\"tibble\")\n\n# Load the packages\nlibrary(dplyr)\nlibrary(tibble)  # For rownames_to_column function\n\n# We'll use the built-in mtcars dataset\ndata(mtcars)\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\n\n\n\ndplyr is built around a set of core verbs (functions) that perform common data manipulation tasks:\n\n\n\n# Select cars with 6 cylinders\nfilter(mtcars, cyl == 6)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nValiant        18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280       19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C      17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nFerrari Dino   19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n\n# Multiple conditions: cars with 6 cylinders AND mpg &gt; 20\nfilter(mtcars, cyl == 6, mpg &gt; 20)\n\n                mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n\n# OR conditions\nfilter(mtcars, cyl == 6 | mpg &gt; 30)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nValiant        18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280       19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C      17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFerrari Dino   19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n\n\n\n\n\n\n# Select specific columns\nselect(mtcars, mpg, cyl, hp)\n\n                     mpg cyl  hp\nMazda RX4           21.0   6 110\nMazda RX4 Wag       21.0   6 110\nDatsun 710          22.8   4  93\nHornet 4 Drive      21.4   6 110\nHornet Sportabout   18.7   8 175\nValiant             18.1   6 105\nDuster 360          14.3   8 245\nMerc 240D           24.4   4  62\nMerc 230            22.8   4  95\nMerc 280            19.2   6 123\nMerc 280C           17.8   6 123\nMerc 450SE          16.4   8 180\nMerc 450SL          17.3   8 180\nMerc 450SLC         15.2   8 180\nCadillac Fleetwood  10.4   8 205\nLincoln Continental 10.4   8 215\nChrysler Imperial   14.7   8 230\nFiat 128            32.4   4  66\nHonda Civic         30.4   4  52\nToyota Corolla      33.9   4  65\nToyota Corona       21.5   4  97\nDodge Challenger    15.5   8 150\nAMC Javelin         15.2   8 150\nCamaro Z28          13.3   8 245\nPontiac Firebird    19.2   8 175\nFiat X1-9           27.3   4  66\nPorsche 914-2       26.0   4  91\nLotus Europa        30.4   4 113\nFord Pantera L      15.8   8 264\nFerrari Dino        19.7   6 175\nMaserati Bora       15.0   8 335\nVolvo 142E          21.4   4 109\n\n# Select a range of columns\nselect(mtcars, mpg:hp)\n\n                     mpg cyl  disp  hp\nMazda RX4           21.0   6 160.0 110\nMazda RX4 Wag       21.0   6 160.0 110\nDatsun 710          22.8   4 108.0  93\nHornet 4 Drive      21.4   6 258.0 110\nHornet Sportabout   18.7   8 360.0 175\nValiant             18.1   6 225.0 105\nDuster 360          14.3   8 360.0 245\nMerc 240D           24.4   4 146.7  62\nMerc 230            22.8   4 140.8  95\nMerc 280            19.2   6 167.6 123\nMerc 280C           17.8   6 167.6 123\nMerc 450SE          16.4   8 275.8 180\nMerc 450SL          17.3   8 275.8 180\nMerc 450SLC         15.2   8 275.8 180\nCadillac Fleetwood  10.4   8 472.0 205\nLincoln Continental 10.4   8 460.0 215\nChrysler Imperial   14.7   8 440.0 230\nFiat 128            32.4   4  78.7  66\nHonda Civic         30.4   4  75.7  52\nToyota Corolla      33.9   4  71.1  65\nToyota Corona       21.5   4 120.1  97\nDodge Challenger    15.5   8 318.0 150\nAMC Javelin         15.2   8 304.0 150\nCamaro Z28          13.3   8 350.0 245\nPontiac Firebird    19.2   8 400.0 175\nFiat X1-9           27.3   4  79.0  66\nPorsche 914-2       26.0   4 120.3  91\nLotus Europa        30.4   4  95.1 113\nFord Pantera L      15.8   8 351.0 264\nFerrari Dino        19.7   6 145.0 175\nMaserati Bora       15.0   8 301.0 335\nVolvo 142E          21.4   4 121.0 109\n\n# Select all columns except some\nselect(mtcars, -gear, -carb)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1\n\n# Rename columns while selecting\nselect(mtcars, miles_per_gallon = mpg, cylinders = cyl)\n\n                    miles_per_gallon cylinders\nMazda RX4                       21.0         6\nMazda RX4 Wag                   21.0         6\nDatsun 710                      22.8         4\nHornet 4 Drive                  21.4         6\nHornet Sportabout               18.7         8\nValiant                         18.1         6\nDuster 360                      14.3         8\nMerc 240D                       24.4         4\nMerc 230                        22.8         4\nMerc 280                        19.2         6\nMerc 280C                       17.8         6\nMerc 450SE                      16.4         8\nMerc 450SL                      17.3         8\nMerc 450SLC                     15.2         8\nCadillac Fleetwood              10.4         8\nLincoln Continental             10.4         8\nChrysler Imperial               14.7         8\nFiat 128                        32.4         4\nHonda Civic                     30.4         4\nToyota Corolla                  33.9         4\nToyota Corona                   21.5         4\nDodge Challenger                15.5         8\nAMC Javelin                     15.2         8\nCamaro Z28                      13.3         8\nPontiac Firebird                19.2         8\nFiat X1-9                       27.3         4\nPorsche 914-2                   26.0         4\nLotus Europa                    30.4         4\nFord Pantera L                  15.8         8\nFerrari Dino                    19.7         6\nMaserati Bora                   15.0         8\nVolvo 142E                      21.4         4\n\n\n\n\n\n\n# Add a new column\nmutate(mtcars, \n       kpl = mpg * 0.425,  # Convert mpg to km per liter\n       hp_per_cyl = hp / cyl)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb     kpl\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  8.9250\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  8.9250\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1  9.6900\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  9.0950\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  7.9475\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  7.6925\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  6.0775\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2 10.3700\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  9.6900\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  8.1600\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  7.5650\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  6.9700\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  7.3525\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  6.4600\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  4.4200\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  4.4200\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  6.2475\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1 13.7700\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2 12.9200\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1 14.4075\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  9.1375\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  6.5875\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  6.4600\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  5.6525\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  8.1600\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1 11.6025\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 11.0500\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2 12.9200\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  6.7150\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  8.3725\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  6.3750\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2  9.0950\n                    hp_per_cyl\nMazda RX4             18.33333\nMazda RX4 Wag         18.33333\nDatsun 710            23.25000\nHornet 4 Drive        18.33333\nHornet Sportabout     21.87500\nValiant               17.50000\nDuster 360            30.62500\nMerc 240D             15.50000\nMerc 230              23.75000\nMerc 280              20.50000\nMerc 280C             20.50000\nMerc 450SE            22.50000\nMerc 450SL            22.50000\nMerc 450SLC           22.50000\nCadillac Fleetwood    25.62500\nLincoln Continental   26.87500\nChrysler Imperial     28.75000\nFiat 128              16.50000\nHonda Civic           13.00000\nToyota Corolla        16.25000\nToyota Corona         24.25000\nDodge Challenger      18.75000\nAMC Javelin           18.75000\nCamaro Z28            30.62500\nPontiac Firebird      21.87500\nFiat X1-9             16.50000\nPorsche 914-2         22.75000\nLotus Europa          28.25000\nFord Pantera L        33.00000\nFerrari Dino          29.16667\nMaserati Bora         41.87500\nVolvo 142E            27.25000\n\n# Modify existing columns and add new ones\nmutate(mtcars,\n       mpg = mpg * 0.425,  # Overwrite mpg with km per liter\n       efficiency = mpg / wt)\n\n                        mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4            8.9250   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag        8.9250   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710           9.6900   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive       9.0950   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout    7.9475   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant              7.6925   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360           6.0775   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           10.3700   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230             9.6900   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280             8.1600   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C            7.5650   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE           6.9700   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL           7.3525   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC          6.4600   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood   4.4200   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental  4.4200   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial    6.2475   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            13.7700   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         12.9200   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      14.4075   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona        9.1375   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger     6.5875   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin          6.4600   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28           5.6525   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird     8.1600   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           11.6025   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       11.0500   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        12.9200   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L       6.7150   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino         8.3725   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora        6.3750   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E           9.0950   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n                    efficiency\nMazda RX4            3.4064885\nMazda RX4 Wag        3.1043478\nDatsun 710           4.1767241\nHornet 4 Drive       2.8289269\nHornet Sportabout    2.3103198\nValiant              2.2232659\nDuster 360           1.7023810\nMerc 240D            3.2507837\nMerc 230             3.0761905\nMerc 280             2.3720930\nMerc 280C            2.1991279\nMerc 450SE           1.7125307\nMerc 450SL           1.9711796\nMerc 450SLC          1.7089947\nCadillac Fleetwood   0.8419048\nLincoln Continental  0.8148968\nChrysler Imperial    1.1688494\nFiat 128             6.2590909\nHonda Civic          8.0000000\nToyota Corolla       7.8514986\nToyota Corona        3.7068966\nDodge Challenger     1.8714489\nAMC Javelin          1.8806405\nCamaro Z28           1.4720052\nPontiac Firebird     2.1222367\nFiat X1-9            5.9961240\nPorsche 914-2        5.1635514\nLotus Europa         8.5393258\nFord Pantera L       2.1182965\nFerrari Dino         3.0225632\nMaserati Bora        1.7857143\nVolvo 142E           3.2715827\n\n\n\n\n\n\n# Sort by mpg (ascending)\narrange(mtcars, mpg)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n\n# Sort by mpg (descending)\narrange(mtcars, desc(mpg))\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n\n# Sort by multiple columns\narrange(mtcars, cyl, desc(mpg))\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n\n\n\n\n\n\n# Calculate summary statistics\nsummarize(mtcars,\n          avg_mpg = mean(mpg),\n          max_hp = max(hp),\n          count = n())\n\n   avg_mpg max_hp count\n1 20.09062    335    32\n\n\n\n\n\n\n# Group by cylinder and calculate statistics per group\nmtcars %&gt;%\n  group_by(cyl) %&gt;%\n  summarize(\n    count = n(),\n    avg_mpg = mean(mpg),\n    avg_hp = mean(hp),\n    .groups = \"drop\"\n  )\n\n# A tibble: 3 × 4\n    cyl count avg_mpg avg_hp\n  &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1     4    11    26.7   82.6\n2     6     7    19.7  122. \n3     8    14    15.1  209. \n\n\n\n\n\n\nThe pipe operator makes code more readable by chaining operations:\n\n# Without pipes\nresult1 &lt;- filter(mtcars, cyl == 4)\nresult2 &lt;- select(result1, mpg, hp, wt)\nresult3 &lt;- arrange(result2, desc(mpg))\nresult3\n\n                mpg  hp    wt\nToyota Corolla 33.9  65 1.835\nFiat 128       32.4  66 2.200\nHonda Civic    30.4  52 1.615\nLotus Europa   30.4 113 1.513\nFiat X1-9      27.3  66 1.935\nPorsche 914-2  26.0  91 2.140\nMerc 240D      24.4  62 3.190\nDatsun 710     22.8  93 2.320\nMerc 230       22.8  95 3.150\nToyota Corona  21.5  97 2.465\nVolvo 142E     21.4 109 2.780\n\n# With pipes - same operations, more readable\nmtcars %&gt;%\n  filter(cyl == 4) %&gt;%\n  select(mpg, hp, wt) %&gt;%\n  arrange(desc(mpg))\n\n                mpg  hp    wt\nToyota Corolla 33.9  65 1.835\nFiat 128       32.4  66 2.200\nHonda Civic    30.4  52 1.615\nLotus Europa   30.4 113 1.513\nFiat X1-9      27.3  66 1.935\nPorsche 914-2  26.0  91 2.140\nMerc 240D      24.4  62 3.190\nDatsun 710     22.8  93 2.320\nMerc 230       22.8  95 3.150\nToyota Corona  21.5  97 2.465\nVolvo 142E     21.4 109 2.780\n\n\n\n\n\n\n\n\n# Get unique values of cyl and gear\nmtcars %&gt;%\n  select(cyl, gear) %&gt;%\n  distinct()\n\n                  cyl gear\nMazda RX4           6    4\nDatsun 710          4    4\nHornet 4 Drive      6    3\nHornet Sportabout   8    3\nToyota Corona       4    3\nPorsche 914-2       4    5\nFord Pantera L      8    5\nFerrari Dino        6    5\n\n\n\n\n\n\n# Count cars by cylinder\nmtcars %&gt;%\n  count(cyl, sort = TRUE)\n\n  cyl  n\n1   8 14\n2   4 11\n3   6  7\n\n# Count by multiple variables\nmtcars %&gt;%\n  count(cyl, gear)\n\n  cyl gear  n\n1   4    3  1\n2   4    4  8\n3   4    5  2\n4   6    3  2\n5   6    4  4\n6   6    5  1\n7   8    3 12\n8   8    5  2\n\n\n\n\n\n\n# Select first 5 rows\nmtcars %&gt;%\n  slice(1:5)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n# Select top 3 rows by mpg\nmtcars %&gt;%\n  arrange(desc(mpg)) %&gt;%\n  slice(1:3)\n\n                mpg cyl disp hp drat    wt  qsec vs am gear carb\nToyota Corolla 33.9   4 71.1 65 4.22 1.835 19.90  1  1    4    1\nFiat 128       32.4   4 78.7 66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4 75.7 52 4.93 1.615 18.52  1  1    4    2\n\n\n\n\n\n\n# Extract mpg column as a vector\nmtcars %&gt;%\n  pull(mpg)\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\n\n\n\n\n\nLet’s solve a more complex problem by combining multiple dplyr functions:\n\n# First add rownames as a column\ncars_with_names &lt;- mtcars %&gt;%\n  tibble::rownames_to_column(\"model\")\n\n# Now perform the analysis\ncars_with_names %&gt;%\n  group_by(cyl) %&gt;%\n  filter(mpg == max(mpg)) %&gt;%\n  select(cyl, model, mpg, hp) %&gt;%\n  arrange(cyl) %&gt;%\n  ungroup()\n\n# A tibble: 3 × 4\n    cyl model              mpg    hp\n  &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n1     4 Toyota Corolla    33.9    65\n2     6 Hornet 4 Drive    21.4   110\n3     8 Pontiac Firebird  19.2   175\n\n\n\n\n\ndplyr provides functions for joining datasets:\n\n# Create sample datasets\nmanufacturers &lt;- data.frame(\n  make = c(\"Honda\", \"Toyota\", \"Ford\", \"BMW\", \"Mercedes\"),\n  country = c(\"Japan\", \"Japan\", \"USA\", \"Germany\", \"Germany\"),\n  stringsAsFactors = FALSE\n)\n\ncars &lt;- data.frame(\n  model = c(\"Civic\", \"Corolla\", \"Focus\", \"3 Series\", \"Fiesta\"),\n  make = c(\"Honda\", \"Toyota\", \"Ford\", \"BMW\", \"Ford\"),\n  stringsAsFactors = FALSE\n)\n\n# Inner join - only matching rows\ninner_join(cars, manufacturers, by = \"make\")\n\n     model   make country\n1    Civic  Honda   Japan\n2  Corolla Toyota   Japan\n3    Focus   Ford     USA\n4 3 Series    BMW Germany\n5   Fiesta   Ford     USA\n\n# Left join - all rows from cars\nleft_join(cars, manufacturers, by = \"make\")\n\n     model   make country\n1    Civic  Honda   Japan\n2  Corolla Toyota   Japan\n3    Focus   Ford     USA\n4 3 Series    BMW Germany\n5   Fiesta   Ford     USA\n\n# Full join - all rows from both\nfull_join(cars, manufacturers, by = \"make\")\n\n     model     make country\n1    Civic    Honda   Japan\n2  Corolla   Toyota   Japan\n3    Focus     Ford     USA\n4 3 Series      BMW Germany\n5   Fiesta     Ford     USA\n6     &lt;NA&gt; Mercedes Germany\n\n\ndplyr makes data manipulation in R more intuitive and efficient. Its consistent syntax and the pipe operator allow you to write code that’s both powerful and readable."
  },
  {
    "objectID": "get-started/dplyr.html#introduction-to-dplyr",
    "href": "get-started/dplyr.html#introduction-to-dplyr",
    "title": "Introduction to dplyr",
    "section": "",
    "text": "The dplyr package is part of the tidyverse and provides a grammar for data manipulation in R. It makes data transformation tasks more intuitive and readable.\n\n\nFirst, let’s install and load the package:\n\n# Install if needed (uncomment to run)\n# install.packages(\"dplyr\")\n# install.packages(\"tibble\")\n\n# Load the packages\nlibrary(dplyr)\nlibrary(tibble)  # For rownames_to_column function\n\n# We'll use the built-in mtcars dataset\ndata(mtcars)\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\n\n\n\ndplyr is built around a set of core verbs (functions) that perform common data manipulation tasks:\n\n\n\n# Select cars with 6 cylinders\nfilter(mtcars, cyl == 6)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nValiant        18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280       19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C      17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nFerrari Dino   19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n\n# Multiple conditions: cars with 6 cylinders AND mpg &gt; 20\nfilter(mtcars, cyl == 6, mpg &gt; 20)\n\n                mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n\n# OR conditions\nfilter(mtcars, cyl == 6 | mpg &gt; 30)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nValiant        18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280       19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C      17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFerrari Dino   19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n\n\n\n\n\n\n# Select specific columns\nselect(mtcars, mpg, cyl, hp)\n\n                     mpg cyl  hp\nMazda RX4           21.0   6 110\nMazda RX4 Wag       21.0   6 110\nDatsun 710          22.8   4  93\nHornet 4 Drive      21.4   6 110\nHornet Sportabout   18.7   8 175\nValiant             18.1   6 105\nDuster 360          14.3   8 245\nMerc 240D           24.4   4  62\nMerc 230            22.8   4  95\nMerc 280            19.2   6 123\nMerc 280C           17.8   6 123\nMerc 450SE          16.4   8 180\nMerc 450SL          17.3   8 180\nMerc 450SLC         15.2   8 180\nCadillac Fleetwood  10.4   8 205\nLincoln Continental 10.4   8 215\nChrysler Imperial   14.7   8 230\nFiat 128            32.4   4  66\nHonda Civic         30.4   4  52\nToyota Corolla      33.9   4  65\nToyota Corona       21.5   4  97\nDodge Challenger    15.5   8 150\nAMC Javelin         15.2   8 150\nCamaro Z28          13.3   8 245\nPontiac Firebird    19.2   8 175\nFiat X1-9           27.3   4  66\nPorsche 914-2       26.0   4  91\nLotus Europa        30.4   4 113\nFord Pantera L      15.8   8 264\nFerrari Dino        19.7   6 175\nMaserati Bora       15.0   8 335\nVolvo 142E          21.4   4 109\n\n# Select a range of columns\nselect(mtcars, mpg:hp)\n\n                     mpg cyl  disp  hp\nMazda RX4           21.0   6 160.0 110\nMazda RX4 Wag       21.0   6 160.0 110\nDatsun 710          22.8   4 108.0  93\nHornet 4 Drive      21.4   6 258.0 110\nHornet Sportabout   18.7   8 360.0 175\nValiant             18.1   6 225.0 105\nDuster 360          14.3   8 360.0 245\nMerc 240D           24.4   4 146.7  62\nMerc 230            22.8   4 140.8  95\nMerc 280            19.2   6 167.6 123\nMerc 280C           17.8   6 167.6 123\nMerc 450SE          16.4   8 275.8 180\nMerc 450SL          17.3   8 275.8 180\nMerc 450SLC         15.2   8 275.8 180\nCadillac Fleetwood  10.4   8 472.0 205\nLincoln Continental 10.4   8 460.0 215\nChrysler Imperial   14.7   8 440.0 230\nFiat 128            32.4   4  78.7  66\nHonda Civic         30.4   4  75.7  52\nToyota Corolla      33.9   4  71.1  65\nToyota Corona       21.5   4 120.1  97\nDodge Challenger    15.5   8 318.0 150\nAMC Javelin         15.2   8 304.0 150\nCamaro Z28          13.3   8 350.0 245\nPontiac Firebird    19.2   8 400.0 175\nFiat X1-9           27.3   4  79.0  66\nPorsche 914-2       26.0   4 120.3  91\nLotus Europa        30.4   4  95.1 113\nFord Pantera L      15.8   8 351.0 264\nFerrari Dino        19.7   6 145.0 175\nMaserati Bora       15.0   8 301.0 335\nVolvo 142E          21.4   4 121.0 109\n\n# Select all columns except some\nselect(mtcars, -gear, -carb)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1\n\n# Rename columns while selecting\nselect(mtcars, miles_per_gallon = mpg, cylinders = cyl)\n\n                    miles_per_gallon cylinders\nMazda RX4                       21.0         6\nMazda RX4 Wag                   21.0         6\nDatsun 710                      22.8         4\nHornet 4 Drive                  21.4         6\nHornet Sportabout               18.7         8\nValiant                         18.1         6\nDuster 360                      14.3         8\nMerc 240D                       24.4         4\nMerc 230                        22.8         4\nMerc 280                        19.2         6\nMerc 280C                       17.8         6\nMerc 450SE                      16.4         8\nMerc 450SL                      17.3         8\nMerc 450SLC                     15.2         8\nCadillac Fleetwood              10.4         8\nLincoln Continental             10.4         8\nChrysler Imperial               14.7         8\nFiat 128                        32.4         4\nHonda Civic                     30.4         4\nToyota Corolla                  33.9         4\nToyota Corona                   21.5         4\nDodge Challenger                15.5         8\nAMC Javelin                     15.2         8\nCamaro Z28                      13.3         8\nPontiac Firebird                19.2         8\nFiat X1-9                       27.3         4\nPorsche 914-2                   26.0         4\nLotus Europa                    30.4         4\nFord Pantera L                  15.8         8\nFerrari Dino                    19.7         6\nMaserati Bora                   15.0         8\nVolvo 142E                      21.4         4\n\n\n\n\n\n\n# Add a new column\nmutate(mtcars, \n       kpl = mpg * 0.425,  # Convert mpg to km per liter\n       hp_per_cyl = hp / cyl)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb     kpl\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  8.9250\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  8.9250\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1  9.6900\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  9.0950\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  7.9475\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  7.6925\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  6.0775\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2 10.3700\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  9.6900\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  8.1600\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  7.5650\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  6.9700\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  7.3525\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  6.4600\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  4.4200\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  4.4200\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  6.2475\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1 13.7700\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2 12.9200\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1 14.4075\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  9.1375\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  6.5875\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  6.4600\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  5.6525\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  8.1600\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1 11.6025\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 11.0500\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2 12.9200\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  6.7150\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  8.3725\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  6.3750\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2  9.0950\n                    hp_per_cyl\nMazda RX4             18.33333\nMazda RX4 Wag         18.33333\nDatsun 710            23.25000\nHornet 4 Drive        18.33333\nHornet Sportabout     21.87500\nValiant               17.50000\nDuster 360            30.62500\nMerc 240D             15.50000\nMerc 230              23.75000\nMerc 280              20.50000\nMerc 280C             20.50000\nMerc 450SE            22.50000\nMerc 450SL            22.50000\nMerc 450SLC           22.50000\nCadillac Fleetwood    25.62500\nLincoln Continental   26.87500\nChrysler Imperial     28.75000\nFiat 128              16.50000\nHonda Civic           13.00000\nToyota Corolla        16.25000\nToyota Corona         24.25000\nDodge Challenger      18.75000\nAMC Javelin           18.75000\nCamaro Z28            30.62500\nPontiac Firebird      21.87500\nFiat X1-9             16.50000\nPorsche 914-2         22.75000\nLotus Europa          28.25000\nFord Pantera L        33.00000\nFerrari Dino          29.16667\nMaserati Bora         41.87500\nVolvo 142E            27.25000\n\n# Modify existing columns and add new ones\nmutate(mtcars,\n       mpg = mpg * 0.425,  # Overwrite mpg with km per liter\n       efficiency = mpg / wt)\n\n                        mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4            8.9250   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag        8.9250   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710           9.6900   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive       9.0950   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout    7.9475   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant              7.6925   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360           6.0775   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           10.3700   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230             9.6900   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280             8.1600   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C            7.5650   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE           6.9700   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL           7.3525   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC          6.4600   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood   4.4200   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental  4.4200   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial    6.2475   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            13.7700   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         12.9200   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      14.4075   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona        9.1375   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger     6.5875   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin          6.4600   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28           5.6525   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird     8.1600   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           11.6025   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       11.0500   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        12.9200   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L       6.7150   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino         8.3725   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora        6.3750   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E           9.0950   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n                    efficiency\nMazda RX4            3.4064885\nMazda RX4 Wag        3.1043478\nDatsun 710           4.1767241\nHornet 4 Drive       2.8289269\nHornet Sportabout    2.3103198\nValiant              2.2232659\nDuster 360           1.7023810\nMerc 240D            3.2507837\nMerc 230             3.0761905\nMerc 280             2.3720930\nMerc 280C            2.1991279\nMerc 450SE           1.7125307\nMerc 450SL           1.9711796\nMerc 450SLC          1.7089947\nCadillac Fleetwood   0.8419048\nLincoln Continental  0.8148968\nChrysler Imperial    1.1688494\nFiat 128             6.2590909\nHonda Civic          8.0000000\nToyota Corolla       7.8514986\nToyota Corona        3.7068966\nDodge Challenger     1.8714489\nAMC Javelin          1.8806405\nCamaro Z28           1.4720052\nPontiac Firebird     2.1222367\nFiat X1-9            5.9961240\nPorsche 914-2        5.1635514\nLotus Europa         8.5393258\nFord Pantera L       2.1182965\nFerrari Dino         3.0225632\nMaserati Bora        1.7857143\nVolvo 142E           3.2715827\n\n\n\n\n\n\n# Sort by mpg (ascending)\narrange(mtcars, mpg)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n\n# Sort by mpg (descending)\narrange(mtcars, desc(mpg))\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n\n# Sort by multiple columns\narrange(mtcars, cyl, desc(mpg))\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n\n\n\n\n\n\n# Calculate summary statistics\nsummarize(mtcars,\n          avg_mpg = mean(mpg),\n          max_hp = max(hp),\n          count = n())\n\n   avg_mpg max_hp count\n1 20.09062    335    32\n\n\n\n\n\n\n# Group by cylinder and calculate statistics per group\nmtcars %&gt;%\n  group_by(cyl) %&gt;%\n  summarize(\n    count = n(),\n    avg_mpg = mean(mpg),\n    avg_hp = mean(hp),\n    .groups = \"drop\"\n  )\n\n# A tibble: 3 × 4\n    cyl count avg_mpg avg_hp\n  &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1     4    11    26.7   82.6\n2     6     7    19.7  122. \n3     8    14    15.1  209. \n\n\n\n\n\n\nThe pipe operator makes code more readable by chaining operations:\n\n# Without pipes\nresult1 &lt;- filter(mtcars, cyl == 4)\nresult2 &lt;- select(result1, mpg, hp, wt)\nresult3 &lt;- arrange(result2, desc(mpg))\nresult3\n\n                mpg  hp    wt\nToyota Corolla 33.9  65 1.835\nFiat 128       32.4  66 2.200\nHonda Civic    30.4  52 1.615\nLotus Europa   30.4 113 1.513\nFiat X1-9      27.3  66 1.935\nPorsche 914-2  26.0  91 2.140\nMerc 240D      24.4  62 3.190\nDatsun 710     22.8  93 2.320\nMerc 230       22.8  95 3.150\nToyota Corona  21.5  97 2.465\nVolvo 142E     21.4 109 2.780\n\n# With pipes - same operations, more readable\nmtcars %&gt;%\n  filter(cyl == 4) %&gt;%\n  select(mpg, hp, wt) %&gt;%\n  arrange(desc(mpg))\n\n                mpg  hp    wt\nToyota Corolla 33.9  65 1.835\nFiat 128       32.4  66 2.200\nHonda Civic    30.4  52 1.615\nLotus Europa   30.4 113 1.513\nFiat X1-9      27.3  66 1.935\nPorsche 914-2  26.0  91 2.140\nMerc 240D      24.4  62 3.190\nDatsun 710     22.8  93 2.320\nMerc 230       22.8  95 3.150\nToyota Corona  21.5  97 2.465\nVolvo 142E     21.4 109 2.780\n\n\n\n\n\n\n\n\n# Get unique values of cyl and gear\nmtcars %&gt;%\n  select(cyl, gear) %&gt;%\n  distinct()\n\n                  cyl gear\nMazda RX4           6    4\nDatsun 710          4    4\nHornet 4 Drive      6    3\nHornet Sportabout   8    3\nToyota Corona       4    3\nPorsche 914-2       4    5\nFord Pantera L      8    5\nFerrari Dino        6    5\n\n\n\n\n\n\n# Count cars by cylinder\nmtcars %&gt;%\n  count(cyl, sort = TRUE)\n\n  cyl  n\n1   8 14\n2   4 11\n3   6  7\n\n# Count by multiple variables\nmtcars %&gt;%\n  count(cyl, gear)\n\n  cyl gear  n\n1   4    3  1\n2   4    4  8\n3   4    5  2\n4   6    3  2\n5   6    4  4\n6   6    5  1\n7   8    3 12\n8   8    5  2\n\n\n\n\n\n\n# Select first 5 rows\nmtcars %&gt;%\n  slice(1:5)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n# Select top 3 rows by mpg\nmtcars %&gt;%\n  arrange(desc(mpg)) %&gt;%\n  slice(1:3)\n\n                mpg cyl disp hp drat    wt  qsec vs am gear carb\nToyota Corolla 33.9   4 71.1 65 4.22 1.835 19.90  1  1    4    1\nFiat 128       32.4   4 78.7 66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4 75.7 52 4.93 1.615 18.52  1  1    4    2\n\n\n\n\n\n\n# Extract mpg column as a vector\nmtcars %&gt;%\n  pull(mpg)\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\n\n\n\n\n\nLet’s solve a more complex problem by combining multiple dplyr functions:\n\n# First add rownames as a column\ncars_with_names &lt;- mtcars %&gt;%\n  tibble::rownames_to_column(\"model\")\n\n# Now perform the analysis\ncars_with_names %&gt;%\n  group_by(cyl) %&gt;%\n  filter(mpg == max(mpg)) %&gt;%\n  select(cyl, model, mpg, hp) %&gt;%\n  arrange(cyl) %&gt;%\n  ungroup()\n\n# A tibble: 3 × 4\n    cyl model              mpg    hp\n  &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n1     4 Toyota Corolla    33.9    65\n2     6 Hornet 4 Drive    21.4   110\n3     8 Pontiac Firebird  19.2   175\n\n\n\n\n\ndplyr provides functions for joining datasets:\n\n# Create sample datasets\nmanufacturers &lt;- data.frame(\n  make = c(\"Honda\", \"Toyota\", \"Ford\", \"BMW\", \"Mercedes\"),\n  country = c(\"Japan\", \"Japan\", \"USA\", \"Germany\", \"Germany\"),\n  stringsAsFactors = FALSE\n)\n\ncars &lt;- data.frame(\n  model = c(\"Civic\", \"Corolla\", \"Focus\", \"3 Series\", \"Fiesta\"),\n  make = c(\"Honda\", \"Toyota\", \"Ford\", \"BMW\", \"Ford\"),\n  stringsAsFactors = FALSE\n)\n\n# Inner join - only matching rows\ninner_join(cars, manufacturers, by = \"make\")\n\n     model   make country\n1    Civic  Honda   Japan\n2  Corolla Toyota   Japan\n3    Focus   Ford     USA\n4 3 Series    BMW Germany\n5   Fiesta   Ford     USA\n\n# Left join - all rows from cars\nleft_join(cars, manufacturers, by = \"make\")\n\n     model   make country\n1    Civic  Honda   Japan\n2  Corolla Toyota   Japan\n3    Focus   Ford     USA\n4 3 Series    BMW Germany\n5   Fiesta   Ford     USA\n\n# Full join - all rows from both\nfull_join(cars, manufacturers, by = \"make\")\n\n     model     make country\n1    Civic    Honda   Japan\n2  Corolla   Toyota   Japan\n3    Focus     Ford     USA\n4 3 Series      BMW Germany\n5   Fiesta     Ford     USA\n6     &lt;NA&gt; Mercedes Germany\n\n\ndplyr makes data manipulation in R more intuitive and efficient. Its consistent syntax and the pipe operator allow you to write code that’s both powerful and readable."
  },
  {
    "objectID": "get-started/r-package-management.html#alternative-package-installers",
    "href": "get-started/r-package-management.html#alternative-package-installers",
    "title": "Installing and Managing R Packages",
    "section": "Alternative Package Installers",
    "text": "Alternative Package Installers\n\nUsing pacman\nThe pacman package provides a simplified interface for package management with automatic loading:\n\n# Install pacman first\ninstall.packages(\"pacman\")\n\n# Load and install packages in one step\npacman::p_load(dplyr, ggplot2, tidyr)\n\n# Check if packages are loaded\npacman::p_loaded(dplyr, ggplot2)\n\n# Unload packages\npacman::p_unload(dplyr, ggplot2)\n\nKey advantage: Combines installation and loading in one function (p_load).\n\n\nUsing pak\nThe pak package offers fast and reliable package installation with superior dependency resolution:\n\n# Install pak\ninstall.packages(\"pak\")\n\n# Install packages with pak\npak::pkg_install(\"dplyr\")\n\n# Install multiple packages\npak::pkg_install(c(\"ggplot2\", \"tidyr\", \"readr\"))\n\n# Install from GitHub\npak::pkg_install(\"tidyverse/ggplot2\")\n\nKey advantages: Much faster installation, better dependency handling, and works with multiple repositories (CRAN, GitHub, etc.)."
  },
  {
    "objectID": "posts/intro-to-r-analytics.html#exploring-a-dataset",
    "href": "posts/intro-to-r-analytics.html#exploring-a-dataset",
    "title": "Introduction to R for Analytics",
    "section": "Exploring a Dataset",
    "text": "Exploring a Dataset\nR comes with several built-in datasets perfect for practice. This example examines the mtcars dataset:\n\n# View the first few rows\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n# Quick summary of the dataset structure\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n# Statistical summary of key variables\nsummary(mtcars[, c(\"mpg\", \"wt\", \"hp\")])\n\n      mpg              wt              hp       \n Min.   :10.40   Min.   :1.513   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:2.581   1st Qu.: 96.5  \n Median :19.20   Median :3.325   Median :123.0  \n Mean   :20.09   Mean   :3.217   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:3.610   3rd Qu.:180.0  \n Max.   :33.90   Max.   :5.424   Max.   :335.0  \n\n\nThe mtcars dataset contains information about 32 cars from Motor Trend magazine, including fuel efficiency (mpg), weight (wt), and horsepower (hp)."
  },
  {
    "objectID": "posts/intro-to-r-analytics.html#data-visualization",
    "href": "posts/intro-to-r-analytics.html#data-visualization",
    "title": "Introduction to R for Analytics",
    "section": "Data Visualization",
    "text": "Data Visualization\nVisualization is essential for understanding patterns in data. The following examples create informative plots:\n\nlibrary(ggplot2)\n\n# 1. A scatter plot with regression line\np1 &lt;- ggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(aes(size = hp, color = factor(cyl)), alpha = 0.7) +\n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"#2c3e50\") +\n  labs(title = \"Car Weight vs. Fuel Efficiency\",\n       subtitle = \"Size represents horsepower, color represents cylinders\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\", name = \"Cylinders\")\n\n# 2. Distribution of fuel efficiency\np2 &lt;- ggplot(mtcars, aes(x = mpg, fill = factor(cyl))) +\n  geom_histogram(bins = 10, alpha = 0.7, position = \"identity\") +\n  labs(title = \"Distribution of Fuel Efficiency\",\n       x = \"Miles Per Gallon\",\n       y = \"Count\") +\n  scale_fill_brewer(palette = \"Set1\", name = \"Cylinders\") +\n  theme_minimal()\n\n# Display plots (if using patchwork)\nlibrary(patchwork)\np1 / p2\n\n\n\n\n\n\n\n\nThese visualizations reveal:\n\nA clear negative correlation between car weight and fuel efficiency\nHigher cylinder cars tend to be heavier with lower MPG\nThe MPG distribution varies significantly by cylinder count"
  },
  {
    "objectID": "posts/intro-to-r-analytics.html#data-transformation",
    "href": "posts/intro-to-r-analytics.html#data-transformation",
    "title": "Introduction to R for Analytics",
    "section": "Data Transformation",
    "text": "Data Transformation\nData rarely comes in the exact format needed. The dplyr package makes transformations straightforward:\n\n# Load required packages\nlibrary(dplyr)\nlibrary(tibble)  # For rownames_to_column function\n\n# Create an enhanced version of the dataset\nmtcars_enhanced &lt;- mtcars %&gt;%\n  # Add car names as a column (they're currently row names)\n  rownames_to_column(\"car_name\") %&gt;%\n  # Create useful derived metrics\n  mutate(\n    # Efficiency ratio (higher is better)\n    efficiency_ratio = mpg / wt,\n    \n    # Power-to-weight ratio (higher is better)\n    power_to_weight = hp / wt,\n    \n    # Categorize cars by efficiency\n    efficiency_category = case_when(\n      mpg &gt; 25 ~ \"High Efficiency\",\n      mpg &gt; 15 ~ \"Medium Efficiency\",\n      TRUE ~ \"Low Efficiency\"\n    )\n  ) %&gt;%\n  # Arrange from most to least efficient\n  arrange(desc(efficiency_ratio))\n\n# Display the top 5 most efficient cars\nhead(mtcars_enhanced[, c(\"car_name\", \"mpg\", \"wt\", \"hp\", \"efficiency_ratio\", \"efficiency_category\")], 5)\n\n        car_name  mpg    wt  hp efficiency_ratio efficiency_category\n1   Lotus Europa 30.4 1.513 113         20.09253     High Efficiency\n2    Honda Civic 30.4 1.615  52         18.82353     High Efficiency\n3 Toyota Corolla 33.9 1.835  65         18.47411     High Efficiency\n4       Fiat 128 32.4 2.200  66         14.72727     High Efficiency\n5      Fiat X1-9 27.3 1.935  66         14.10853     High Efficiency"
  },
  {
    "objectID": "posts/intro-to-r-analytics.html#handling-missing-data",
    "href": "posts/intro-to-r-analytics.html#handling-missing-data",
    "title": "Introduction to R for Analytics",
    "section": "Handling Missing Data",
    "text": "Handling Missing Data\nMissing data is a common challenge. This practical example demonstrates handling techniques:\n\n# Create a simulated customer dataset with missing values\nset.seed(123) # For reproducibility\n\ncustomers &lt;- data.frame(\n  customer_id = 1:100,\n  age = sample(18:70, 100, replace = TRUE),\n  income = round(rnorm(100, 50000, 15000)),\n  years_as_customer = sample(0:20, 100, replace = TRUE),\n  purchase_frequency = sample(1:10, 100, replace = TRUE)\n)\n\n# Introduce missing values randomly\nset.seed(456)\ncustomers$age[sample(1:100, 10)] &lt;- NA\ncustomers$income[sample(1:100, 15)] &lt;- NA\ncustomers$purchase_frequency[sample(1:100, 5)] &lt;- NA\n\n# 1. Identify missing data\nmissing_summary &lt;- sapply(customers, function(x) sum(is.na(x)))\nmissing_summary\n\n       customer_id                age             income  years_as_customer \n                 0                 10                 15                  0 \npurchase_frequency \n                 5 \n\n# 2. Visualize the pattern of missing data\nlibrary(naniar) # May need to install this package\nvis_miss(customers)\n\n\n\n\n\n\n\n# 3. Handle missing data with multiple approaches\n\n# Option A: Remove rows with any missing values\nclean_customers &lt;- na.omit(customers)\nnrow(customers) - nrow(clean_customers) # Number of rows removed\n\n[1] 26\n\n# Option B: Impute with mean/median (numeric variables only)\nimputed_customers &lt;- customers %&gt;%\n  mutate(\n    age = ifelse(is.na(age), median(age, na.rm = TRUE), age),\n    income = ifelse(is.na(income), mean(income, na.rm = TRUE), income),\n    purchase_frequency = ifelse(is.na(purchase_frequency), \n                               median(purchase_frequency, na.rm = TRUE), \n                               purchase_frequency)\n  )\n\n# Option C: Predictive imputation (using age to predict income)\nlibrary(mice) # For more sophisticated imputation\n# Quick imputation model - in practice more parameters would be used\nimputed_data &lt;- mice(customers, m = 5, method = \"pmm\", printFlag = FALSE)\ncustomers_complete &lt;- complete(imputed_data)\n\n# Compare results by calculating customer value score\ncalculate_value &lt;- function(df) {\n  df %&gt;%\n    mutate(customer_value = (income/10000) * (purchase_frequency/10) * log(years_as_customer + 1)) %&gt;%\n    arrange(desc(customer_value)) %&gt;%\n    select(customer_id, customer_value, everything())\n}\n\n# Top 5 customers by value (original with NAs removed)\nhead(calculate_value(clean_customers), 5)\n\n  customer_id customer_value age income years_as_customer purchase_frequency\n1           7       24.63960  67  82249                19                 10\n2          54       15.73965  22  70961                15                  8\n3          59       15.67045  50  70649                15                  8\n4          84       15.09251  21  55732                14                 10\n5          72       14.27848  23  61853                12                  9\n\n# Top 5 customers by value (with imputed values)\nhead(calculate_value(customers_complete), 5)\n\n  customer_id customer_value age income years_as_customer purchase_frequency\n1           7       24.63960  67  82249                19                 10\n2          54       15.73965  22  70961                15                  8\n3          59       15.67045  50  70649                15                  8\n4          84       15.09251  21  55732                14                 10\n5          72       14.27848  23  61853                12                  9"
  },
  {
    "objectID": "get-started/reading-data.html#reading-csv-files",
    "href": "get-started/reading-data.html#reading-csv-files",
    "title": "Reading Data into R",
    "section": "Reading CSV Files",
    "text": "Reading CSV Files\nComma-separated values (CSV) files are one of the most common data formats:\n\n# Create a sample CSV file\nwrite.csv(mtcars[1:5, ], \"sample_cars.csv\", row.names = TRUE)\n\n# Read the CSV file\ncars_data &lt;- read.csv(\"sample_cars.csv\")\nhead(cars_data)\n\n                  X  mpg cyl disp  hp drat    wt  qsec vs am gear carb\n1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n# Read with specific options\ncars_data2 &lt;- read.csv(\"sample_cars.csv\", \n                      header = TRUE,       # First row contains column names\n                      sep = \",\",           # Separator is a comma\n                      stringsAsFactors = FALSE, # Don't convert strings to factors\n                      na.strings = c(\"NA\", \"N/A\", \"\")) # Values to treat as NA\nhead(cars_data2)\n\n                  X  mpg cyl disp  hp drat    wt  qsec vs am gear carb\n1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2"
  },
  {
    "objectID": "get-started/reading-data.html#reading-fixed-width-files",
    "href": "get-started/reading-data.html#reading-fixed-width-files",
    "title": "Reading Data into R",
    "section": "Reading Fixed-Width Files",
    "text": "Reading Fixed-Width Files\nFixed-width files have fields of consistent width:\n\n# Create a sample fixed-width file\ncat(\"John  Smith 35\\nMary  Jones 28\\nDavid Brown 42\\n\", file = \"sample_people.txt\")\n\n# Read the fixed-width file\npeople_data &lt;- read.fwf(\"sample_people.txt\", \n                       widths = c(5, 6, 3),  # Width of each column\n                       col.names = c(\"First\", \"Last\", \"Age\"))\npeople_data\n\n  First   Last Age\n1 John   Smith  35\n2 Mary   Jones  28\n3 David  Brown  42"
  },
  {
    "objectID": "get-started/reading-data.html#reading-from-r-data-files",
    "href": "get-started/reading-data.html#reading-from-r-data-files",
    "title": "Reading Data into R",
    "section": "Reading from R Data Files",
    "text": "Reading from R Data Files\nR has its own binary file format for saving and loading R objects:\n\n# Save R objects to a file\nsample_data &lt;- list(x = 1:10, y = letters[1:10])\nsave(sample_data, file = \"sample_data.RData\")\n\n# Load the saved objects\nload(\"sample_data.RData\")\nsample_data\n\n$x\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$y\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\n\n# Save a single object\nsaveRDS(mtcars[1:5, ], \"sample_cars.rds\")\n\n# Read the saved object\ncars_subset &lt;- readRDS(\"sample_cars.rds\")\nhead(cars_subset)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2"
  },
  {
    "objectID": "get-started/reading-data.html#reading-from-urls",
    "href": "get-started/reading-data.html#reading-from-urls",
    "title": "Reading Data into R",
    "section": "Reading from URLs",
    "text": "Reading from URLs\nYou can read data directly from the web:\n\n# Read CSV from a URL (example with a small dataset)\nurl &lt;- \"https://raw.githubusercontent.com/datasets/iris/master/data/iris.csv\"\niris_data &lt;- try(read.csv(url), silent = TRUE)\n\n# Check if the data was loaded successfully\nif (!inherits(iris_data, \"try-error\")) {\n  head(iris_data)\n} else {\n  print(\"Could not access the URL. Check your internet connection.\")\n}\n\n[1] \"Could not access the URL. Check your internet connection.\""
  },
  {
    "objectID": "get-started/reading-data.html#reading-excel-files",
    "href": "get-started/reading-data.html#reading-excel-files",
    "title": "Reading Data into R",
    "section": "Reading Excel Files",
    "text": "Reading Excel Files\nWhile not part of base R, the readxl package is commonly used:\n\n# Check if readxl is installed\nif (!requireNamespace(\"readxl\", quietly = TRUE)) {\n  message(\"The readxl package is not installed. You can install it with: install.packages('readxl')\")\n} else {\n  library(readxl)\n  # This would read an Excel file if it existed\n  # excel_data &lt;- read_excel(\"sample.xlsx\", sheet = 1)\n}"
  },
  {
    "objectID": "get-started/reading-data.html#reading-from-databases",
    "href": "get-started/reading-data.html#reading-from-databases",
    "title": "Reading Data into R",
    "section": "Reading from Databases",
    "text": "Reading from Databases\nBase R provides the DBI package for database connections:\n\n# Example of connecting to SQLite (not run)\n# if (!requireNamespace(\"RSQLite\", quietly = TRUE)) {\n#   message(\"The RSQLite package is not installed\")\n# } else {\n#   library(DBI)\n#   con &lt;- dbConnect(RSQLite::SQLite(), \":memory:\")\n#   dbWriteTable(con, \"mtcars\", mtcars)\n#   data &lt;- dbGetQuery(con, \"SELECT * FROM mtcars WHERE cyl = 4\")\n#   dbDisconnect(con)\n# }"
  },
  {
    "objectID": "get-started/reading-data.html#handling-file-paths",
    "href": "get-started/reading-data.html#handling-file-paths",
    "title": "Reading Data into R",
    "section": "Handling File Paths",
    "text": "Handling File Paths\nR provides functions to work with file paths:\n\n# Get current working directory\ngetwd()\n\n[1] \"C:/Users/riddh/OneDrive/Desktop/rtichoke-github/get-started\"\n\n# List files in the current directory\nlist.files(pattern = \".csv\")\n\n[1] \"sample_cars.csv\"\n\n# Check if a file exists\nfile.exists(\"sample_cars.csv\")\n\n[1] TRUE\n\n# Get full path to a file\nnormalizePath(\"sample_cars.csv\", mustWork = FALSE)\n\n[1] \"C:\\\\Users\\\\riddh\\\\OneDrive\\\\Desktop\\\\rtichoke-github\\\\get-started\\\\sample_cars.csv\""
  },
  {
    "objectID": "get-started/reading-data.html#cleaning-up",
    "href": "get-started/reading-data.html#cleaning-up",
    "title": "Reading Data into R",
    "section": "Cleaning Up",
    "text": "Cleaning Up\nLet’s remove the sample files we created:\n\n# List of files to remove\nfiles_to_remove &lt;- c(\"sample_cars.csv\", \"sample_cars.txt\", \n                    \"sample_people.txt\", \"sample_data.RData\", \n                    \"sample_cars.rds\")\n\n# Remove files\nfor (file in files_to_remove) {\n  if (file.exists(file)) {\n    file.remove(file)\n  }\n}\n\nRemember to check the documentation with ?read.csv or similar commands to explore all available options for these functions."
  },
  {
    "objectID": "get-started/reading-data.html#reading-tab-delimited-files",
    "href": "get-started/reading-data.html#reading-tab-delimited-files",
    "title": "Reading Data into R",
    "section": "Reading Tab-Delimited Files",
    "text": "Reading Tab-Delimited Files\nTab-delimited files are another common format:\n\n# Create a sample tab-delimited file\nwrite.table(mtcars[1:5, ], \"sample_cars.txt\", sep = \"\\t\", row.names = TRUE)\n\n# Read the tab-delimited file\ncars_data_tab &lt;- read.delim(\"sample_cars.txt\")\nhead(cars_data_tab)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n# Or use read.table with tab separator\ncars_data_tab2 &lt;- read.table(\"sample_cars.txt\", \n                            header = TRUE, \n                            sep = \"\\t\")\nhead(cars_data_tab2)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2"
  },
  {
    "objectID": "webr.html",
    "href": "webr.html",
    "title": "Interactive R in Your Browser with WebR",
    "section": "",
    "text": "The WebR Playground allows users to write and execute R code directly in their browser. This feature is ideal for beginners wanting to experiment with R or for quick code testing when away from the main development environment."
  },
  {
    "objectID": "webr.html#run-r-code-in-the-browser---no-installation-required",
    "href": "webr.html#run-r-code-in-the-browser---no-installation-required",
    "title": "Interactive R in Your Browser with WebR",
    "section": "",
    "text": "The WebR Playground allows users to write and execute R code directly in their browser. This feature is ideal for beginners wanting to experiment with R or for quick code testing when away from the main development environment."
  },
  {
    "objectID": "webr.html#what-is-webr",
    "href": "webr.html#what-is-webr",
    "title": "Interactive R in Your Browser with WebR",
    "section": "What is WebR?",
    "text": "What is WebR?\nWebR is a version of R compiled to WebAssembly, enabling R code execution directly in web browsers. This provides:\n\nNo installation required\nInstant access to R functionality\nCode execution on any device with a modern browser"
  },
  {
    "objectID": "webr.html#using-the-webr-playground",
    "href": "webr.html#using-the-webr-playground",
    "title": "Interactive R in Your Browser with WebR",
    "section": "Using the WebR Playground",
    "text": "Using the WebR Playground\n\n\n\n\n\n\nCurrent Limitations\n\n\n\nPlease note that this basic implementation has some limitations:\n\nNo plotting capability is available\nPackage installation is not supported\nOnly basic R functionality is available\n\nFor full R functionality, RStudio or other complete R environments are recommended."
  },
  {
    "objectID": "webr.html#example-code-for-practice",
    "href": "webr.html#example-code-for-practice",
    "title": "Interactive R in Your Browser with WebR",
    "section": "Example Code for Practice",
    "text": "Example Code for Practice\nThe following simple R code examples can be tested in the WebR Playground:\n\nBasic Calculations\n# Basic arithmetic\n2 + 2 * 5\nsqrt(16)\nlog(10)\n\n# Create and manipulate vectors\nx &lt;- c(1, 2, 3, 4, 5)\nmean(x)\nsd(x)\nsum(x)\n\n\nData Analysis\n# Create some sample data\nx &lt;- 1:10\ny &lt;- c(2, 4, 6, 8, 7, 12, 14, 16, 18, 20)\n\n# Print the data\ncat(\"x values:\", x, \"\\n\")\ncat(\"y values:\", y, \"\\n\")\n\n# Calculate some statistics\nmean_x &lt;- mean(x)\nmean_y &lt;- mean(y)\ncat(\"Mean of x:\", mean_x, \"\\n\")\ncat(\"Mean of y:\", mean_y, \"\\n\")\n\n# Fit a linear model\nmodel &lt;- lm(y ~ x)\nsummary(model)\n\n\nWorking with Built-in Datasets\n# Explore the built-in mtcars dataset\ndata(mtcars)\nhead(mtcars)\nsummary(mtcars)\n\n# Basic statistics\ncor(mtcars$mpg, mtcars$wt)\nt.test(mtcars$mpg[mtcars$am == 0], mtcars$mpg[mtcars$am == 1])"
  },
  {
    "objectID": "webr.html#future-enhancements",
    "href": "webr.html#future-enhancements",
    "title": "Interactive R in Your Browser with WebR",
    "section": "Future Enhancements",
    "text": "Future Enhancements\n\nSupport for basic plotting\nAccess to more packages\nAbility to save and share code snippets\n\nAccess the WebR Playground →"
  },
  {
    "objectID": "posts/assessing-score-reliability.html#introduction",
    "href": "posts/assessing-score-reliability.html#introduction",
    "title": "Tutorial: Assessing Credit Score Prediction Reliability Using Bootstrap Resampling",
    "section": "",
    "text": "Credit scoring models demonstrate optimal performance within the central regions of the score distribution, yet exhibit diminished reliability at the distribution extremes where data becomes sparse. This tutorial provides a comprehensive methodology for employing bootstrap resampling techniques to quantify prediction variability across different score ranges, thereby enabling practitioners to identify regions where their models demonstrate the highest degree of dependability."
  },
  {
    "objectID": "posts/assessing-score-reliability.html#theoretical-foundation-understanding-estimation-variance",
    "href": "posts/assessing-score-reliability.html#theoretical-foundation-understanding-estimation-variance",
    "title": "Tutorial: Assessing Credit Score Prediction Reliability Using Bootstrap Resampling",
    "section": "Theoretical Foundation: Understanding Estimation Variance",
    "text": "Theoretical Foundation: Understanding Estimation Variance\nReduced sample sizes inherently result in increased variance in statistical estimates, particularly for extreme values within a distribution. While central tendency measures such as means demonstrate relative stability when estimated from limited data, tail percentiles (95th, 99th) exhibit substantially greater volatility. This phenomenon is of critical importance in credit scoring applications, where extremely high and low scores correspond to these unstable tail regions of the distribution.\n\n# Number of samples to be drawn from a probability distribution\nn_samples &lt;- 1000\n\n# Number of times, sampling should be repeated\nrepeats &lt;- 100\n\n# Mean and std-dev for a standard normal distribution\nmu &lt;- 5\nstd_dev &lt;- 2\n\n# Sample\nsamples &lt;- rnorm(n_samples * repeats, mean = 10)\n\n# Fit into a matrix like object with `n_samples' number of rows \n# and `repeats' number of columns\nsamples &lt;- matrix(samples, nrow = n_samples, ncol = repeats)\n\n# Compute mean across each column\nsample_means &lt;- apply(samples, 1, mean)\n\n# Similarly, compute 75% and 95% quantile across each column\nsample_75_quantile &lt;- apply(samples, 1, quantile, p = 0.75)\nsample_95_quantile &lt;- apply(samples, 1, quantile, p = 0.95)\nsample_99_quantile &lt;- apply(samples, 1, quantile, p = 0.99)\n\n# Compare coefficient of variation\nsd(sample_means)/mean(sample_means)\n\n[1] 0.01043118\n\nsd(sample_75_quantile)/mean(sample_75_quantile)\n\n[1] 0.0126438\n\nsd(sample_95_quantile)/mean(sample_75_quantile)\n\n[1] 0.01943785\n\n# Plot the distributions\ncombined_vec &lt;- c(sample_means, sample_75_quantile, sample_95_quantile, sample_99_quantile)\n\nplot(density(sample_means), \n     col = \"#6F69AC\", \n     lwd = 3, \n     main = \"Estimating the mean vs tail quantiles\", \n     xlab = \"\", \n     xlim = c(min(combined_vec), max(combined_vec)))\n\nlines(density(sample_75_quantile), col = \"#95DAC1\", lwd = 3)\nlines(density(sample_95_quantile), col = \"#FFEBA1\", lwd = 3)\nlines(density(sample_99_quantile), col = \"#FD6F96\", lwd = 3)\ngrid()\n\nlegend(\"topright\", \n       fill = c(\"#6F69AC\", \"#95DAC1\", \"#FFEBA1\", \"#FD6F96\"), \n       legend = c(\"Mean\", \"75% Quantile\", \"95% Quantile\", \"99% Quantile\"), \n       cex = 0.7)\n\n\n\n\n\n\n\n\nThe visualization demonstrates the substantial increase in uncertainty when estimating extreme values compared to central tendencies. The distribution corresponding to the mean (purple) exhibits considerably narrower dispersion than that of the 99th percentile (pink). This statistical principle directly applies to credit scoring applications, where scores at the extremes of the distribution inherently possess greater uncertainty.\n\n# Load required packages\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(rsample)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/assessing-score-reliability.html#step-1-data-acquisition-and-preprocessing",
    "href": "posts/assessing-score-reliability.html#step-1-data-acquisition-and-preprocessing",
    "title": "Tutorial: Assessing Credit Score Prediction Reliability Using Bootstrap Resampling",
    "section": "Step 1: Data Acquisition and Preprocessing",
    "text": "Step 1: Data Acquisition and Preprocessing\nIn this tutorial, we will utilize a sample from the Lending Club dataset. Loans classified as “Charged Off” will be designated as defaults. The observed class imbalance represents a typical characteristic of credit portfolios and contributes significantly to prediction challenges at distribution extremes.\n\n# Load sample data (sample of the lending club data)\nsample &lt;- read.csv(\"http://bit.ly/42ypcnJ\")\n\n# Mark which loan status will be tagged as default\ncodes &lt;- c(\"Charged Off\", \"Does not meet the credit policy. Status:Charged Off\")\n\n# Apply above codes and create target\nsample %&lt;&gt;% mutate(bad_flag = ifelse(loan_status %in% codes, 1, 0))\n\n# Replace missing values with a default value\nsample[is.na(sample)] &lt;- -1\n\n# Get summary tally\ntable(sample$bad_flag)\n\n\n   0    1 \n8838 1162"
  },
  {
    "objectID": "posts/assessing-score-reliability.html#step-2-implementing-bootstrap-resampling-strategy",
    "href": "posts/assessing-score-reliability.html#step-2-implementing-bootstrap-resampling-strategy",
    "title": "Tutorial: Assessing Credit Score Prediction Reliability Using Bootstrap Resampling",
    "section": "Step 2: Implementing Bootstrap Resampling Strategy",
    "text": "Step 2: Implementing Bootstrap Resampling Strategy\nThis section demonstrates the creation of 100 bootstrap samples to quantify the variation in model predictions across different score ranges. Bootstrap resampling is a statistical technique that generates multiple simulated datasets to measure prediction uncertainty without requiring additional data collection.\n\n# Create 100 samples\nboot_sample &lt;- bootstraps(data = sample, times = 100)\n\nhead(boot_sample, 3)\n\n# A tibble: 3 × 2\n  splits               id          \n  &lt;list&gt;               &lt;chr&gt;       \n1 &lt;split [10000/3674]&gt; Bootstrap001\n2 &lt;split [10000/3745]&gt; Bootstrap002\n3 &lt;split [10000/3629]&gt; Bootstrap003\n\n# Each row represents a separate bootstrapped sample with an analysis set and assessment set\nboot_sample$splits[[1]]\n\n&lt;Analysis/Assess/Total&gt;\n&lt;10000/3674/10000&gt;\n\n# Show the first 5 rows and 5 columns of the first sample\nanalysis(boot_sample$splits[[1]]) %&gt;% .[1:5, 1:5]\n\n     V1       id member_id loan_amnt funded_amnt\n1 66896  5946209        -1     21000       21000\n2 14414 91975421        -1     10000       10000\n3 55223 60803515        -1     30000       30000\n4 28848 80888299        -1      8000        8000\n5 69499 91286249        -1     14400       14400\n\n\nEach bootstrap sample consists of random draws with replacement from the original dataset, creating controlled variations that effectively reveal model sensitivity to different data compositions."
  },
  {
    "objectID": "posts/assessing-score-reliability.html#step-3-developing-the-predictive-model-framework",
    "href": "posts/assessing-score-reliability.html#step-3-developing-the-predictive-model-framework",
    "title": "Tutorial: Assessing Credit Score Prediction Reliability Using Bootstrap Resampling",
    "section": "Step 3: Developing the Predictive Model Framework",
    "text": "Step 3: Developing the Predictive Model Framework\nThis tutorial employs logistic regression as the predictive modeling technique. Logistic regression represents the industry standard for credit risk modeling due to its interpretability and regulatory acceptance. The model specification incorporates typical credit variables including loan amount, income, and credit history metrics.\n\nglm_model &lt;- function(df){\n  \n  # Fit a simple model with a set specification\n  mdl &lt;- glm(bad_flag ~\n               loan_amnt + funded_amnt + annual_inc + delinq_2yrs +\n               inq_last_6mths + mths_since_last_delinq + fico_range_low +\n               mths_since_last_record + revol_util + total_pymnt,\n             family = \"binomial\",\n             data = df)\n  \n  # Return fitted values\n  return(predict(mdl))\n}\n\n# Test the function\n# Retrieve a data frame\ntrain &lt;- analysis(boot_sample$splits[[1]])\n\n# Predict\npred &lt;- glm_model(train)\n\n# Check output\nrange(pred)  # Output is on log odds scale\n\n[1] -28.542188   1.459948\n\n\nThe function returns predictions in log-odds format, which will subsequently be transformed to a more intuitive credit score scale in later steps."
  },
  {
    "objectID": "posts/assessing-score-reliability.html#step-4-iterative-model-training-and-prediction-collection",
    "href": "posts/assessing-score-reliability.html#step-4-iterative-model-training-and-prediction-collection",
    "title": "Tutorial: Assessing Credit Score Prediction Reliability Using Bootstrap Resampling",
    "section": "Step 4: Iterative Model Training and Prediction Collection",
    "text": "Step 4: Iterative Model Training and Prediction Collection\n\n# First apply the glm fitting function to each of the sample\n# Note the use of lapply\noutput &lt;- lapply(boot_sample$splits, function(x){\n  train &lt;- analysis(x)\n  pred &lt;- glm_model(train)\n\n  return(pred)\n})\n\n# Collate all predictions into a vector \nboot_preds &lt;- do.call(c, output)\nrange(boot_preds)\n\n[1] -142.761039    8.111023\n\n# Get outliers\nq_high &lt;- quantile(boot_preds, 0.99)\nq_low &lt;- quantile(boot_preds, 0.01)\n\n# Truncate the overall distribution to within the lower 1% and upper 1% quantiles\n# Doing this since it creates issues later on when scaling the output\nboot_preds[boot_preds &gt; q_high] &lt;- q_high\nboot_preds[boot_preds &lt; q_low] &lt;- q_low\n\nrange(boot_preds)\n\n[1] -5.059623 -0.219829\n\n# Convert to a data frame\nboot_preds &lt;- data.frame(pred = boot_preds, \n                         id = rep(1:length(boot_sample$splits), each = nrow(sample)))\nhead(boot_preds)\n\n       pred id\n1 -2.910835  1\n2 -2.320412  1\n3 -4.130248  1\n4 -1.437957  1\n5 -1.553018  1\n6 -2.359667  1\n\n\nIn this step, we apply the logistic regression model to each bootstrap sample and systematically collect the resulting predictions. Subsequently, we truncate extreme values (beyond the 1st and 99th percentiles) to remove outliers—a procedure analogous to capping techniques commonly employed in production credit models."
  },
  {
    "objectID": "posts/assessing-score-reliability.html#step-5-transforming-predictions-to-credit-score-scale",
    "href": "posts/assessing-score-reliability.html#step-5-transforming-predictions-to-credit-score-scale",
    "title": "Tutorial: Assessing Credit Score Prediction Reliability Using Bootstrap Resampling",
    "section": "Step 5: Transforming Predictions to Credit Score Scale",
    "text": "Step 5: Transforming Predictions to Credit Score Scale\nThis section demonstrates the conversion of log-odds predictions to a recognizable credit score format using the industry-standard Points to Double Odds (PDO) methodology. By employing parameters consistent with real-world credit systems (PDO=30, Anchor=700), we transform the model predictions into intuitive scores where higher numerical values indicate lower credit risk.\n\nscaling_func &lt;- function(vec, PDO = 30, OddsAtAnchor = 5, Anchor = 700){\n  beta &lt;- PDO / log(2)\n  alpha &lt;- Anchor - PDO * OddsAtAnchor\n  \n  # Simple linear scaling of the log odds\n  scr &lt;- alpha - beta * vec  \n  \n  # Round off\n  return(round(scr, 0))\n}\n\nboot_preds$scores &lt;- scaling_func(boot_preds$pred, 30, 2, 700)\n\n# Chart the distribution of predictions across all the samples\nggplot(boot_preds, aes(x = scores, color = factor(id))) + \n  geom_density() + \n  theme_minimal() + \n  theme(legend.position = \"none\") + \n  scale_color_grey() + \n  labs(title = \"Predictions from bootstrapped samples\", \n       subtitle = \"Density function\", \n       x = \"Predictions (Log odds)\", \n       y = \"Density\")\n\n\n\n\n\n\n\n\nEach gray line in the visualization represents the score distribution from a distinct bootstrap sample. Regions where lines demonstrate tight clustering indicate stable predictions, while areas exhibiting divergent patterns reflect higher uncertainty levels."
  },
  {
    "objectID": "posts/assessing-score-reliability.html#step-6-quantifying-prediction-uncertainty-across-score-ranges",
    "href": "posts/assessing-score-reliability.html#step-6-quantifying-prediction-uncertainty-across-score-ranges",
    "title": "Tutorial: Assessing Credit Score Prediction Reliability Using Bootstrap Resampling",
    "section": "Step 6: Quantifying Prediction Uncertainty Across Score Ranges",
    "text": "Step 6: Quantifying Prediction Uncertainty Across Score Ranges\nThis section provides a methodology to directly measure prediction reliability variation across different score ranges through the calculation of standard deviation within each score bin. This approach enables precise quantification of uncertainty at different score levels.\n\n# Create bins using quantiles\nbreaks &lt;- quantile(boot_preds$scores, probs = seq(0, 1, length.out = 20))\nboot_preds$bins &lt;- cut(boot_preds$scores, breaks = unique(breaks), include.lowest = T, right = T)\n\n# Chart standard deviation of model predictions across each score bin\nboot_preds %&gt;%\n  group_by(bins) %&gt;%\n  summarise(std_dev = sd(scores)) %&gt;%\n  ggplot(aes(x = bins, y = std_dev)) +\n  geom_col(color = \"black\", fill = \"#90AACB\") +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 90)) + \n  theme(legend.position = \"none\") + \n  labs(title = \"Variability in model predictions across samples\", \n       subtitle = \"(measured using standard deviation)\", \n       x = \"Score Range\", \n       y = \"Standard Deviation\")\n\n\n\n\n\n\n\n\nAs anticipated, the model’s predictions demonstrate enhanced reliability within a specific range of values (700-800), while exhibiting significant variability in the lowest and highest score buckets.\nThe visualization reveals a characteristic “U-shaped” pattern of prediction variability—a well-documented phenomenon in credit risk modeling. The highest uncertainty manifests in the extreme score ranges (very high and very low scores), while predictions in the middle range demonstrate greater stability. The analysis confirms the initial hypothesis: variability reaches its maximum at score extremes and achieves its minimum in the middle range (600-800). This finding provides direct guidance for credit policy development—scores in the middle range demonstrate the highest reliability, while decisions at the extremes should incorporate additional caution due to elevated uncertainty.\n\nPractical Business Applications\nThese findings yield direct business applications:\n\nHigh Score Management: For extremely high scores, implement additional verification steps before automated approval\nLow Score Management: For very low scores, consider manual review procedures rather than automatic rejection"
  },
  {
    "objectID": "posts/assessing-score-reliability.html#advanced-methodology-isolating-training-data-effects",
    "href": "posts/assessing-score-reliability.html#advanced-methodology-isolating-training-data-effects",
    "title": "Tutorial: Assessing Credit Score Prediction Reliability Using Bootstrap Resampling",
    "section": "Advanced Methodology: Isolating Training Data Effects",
    "text": "Advanced Methodology: Isolating Training Data Effects\nCredit: Richard Warnung\nFor enhanced analytical control, this section presents an alternative approach where models are trained on bootstrap samples while being evaluated on a consistent validation set. This methodology effectively isolates the impact of training data variation on model predictions.\n\nVs &lt;- function(boot_split){\n  # Train model on the bootstrapped data\n  train &lt;- analysis(boot_split)\n  \n  # Fit model\n  mdl &lt;- glm(bad_flag ~\n               loan_amnt + funded_amnt + annual_inc + delinq_2yrs +\n               inq_last_6mths + mths_since_last_delinq + fico_range_low +\n               mths_since_last_record + revol_util + total_pymnt,\n             family = \"binomial\",\n             data = train)\n  \n  # Apply to a common validation set\n  validate_preds &lt;- predict(mdl, newdata = validate_set)\n  \n  # Return predictions\n  return(validate_preds)\n}\n\nThis methodology provides enhanced insight into how variations in training data composition affect model predictions, which proves valuable when evaluating model updates in production environments.\n\n# Create overall training and testing datasets \nid &lt;- sample(1:nrow(sample), size = nrow(sample)*0.8, replace = F)\n\ntrain_data &lt;- sample[id,]\ntest_data &lt;- sample[-id,]\n\n# Bootstrapped samples are now pulled only from the overall training dataset\nboot_sample &lt;- bootstraps(data = train_data, times = 80)\n\n# Using the same function from before but predicting on the same test dataset\nglm_model &lt;- function(train, test){\n  \n  mdl &lt;- glm(bad_flag ~\n               loan_amnt + funded_amnt + annual_inc + delinq_2yrs +\n               inq_last_6mths + mths_since_last_delinq + fico_range_low +\n               mths_since_last_record + revol_util + total_pymnt,\n             family = \"binomial\",\n             data = train)\n  \n  # Return fitted values on the test dataset\n  return(predict(mdl, newdata = test))\n}\n\n# Apply the glm fitting function to each of the sample\n# But predict on the same test dataset\noutput &lt;- lapply(boot_sample$splits, function(x){\n  train &lt;- analysis(x)\n  pred &lt;- glm_model(train, test_data)\n\n  return(pred)\n})"
  },
  {
    "objectID": "posts/assessing-score-reliability.html#conclusion",
    "href": "posts/assessing-score-reliability.html#conclusion",
    "title": "Tutorial: Assessing Credit Score Prediction Reliability Using Bootstrap Resampling",
    "section": "Conclusion",
    "text": "Conclusion\nThis tutorial has demonstrated a comprehensive methodology for assessing credit score prediction reliability using bootstrap resampling techniques. The key findings include:\n\nPrediction uncertainty follows a U-shaped pattern, with highest variability at score extremes and greatest stability in the middle ranges\nBootstrap resampling provides a robust framework for quantifying model uncertainty without requiring additional data collection\nThe Points to Double Odds (PDO) transformation enables conversion of log-odds predictions to interpretable credit scores\nPractical business applications can be directly derived from uncertainty measurements to inform credit policy decisions\n\n\nKey Takeaways for Practitioners\n\nImplement additional verification procedures for extreme scores (both high and low)\nFocus model validation efforts on middle score ranges where predictions are most reliable\nUse bootstrap resampling as a standard practice for uncertainty quantification in credit risk models\nConsider advanced methodologies that isolate training data effects for production model evaluation\n\nThis methodology can be extended to other domains where prediction reliability assessment is critical, including fraud detection, insurance underwriting, and customer segmentation models."
  },
  {
    "objectID": "posts/assessing-score-reliability.html#key-takeaways",
    "href": "posts/assessing-score-reliability.html#key-takeaways",
    "title": "Tutorial: Assessing Credit Score Prediction Reliability Using Bootstrap Resampling",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nPrediction reliability varies significantly across score ranges, with highest uncertainty at distribution extremes\nBootstrap methodology provides a practical framework for measuring model uncertainty without additional data requirements\n\nRisk management strategies should be adjusted based on score reliability, with enhanced verification for extreme scores\nThe PDO transformation enables intuitive score interpretation while preserving the underlying risk relationships\n\nThese techniques can be applied beyond credit scoring to any prediction problem where understanding model uncertainty is critical for decision-making."
  },
  {
    "objectID": "posts/bayesian-optimization-xgboost.html#introduction",
    "href": "posts/bayesian-optimization-xgboost.html#introduction",
    "title": "Optimizing XGBoost Hyperparameters Using Bayesian Optimization in R",
    "section": "",
    "text": "Hyperparameter tuning for machine learning models represents a computationally intensive and time-consuming process. This tutorial demonstrates the implementation of Bayesian optimization techniques to efficiently identify optimal XGBoost hyperparameters, thereby reducing computational overhead while enhancing model performance. The methodology presented provides a systematic approach to hyperparameter optimization that significantly outperforms traditional grid search methods."
  },
  {
    "objectID": "posts/bayesian-optimization-xgboost.html#package-dependencies",
    "href": "posts/bayesian-optimization-xgboost.html#package-dependencies",
    "title": "Optimizing XGBoost Hyperparameters Using Bayesian Optimization in R",
    "section": "Package Dependencies",
    "text": "Package Dependencies\nThis tutorial requires several specialized R packages for machine learning, optimization, and data preprocessing. The following packages must be installed and loaded before proceeding with the implementation.\n\n# Load required packages\nlibrary(xgboost)\nlibrary(ParBayesianOptimization)\nlibrary(mlbench)\nlibrary(dplyr)\nlibrary(recipes)\nlibrary(rsample)"
  },
  {
    "objectID": "posts/bayesian-optimization-xgboost.html#data-acquisition-and-initial-processing",
    "href": "posts/bayesian-optimization-xgboost.html#data-acquisition-and-initial-processing",
    "title": "Optimizing XGBoost Hyperparameters Using Bayesian Optimization in R",
    "section": "Data Acquisition and Initial Processing",
    "text": "Data Acquisition and Initial Processing\nThis tutorial utilizes the Boston Housing dataset, a canonical regression problem that incorporates both numerical and categorical variables. This dataset provides an appropriate foundation for demonstrating Bayesian optimization techniques in a supervised learning context.\n\n# Load the Boston Housing dataset\ndata(\"BostonHousing2\")\n\n# Quick look at the data structure\nstr(BostonHousing2)\n\n'data.frame':   506 obs. of  19 variables:\n $ town   : Factor w/ 92 levels \"Arlington\",\"Ashland\",..: 54 77 77 46 46 46 69 69 69 69 ...\n $ tract  : int  2011 2021 2022 2031 2032 2033 2041 2042 2043 2044 ...\n $ lon    : num  -71 -71 -70.9 -70.9 -70.9 ...\n $ lat    : num  42.3 42.3 42.3 42.3 42.3 ...\n $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n $ cmedv  : num  24 21.6 34.7 33.4 36.2 28.7 22.9 22.1 16.5 18.9 ...\n $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n $ chas   : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n $ rm     : num  6.58 6.42 7.18 7 7.15 ...\n $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...\n $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...\n $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...\n $ tax    : int  296 242 242 222 222 222 311 311 311 311 ...\n $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n $ b      : num  397 397 393 395 397 ...\n $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...\n\n\nXGBoost algorithms require numerical input features exclusively. Therefore, we employ the recipes package to systematically transform categorical variables through appropriate preprocessing techniques:\n\n# Create a recipe for preprocessing\nrec &lt;- recipe(cmedv ~ ., data = BostonHousing2) %&gt;%\n  # Collapse categories where population is &lt; 3%\n  step_other(town, chas, threshold = .03, other = \"Other\") %&gt;% \n  # Create dummy variables for all factor variables \n  step_dummy(all_nominal_predictors())\n\n# Train the recipe on the dataset\nprep &lt;- prep(rec, training = BostonHousing2)\n\n# Create the final model matrix\nmodel_df &lt;- bake(prep, new_data = BostonHousing2)\n\n# Check the column names after one-hot encoding\ncolnames(model_df)\n\n [1] \"tract\"                  \"lon\"                    \"lat\"                   \n [4] \"medv\"                   \"crim\"                   \"zn\"                    \n [7] \"indus\"                  \"nox\"                    \"rm\"                    \n[10] \"age\"                    \"dis\"                    \"rad\"                   \n[13] \"tax\"                    \"ptratio\"                \"b\"                     \n[16] \"lstat\"                  \"cmedv\"                  \"town_Boston.Savin.Hill\"\n[19] \"town_Cambridge\"         \"town_Lynn\"              \"town_Newton\"           \n[22] \"town_Other\"             \"chas_X1\"               \n\n\nSubsequently, we partition the dataset into training and testing subsets to enable proper model evaluation and prevent data leakage:\n\n# Create a 70/30 train-test split\nsplits &lt;- rsample::initial_split(model_df, prop = 0.7)\ntrain_df &lt;- rsample::training(splits)\ntest_df &lt;- rsample::testing(splits)\n\n# Prepare the training data for XGBoost\nX &lt;- train_df %&gt;%\n  select(!medv, !cmedv) %&gt;%\n  as.matrix()\n\n# Get the target variable\ny &lt;- train_df %&gt;% pull(cmedv)\n\n# Create cross-validation folds\nfolds &lt;- list(\n  fold1 = as.integer(seq(1, nrow(X), by = 5)),\n  fold2 = as.integer(seq(2, nrow(X), by = 5))\n)"
  },
  {
    "objectID": "posts/bayesian-optimization-xgboost.html#bayesian-optimization-framework-implementation",
    "href": "posts/bayesian-optimization-xgboost.html#bayesian-optimization-framework-implementation",
    "title": "Optimizing XGBoost Hyperparameters Using Bayesian Optimization in R",
    "section": "Bayesian Optimization Framework Implementation",
    "text": "Bayesian Optimization Framework Implementation\nThe implementation of Bayesian optimization requires the development of two fundamental components:\n\nObjective Function: A function that evaluates model performance for given hyperparameter configurations\nParameter Space Definition: The bounds and constraints that define the search space for optimization\n\n\n# Our objective function takes hyperparameters as inputs\nobj_func &lt;- function(eta, max_depth, min_child_weight, subsample, lambda, alpha) {\n  \n  param &lt;- list(\n    # Learning parameters\n    eta = eta,                       # Learning rate\n    max_depth = max_depth,           # Tree depth\n    min_child_weight = min_child_weight, # Min observations per node\n    subsample = subsample,           # Data subsampling\n    lambda = lambda,                 # L2 regularization\n    alpha = alpha,                   # L1 regularization\n    \n    booster = \"gbtree\",             # Use tree model\n    objective = \"reg:squarederror\",  # Regression task\n    eval_metric = \"mape\"            # Mean Absolute Percentage Error\n  )\n  \n  xgbcv &lt;- xgb.cv(params = param,\n                  data = X,\n                  label = y,\n                  nround = 50,\n                  folds = folds,\n                  prediction = TRUE,\n                  early_stopping_rounds = 5,\n                  verbose = 0,\n                  maximize = FALSE)\n  \n  lst &lt;- list(\n    # First argument must be named as \"Score\"\n    # Function finds maxima so inverting the output\n    Score = -min(xgbcv$evaluation_log$test_mape_mean),\n    \n    # Get number of trees for the best performing model\n    nrounds = xgbcv$best_iteration\n  )\n  \n  return(lst)\n}\n\n# Define the search space for each parameter\nbounds &lt;- list(\n  eta = c(0.001, 0.2),             # Learning rate range\n  max_depth = c(1L, 10L),           # Tree depth range\n  min_child_weight = c(1, 50),      # Min observations range\n  subsample = c(0.1, 1),            # Subsampling range\n  lambda = c(1, 10),                # L2 regularization range\n  alpha = c(1, 10)                  # L1 regularization range\n)"
  },
  {
    "objectID": "posts/bayesian-optimization-xgboost.html#executing-the-optimization-process",
    "href": "posts/bayesian-optimization-xgboost.html#executing-the-optimization-process",
    "title": "Optimizing XGBoost Hyperparameters Using Bayesian Optimization in R",
    "section": "Executing the Optimization Process",
    "text": "Executing the Optimization Process\nThe optimization procedure employs intelligent search strategies to systematically explore the hyperparameter space, utilizing Gaussian process modeling to predict promising parameter combinations:\n\nset.seed(1234)\nbayes_out &lt;- bayesOpt(\n  FUN = obj_func,                    # Our objective function\n  bounds = bounds,                   # Parameter bounds\n  initPoints = length(bounds) + 2,   # Initial random points\n  iters.n = 10,                      # Number of iterations\n  verbose = 0                        # Suppress output\n)\n\n# View top results\nbayes_out$scoreSummary[1:5, c(3:8, 13)]\n\n          eta max_depth min_child_weight subsample   lambda    alpha      Score\n        &lt;num&gt;     &lt;num&gt;            &lt;num&gt;     &lt;num&gt;    &lt;num&gt;    &lt;num&gt;      &lt;num&gt;\n1: 0.13392137         8         4.913332 0.2105925 4.721124 3.887629 -0.1292920\n2: 0.19400811         2        25.454160 0.9594105 9.329695 3.173695 -0.1790158\n3: 0.16079775         2        14.035652 0.5118349 1.229953 5.093530 -0.1662595\n4: 0.08957707         4        12.534842 0.3844404 4.358837 1.788342 -0.1672395\n5: 0.02876388         4        36.586761 0.8107181 6.137100 6.039125 -0.3320015\n\n# Get the best parameters\nbest_params &lt;- getBestPars(bayes_out)\ndata.frame(best_params)\n\n        eta max_depth min_child_weight subsample lambda    alpha\n1 0.1251447        10                1         1      1 5.905011"
  },
  {
    "objectID": "posts/bayesian-optimization-xgboost.html#final-model-development-and-evaluation",
    "href": "posts/bayesian-optimization-xgboost.html#final-model-development-and-evaluation",
    "title": "Optimizing XGBoost Hyperparameters Using Bayesian Optimization in R",
    "section": "Final Model Development and Evaluation",
    "text": "Final Model Development and Evaluation\nUpon identification of optimal hyperparameter configurations, we proceed to train the final XGBoost model and evaluate its performance on the held-out test dataset.\n\n# Combine best params with base params\nopt_params &lt;- append(\n  list(booster = \"gbtree\", \n       objective = \"reg:squarederror\", \n       eval_metric = \"mae\"), \n  best_params\n)\n\n# Run cross-validation to determine optimal number of rounds\nxgbcv &lt;- xgb.cv(\n  params = opt_params,\n  data = X,\n  label = y,\n  nround = 100,\n  folds = folds,\n  prediction = TRUE,\n  early_stopping_rounds = 5,\n  verbose = 0,\n  maximize = FALSE\n)\n\n# Get optimal number of rounds\nnrounds = xgbcv$best_iteration\n\n# Fit the final XGBoost model\nmdl &lt;- xgboost(\n  data = X, \n  label = y, \n  params = opt_params, \n  maximize = FALSE, \n  early_stopping_rounds = 5, \n  nrounds = nrounds, \n  verbose = 0\n)\n\n# Make predictions on the test set\nactuals &lt;- test_df$cmedv\npredicted &lt;- test_df %&gt;%\n  select_at(mdl$feature_names) %&gt;%\n  as.matrix() %&gt;%\n  predict(mdl, newdata = .)\n\n# Evaluate performance using Mean Absolute Percentage Error (MAPE)\nmape &lt;- mean(abs(actuals - predicted)/actuals)\ncat(\"MAPE on test set:\", mape)\n\nMAPE on test set: 0.006424492"
  },
  {
    "objectID": "posts/bayesian-optimization-xgboost.html#theoretical-advantages-of-bayesian-optimization",
    "href": "posts/bayesian-optimization-xgboost.html#theoretical-advantages-of-bayesian-optimization",
    "title": "Optimizing XGBoost Hyperparameters Using Bayesian Optimization in R",
    "section": "Theoretical Advantages of Bayesian Optimization",
    "text": "Theoretical Advantages of Bayesian Optimization\nBayesian optimization demonstrates significant superiority over conventional hyperparameter tuning methodologies, particularly grid search approaches:\n\nComputational Efficiency: Achieves optimal parameter identification through substantially fewer iterations\nAdaptive Learning: Incorporates knowledge from previous evaluations to concentrate search efforts on promising parameter regions\nScalability: Maintains computational efficiency regardless of hyperparameter dimensionality\nTime Optimization: Completes optimization procedures in significantly reduced timeframes while achieving equivalent or superior performance outcomes\n\n\nPractical Implementation Considerations\nThis methodology becomes increasingly critical as model complexity and hyperparameter spaces expand. For production environments, practitioners should consider increasing the iteration count (iters.n) to ensure comprehensive exploration of the parameter landscape.\nThe ParBayesianOptimization package provides R practitioners with accessible implementation of these sophisticated optimization techniques, enabling the development of superior models with reduced computational requirements."
  },
  {
    "objectID": "posts/bayesian-optimization-xgboost.html#key-takeaways",
    "href": "posts/bayesian-optimization-xgboost.html#key-takeaways",
    "title": "Optimizing XGBoost Hyperparameters Using Bayesian Optimization in R",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nGaussian process modeling enables intelligent parameter space exploration by learning from previous evaluations\nProper data preprocessing is critical for XGBoost implementation, particularly categorical variable encoding\nCross-validation integration ensures robust hyperparameter evaluation and prevents overfitting to specific data partitions\nThe methodology scales effectively to high-dimensional hyperparameter spaces without proportional computational increases\nProduction implementations benefit from increased iteration counts to ensure thorough parameter space exploration\n\nThis technique can extend beyond XGBoost to any machine learning algorithm requiring hyperparameter optimization, providing a foundation for efficient model development across diverse analytical contexts."
  },
  {
    "objectID": "posts/bayesian-optimization-xgboost.html#key-advantages-of-bayesian-optimization",
    "href": "posts/bayesian-optimization-xgboost.html#key-advantages-of-bayesian-optimization",
    "title": "Optimizing XGBoost Hyperparameters Using Bayesian Optimization in R",
    "section": "Key Advantages of Bayesian Optimization",
    "text": "Key Advantages of Bayesian Optimization\nBayesian optimization demonstrates significant superiority over conventional hyperparameter tuning methodologies, particularly grid search approaches:\n\nComputational Efficiency: Achieves optimal parameter identification through substantially fewer iterations\nAdaptive Learning: Incorporates knowledge from previous evaluations to concentrate search efforts on promising parameter regions\nScalability: Maintains computational efficiency regardless of hyperparameter dimensionality\nTime Optimization: Completes optimization procedures in significantly reduced timeframes while achieving equivalent or superior performance outcomes\n\n\nPractical Considerations\nThis methodology becomes increasingly critical as model complexity and hyperparameter spaces expand. For production environments, increasing the iteration count (iters.n) could help ensure comprehensive exploration of the parameter landscape."
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#introduction",
    "href": "posts/building-particle-swarm-optimizer.html#introduction",
    "title": "Implementing Particle Swarm Optimization from Scratch in R",
    "section": "",
    "text": "Nature-inspired optimization algorithms demonstrate remarkable efficiency in solving complex optimization problems. This post provides a implementation of Particle Swarm Optimization (PSO) from fundamental principles in R. The methodology presented enables efficient exploration of complex solution spaces through coordinated swarm intelligence."
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#package-dependencies",
    "href": "posts/building-particle-swarm-optimizer.html#package-dependencies",
    "title": "Implementing Particle Swarm Optimization from Scratch in R",
    "section": "Package Dependencies",
    "text": "Package Dependencies\n\n# Load required packages\nlibrary(dplyr)     # For data manipulation\nlibrary(ggplot2)   # For visualization\nlibrary(gganimate) # For animations\nlibrary(metR)      # For geom_arrow"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#optimization-test-function-ackleys-function",
    "href": "posts/building-particle-swarm-optimizer.html#optimization-test-function-ackleys-function",
    "title": "Implementing Particle Swarm Optimization from Scratch in R",
    "section": "Optimization Test Function: Ackley’s Function",
    "text": "Optimization Test Function: Ackley’s Function\nThis tutorial employs Ackley’s function as the optimization benchmark—a challenging multimodal function characterized by numerous local minima that frequently entrap conventional optimization algorithms:\n\nobj_func &lt;- function(x, y){\n  # Modified Ackley function with global minimum at (1,1)\n  -20 * exp(-0.2 * sqrt(0.5 *((x-1)^2 + (y-1)^2))) - \n    exp(0.5*(cos(2*pi*x) + cos(2*pi*y))) + exp(1) + 20\n}\n\n# Create a visualization grid\nx &lt;- seq(-5, 5, length.out = 50)\ny &lt;- seq(-5, 5, length.out = 50)\ngrid &lt;- expand.grid(x, y, stringsAsFactors = FALSE)\ngrid$z &lt;- obj_func(grid[,1], grid[,2])\n\n# Create a contour plot\ncontour_plot &lt;- ggplot(grid, aes(x = Var1, y = Var2)) +\n  geom_contour_filled(aes(z = z), color = \"black\", alpha = 0.5) +\n  scale_fill_brewer(palette = \"Spectral\") + \n  theme_minimal() + \n  labs(x = \"x\", y = \"y\", title = \"Ackley's Function\")\n\ncontour_plot"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#theoretical-foundation-of-particle-swarm-optimization",
    "href": "posts/building-particle-swarm-optimizer.html#theoretical-foundation-of-particle-swarm-optimization",
    "title": "Implementing Particle Swarm Optimization from Scratch in R",
    "section": "Theoretical Foundation of Particle Swarm Optimization",
    "text": "Theoretical Foundation of Particle Swarm Optimization\nPSO emulates the collective foraging behavior of biological swarms through the integration of individual memory with social information sharing:\n\nInitialization: Random distribution of particles across the search space\nMemory Mechanism: Each particle maintains a record of its personal best position\nInformation Sharing: The swarm collectively maintains knowledge of the global best position\nMovement Dynamics: Particles adjust their trajectories based on both personal experience and collective knowledge\n\nThe velocity update equation represents a balance of three fundamental forces:\n\\[v_{new} = w \\cdot v_{current} + c_1 \\cdot r_1 \\cdot (p_{best} - p_{current}) + c_2 \\cdot r_2 \\cdot (g_{best} - p_{current})\\]\nWhere:\n\nw: Inertia (momentum preservation)\nc1: Personal influence (individual memory)\nc2: Social influence (collective cooperation)\nr1,r2: Stochastic components"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#systematic-pso-implementation",
    "href": "posts/building-particle-swarm-optimizer.html#systematic-pso-implementation",
    "title": "Implementing Particle Swarm Optimization from Scratch in R",
    "section": "Systematic PSO Implementation",
    "text": "Systematic PSO Implementation\n\nSwarm Initialization\nThe initial phase involves creating a randomized swarm of particles and distributing them across the defined search space:\n\n# Set parameters\nn_particles &lt;- 20\nw &lt;- 0.5     # Inertia weight\nc1 &lt;- 0.05   # Personal learning rate\nc2 &lt;- 0.1    # Social learning rate\n\n# Create random particle positions\nx_range &lt;- seq(-5, 5, length.out = 20)\ny_range &lt;- seq(-5, 5, length.out = 20)\nX &lt;- data.frame(\n  x = sample(x_range, n_particles, replace = FALSE),\n  y = sample(y_range, n_particles, replace = FALSE)\n)\n\n# Visualize initial positions\ncontour_plot + \n  geom_point(data = X, aes(x, y), color = \"red\", size = 2.5) + \n  labs(title = \"Initial Particle Positions\")\n\n\n\n\n\n\n\n\n\n\nBest Position Tracking and Velocity Initialization\nSubsequently, we establish tracking mechanisms for each particle’s personal best position and the swarm’s global best position, while initializing velocity vectors:\n\n# Initialize random velocities\ndX &lt;- matrix(runif(n_particles * 2), ncol = 2) * w\n\n# Set initial personal best positions\npbest &lt;- X\npbest_obj &lt;- obj_func(X[,1], X[,2])\n\n# Find global best position\ngbest &lt;- pbest[which.min(pbest_obj),]\ngbest_obj &lt;- min(pbest_obj)\n\n# Visualize with arrows showing pull toward global best\nX_dir &lt;- X %&gt;% \n  mutate(g_x = gbest[1,1], \n         g_y = gbest[1,2], \n         angle = atan((g_y - y)/(g_x - x))*180/pi,\n         angle = ifelse(g_x &lt; x, 180 + angle, angle))\n\ncontour_plot + \n  geom_point(data = X, aes(x, y), color = \"red\", size = 2.5) + \n  geom_segment(data = X_dir, \n               aes(x = x, y = y, \n                   xend = x + 0.5*cos(angle*pi/180), \n                   yend = y + 0.5*sin(angle*pi/180)), \n               arrow = arrow(length = unit(0.1, \"cm\")), \n               color = \"blue\") +   labs(title = \"Forces Acting on Particles\")\n\n\n\n\n\n\n\n\n\n\nPosition Update Mechanism\nThe position update process implements the core PSO algorithm, calculating new velocities based on the fundamental equation and updating particle positions accordingly:\n\n# Calculate new velocities using PSO equation\ndX &lt;- w * dX + \n      c1*runif(1)*(pbest - X) + \n      c2*runif(1)*(as.matrix(gbest) - X)\n\n# Update positions\nX &lt;- X + dX\n\n# Evaluate function at new positions\nobj &lt;- obj_func(X[,1], X[,2])\n\n# Update personal best positions if improved\nidx &lt;- which(obj &lt;= pbest_obj)\npbest[idx,] &lt;- X[idx,]\npbest_obj[idx] &lt;- obj[idx]\n\n# Update global best position\nidx &lt;- which.min(pbest_obj)\ngbest &lt;- pbest[idx,]\ngbest_obj &lt;- min(pbest_obj)\n\n# Visualize updated positions\nX_dir &lt;- X %&gt;% \n  mutate(g_x = gbest[1,1], \n         g_y = gbest[1,2], \n         angle = atan((g_y - y)/(g_x - x))*180/pi,\n         angle = ifelse(g_x &lt; x, 180 + angle, angle))\n\ncontour_plot + \n  geom_point(data = X, aes(x, y), color = \"red\", size = 2.5) + \n  geom_segment(data = X_dir, \n               aes(x = x, y = y, \n                   xend = x + 0.5*cos(angle*pi/180), \n                   yend = y + 0.5*sin(angle*pi/180)), \n               arrow = arrow(length = unit(0.1, \"cm\")), \n               color = \"blue\") + \n  labs(title = \"Particles After First Update\")"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#comprehensive-pso-algorithm-implementation",
    "href": "posts/building-particle-swarm-optimizer.html#comprehensive-pso-algorithm-implementation",
    "title": "Implementing Particle Swarm Optimization from Scratch in R",
    "section": "Comprehensive PSO Algorithm Implementation",
    "text": "Comprehensive PSO Algorithm Implementation\nThe following implementation encapsulates the complete PSO algorithm within a reusable function framework:\n\npso_optim &lt;- function(obj_func,      # Function to minimize\n                      c1 = 0.05,      # Personal learning rate\n                      c2 = 0.05,      # Social learning rate\n                      w = 0.8,        # Inertia weight\n                      n_particles = 20,  # Swarm size\n                      init_fact = 0.1,   # Initial velocity factor\n                      n_iter = 50        # Maximum iterations\n){\n  # Define search domain\n  x &lt;- seq(-5, 5, length.out = 100)\n  y &lt;- seq(-5, 5, length.out = 100)\n  \n  # Initialize particles\n  X &lt;- cbind(sample(x, n_particles, replace = FALSE),\n             sample(y, n_particles, replace = FALSE))\n  dX &lt;- matrix(runif(n_particles * 2) * init_fact, ncol = 2)\n  \n  # Initialize best positions\n  pbest &lt;- X\n  pbest_obj &lt;- obj_func(x = X[,1], y = X[,2])\n  gbest &lt;- pbest[which.min(pbest_obj),]\n  gbest_obj &lt;- min(pbest_obj)\n  \n  # Store positions for visualization\n  loc_df &lt;- data.frame(X, iter = 0)\n  iter &lt;- 1\n  \n  # Main optimization loop\n  while(iter &lt; n_iter){\n    # Update velocities\n    dX &lt;- w * dX + \n          c1*runif(1)*(pbest - X) + \n          c2*runif(1)*t(gbest - t(X))\n    \n    # Update positions\n    X &lt;- X + dX\n    \n    # Evaluate and update best positions\n    obj &lt;- obj_func(x = X[,1], y = X[,2])\n    idx &lt;- which(obj &lt;= pbest_obj)\n    pbest[idx,] &lt;- X[idx,]\n    pbest_obj[idx] &lt;- obj[idx]\n    \n    # Update global best\n    idx &lt;- which.min(pbest_obj)\n    gbest &lt;- pbest[idx,]\n    gbest_obj &lt;- min(pbest_obj)\n    \n    # Store for visualization\n    iter &lt;- iter + 1\n    loc_df &lt;- rbind(loc_df, data.frame(X, iter = iter))\n  }\n  \n  return(list(X = loc_df, \n              obj = gbest_obj, \n              obj_loc = paste0(gbest, collapse = \",\")))\n}\n\nThe following demonstration applies the PSO algorithm to the Ackley function optimization:\n\n# Run the PSO algorithm\nout &lt;- pso_optim(obj_func,\n                 c1 = 0.01,    # Low personal influence\n                 c2 = 0.05,    # Moderate social influence\n                 w = 0.5,      # Medium inertia\n                 n_particles = 50,\n                 init_fact = 0.1,\n                 n_iter = 200)\n\n# Check the result (global minimum should be at (1,1))\nout$obj_loc\n\n[1] \"0.999987965592705,0.999978373637581\""
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#dynamic-visualization-of-swarm-behavior",
    "href": "posts/building-particle-swarm-optimizer.html#dynamic-visualization-of-swarm-behavior",
    "title": "Implementing Particle Swarm Optimization from Scratch in R",
    "section": "Dynamic Visualization of Swarm Behavior",
    "text": "Dynamic Visualization of Swarm Behavior\nThe optimization process can be effectively visualized through animation, demonstrating the collective convergence behavior of the particle swarm:\n\n# Create animation of the optimization process\nggplot(out$X) +\n  geom_contour(data = grid, aes(x = Var1, y = Var2, z = z), color = \"black\") +\n  geom_point(aes(X1, X2)) +\n  labs(x = \"X\", y = \"Y\") +\n  transition_time(iter) +\n  ease_aes(\"linear\")"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#parameter-optimization-and-algorithm-tuning",
    "href": "posts/building-particle-swarm-optimizer.html#parameter-optimization-and-algorithm-tuning",
    "title": "Implementing Particle Swarm Optimization from Scratch in R",
    "section": "Parameter Optimization and Algorithm Tuning",
    "text": "Parameter Optimization and Algorithm Tuning\nThe PSO algorithm’s performance characteristics can be substantially modified through systematic adjustment of three parameters:\n\nInertia Weight (w)\n\nHigh values (&gt;0.8): Particles maintain substantial momentum, aiding extensive exploration\nLow values (&lt;0.4): Particles exhibit reduced momentum, facilitating solution refinement\n\nPersonal Learning Rate (c1)\n\nHigh values: Particles prioritize individual discoveries and historical performance\nLow values: Particles demonstrate reduced reliance on personal experience\n\nSocial Learning Rate (c2)\n\nHigh values: Particles demonstrate strong attraction toward the global optimum\nLow values: Particles maintain greater independence in exploration"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#advanced-implementation-considerations",
    "href": "posts/building-particle-swarm-optimizer.html#advanced-implementation-considerations",
    "title": "Implementing Particle Swarm Optimization from Scratch in R",
    "section": "Advanced Implementation Considerations",
    "text": "Advanced Implementation Considerations\nFor production-level applications, the following enhancements should be considered:\n\nBoundary Constraint Implementation: Enforce particle confinement within valid solution regions\nAdaptive Parameter Strategies: Implement dynamic parameter adjustment during optimization execution\nConvergence-Based Termination: Establish sophisticated stopping criteria based on solution convergence\nHigh-Dimensional Extension: Adapt the algorithm for complex, multi-dimensional optimization problems\n\n\nProduction-Ready Implementation\nThe R package pso provides a comprehensive, production-ready implementation suitable for industrial applications."
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#key-takeaways",
    "href": "posts/building-particle-swarm-optimizer.html#key-takeaways",
    "title": "Implementing Particle Swarm Optimization from Scratch in R",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nPSO successfully emulates biological swarm intelligence through mathematical modeling of collective behavior patterns\nThe velocity update equation balances three critical forces: inertia, personal experience, and social influence\nParameter tuning significantly affects algorithm performance, with distinct configurations optimizing exploration versus exploitation\nThe algorithm scales effectively to high-dimensional problems while maintaining computational efficiency\nProduction implementations require boundary constraints and adaptive parameters for robust performance"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#implementation",
    "href": "posts/building-particle-swarm-optimizer.html#implementation",
    "title": "Implementing Particle Swarm Optimization from Scratch in R",
    "section": "Implementation",
    "text": "Implementation\n\nSwarm Initialization\nThe initial phase involves creating a randomized swarm of particles and distributing them across the defined search space:\n\n# Set parameters\nn_particles &lt;- 20\nw &lt;- 0.5     # Inertia weight\nc1 &lt;- 0.05   # Personal learning rate\nc2 &lt;- 0.1    # Social learning rate\n\n# Create random particle positions\nx_range &lt;- seq(-5, 5, length.out = 20)\ny_range &lt;- seq(-5, 5, length.out = 20)\nX &lt;- data.frame(\n  x = sample(x_range, n_particles, replace = FALSE),\n  y = sample(y_range, n_particles, replace = FALSE)\n)\n\n# Visualize initial positions\ncontour_plot + \n  geom_point(data = X, aes(x, y), color = \"red\", size = 2.5) + \n  labs(title = \"Initial Particle Positions\")\n\n\n\n\n\n\n\n\n\n\nGlobal Best Position and Velocity Initialization\nNext, we need to track each particle’s personal best position and the swarm’s global best position, while initializing velocity vectors:\n\n# Initialize random velocities\ndX &lt;- matrix(runif(n_particles * 2), ncol = 2) * w\n\n# Set initial personal best positions\npbest &lt;- X\npbest_obj &lt;- obj_func(X[,1], X[,2])\n\n# Find global best position\ngbest &lt;- pbest[which.min(pbest_obj),]\ngbest_obj &lt;- min(pbest_obj)\n\n# Visualize with arrows showing pull toward global best\nX_dir &lt;- X %&gt;% \n  mutate(g_x = gbest[1,1], \n         g_y = gbest[1,2], \n         angle = atan((g_y - y)/(g_x - x))*180/pi,\n         angle = ifelse(g_x &lt; x, 180 + angle, angle))\n\ncontour_plot + \n  geom_point(data = X, aes(x, y), color = \"red\", size = 2.5) + \n  geom_segment(data = X_dir, \n               aes(x = x, y = y, \n                   xend = x + 0.5*cos(angle*pi/180), \n                   yend = y + 0.5*sin(angle*pi/180)), \n               arrow = arrow(length = unit(0.1, \"cm\")), \n               color = \"blue\") +   labs(title = \"Forces Acting on Particles\")\n\n\n\n\n\n\n\n\n\n\nPosition Update Mechanism\nThe position update process implements the core PSO algorithm, calculating new velocities based on the fundamental equation and updating particle positions accordingly:\n\n# Calculate new velocities using PSO equation\ndX &lt;- w * dX + \n      c1*runif(1)*(pbest - X) + \n      c2*runif(1)*(as.matrix(gbest) - X)\n\n# Update positions\nX &lt;- X + dX\n\n# Evaluate function at new positions\nobj &lt;- obj_func(X[,1], X[,2])\n\n# Update personal best positions if improved\nidx &lt;- which(obj &lt;= pbest_obj)\npbest[idx,] &lt;- X[idx,]\npbest_obj[idx] &lt;- obj[idx]\n\n# Update global best position\nidx &lt;- which.min(pbest_obj)\ngbest &lt;- pbest[idx,]\ngbest_obj &lt;- min(pbest_obj)\n\n# Visualize updated positions\nX_dir &lt;- X %&gt;% \n  mutate(g_x = gbest[1,1], \n         g_y = gbest[1,2], \n         angle = atan((g_y - y)/(g_x - x))*180/pi,\n         angle = ifelse(g_x &lt; x, 180 + angle, angle))\n\ncontour_plot + \n  geom_point(data = X, aes(x, y), color = \"red\", size = 2.5) + \n  geom_segment(data = X_dir, \n               aes(x = x, y = y, \n                   xend = x + 0.5*cos(angle*pi/180), \n                   yend = y + 0.5*sin(angle*pi/180)), \n               arrow = arrow(length = unit(0.1, \"cm\")), \n               color = \"blue\") + \n  labs(title = \"Particles After First Update\")"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#sample-implementation",
    "href": "posts/building-particle-swarm-optimizer.html#sample-implementation",
    "title": "Implementing Particle Swarm Optimization from Scratch in R",
    "section": "Sample Implementation",
    "text": "Sample Implementation\n\nSwarm Initialization\nThe initial phase involves creating a randomized swarm of particles and distributing them across the defined search space:\n\n# Set parameters\nn_particles &lt;- 20\nw &lt;- 0.5     # Inertia weight\nc1 &lt;- 0.05   # Personal learning rate\nc2 &lt;- 0.1    # Social learning rate\n\n# Create random particle positions\nx_range &lt;- seq(-5, 5, length.out = 20)\ny_range &lt;- seq(-5, 5, length.out = 20)\nX &lt;- data.frame(\n  x = sample(x_range, n_particles, replace = FALSE),\n  y = sample(y_range, n_particles, replace = FALSE)\n)\n\n# Visualize initial positions\ncontour_plot + \n  geom_point(data = X, aes(x, y), color = \"red\", size = 2.5) + \n  labs(title = \"Initial Particle Positions\")\n\n\n\n\n\n\n\n\n\n\nGlobal Best Position and Velocity Initialization\nNext, we need to track each particle’s personal best position and the swarm’s global best position, while initializing velocity vectors:\n\n# Initialize random velocities\ndX &lt;- matrix(runif(n_particles * 2), ncol = 2) * w\n\n# Set initial personal best positions\npbest &lt;- X\npbest_obj &lt;- obj_func(X[,1], X[,2])\n\n# Find global best position\ngbest &lt;- pbest[which.min(pbest_obj),]\ngbest_obj &lt;- min(pbest_obj)\n\n# Visualize with arrows showing pull toward global best\nX_dir &lt;- X %&gt;% \n  mutate(g_x = gbest[1,1], \n         g_y = gbest[1,2], \n         angle = atan((g_y - y)/(g_x - x))*180/pi,\n         angle = ifelse(g_x &lt; x, 180 + angle, angle))\n\ncontour_plot + \n  geom_point(data = X, aes(x, y), color = \"red\", size = 2.5) + \n  geom_segment(data = X_dir, \n               aes(x = x, y = y, \n                   xend = x + 0.5*cos(angle*pi/180), \n                   yend = y + 0.5*sin(angle*pi/180)), \n               arrow = arrow(length = unit(0.1, \"cm\")), \n               color = \"blue\") +   labs(title = \"Forces Acting on Particles\")\n\n\n\n\n\n\n\n\n\n\nPosition Update Mechanism\nThe position update process implements the core PSO algorithm, calculating new velocities based on the fundamental equation and updating particle positions accordingly:\n\n# Calculate new velocities using PSO equation\ndX &lt;- w * dX + \n      c1*runif(1)*(pbest - X) + \n      c2*runif(1)*(as.matrix(gbest) - X)\n\n# Update positions\nX &lt;- X + dX\n\n# Evaluate function at new positions\nobj &lt;- obj_func(X[,1], X[,2])\n\n# Update personal best positions if improved\nidx &lt;- which(obj &lt;= pbest_obj)\npbest[idx,] &lt;- X[idx,]\npbest_obj[idx] &lt;- obj[idx]\n\n# Update global best position\nidx &lt;- which.min(pbest_obj)\ngbest &lt;- pbest[idx,]\ngbest_obj &lt;- min(pbest_obj)\n\n# Visualize updated positions\nX_dir &lt;- X %&gt;% \n  mutate(g_x = gbest[1,1], \n         g_y = gbest[1,2], \n         angle = atan((g_y - y)/(g_x - x))*180/pi,\n         angle = ifelse(g_x &lt; x, 180 + angle, angle))\n\ncontour_plot + \n  geom_point(data = X, aes(x, y), color = \"red\", size = 2.5) + \n  geom_segment(data = X_dir, \n               aes(x = x, y = y, \n                   xend = x + 0.5*cos(angle*pi/180), \n                   yend = y + 0.5*sin(angle*pi/180)), \n               arrow = arrow(length = unit(0.1, \"cm\")), \n               color = \"blue\") + \n  labs(title = \"Particles After First Update\")"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#full-implementation",
    "href": "posts/building-particle-swarm-optimizer.html#full-implementation",
    "title": "Implementing Particle Swarm Optimization from Scratch in R",
    "section": "Full Implementation",
    "text": "Full Implementation\nThe following implementation encapsulates the complete PSO algorithm within a reusable function framework:\n\npso_optim &lt;- function(obj_func,      # Function to minimize\n                      c1 = 0.05,      # Personal learning rate\n                      c2 = 0.05,      # Social learning rate\n                      w = 0.8,        # Inertia weight\n                      n_particles = 20,  # Swarm size\n                      init_fact = 0.1,   # Initial velocity factor\n                      n_iter = 50        # Maximum iterations\n){\n  # Define search domain\n  x &lt;- seq(-5, 5, length.out = 100)\n  y &lt;- seq(-5, 5, length.out = 100)\n  \n  # Initialize particles\n  X &lt;- cbind(sample(x, n_particles, replace = FALSE),\n             sample(y, n_particles, replace = FALSE))\n  dX &lt;- matrix(runif(n_particles * 2) * init_fact, ncol = 2)\n  \n  # Initialize best positions\n  pbest &lt;- X\n  pbest_obj &lt;- obj_func(x = X[,1], y = X[,2])\n  gbest &lt;- pbest[which.min(pbest_obj),]\n  gbest_obj &lt;- min(pbest_obj)\n  \n  # Store positions for visualization\n  loc_df &lt;- data.frame(X, iter = 0)\n  iter &lt;- 1\n  \n  # Main optimization loop\n  while(iter &lt; n_iter){\n    # Update velocities\n    dX &lt;- w * dX + \n          c1*runif(1)*(pbest - X) + \n          c2*runif(1)*t(gbest - t(X))\n    \n    # Update positions\n    X &lt;- X + dX\n    \n    # Evaluate and update best positions\n    obj &lt;- obj_func(x = X[,1], y = X[,2])\n    idx &lt;- which(obj &lt;= pbest_obj)\n    pbest[idx,] &lt;- X[idx,]\n    pbest_obj[idx] &lt;- obj[idx]\n    \n    # Update global best\n    idx &lt;- which.min(pbest_obj)\n    gbest &lt;- pbest[idx,]\n    gbest_obj &lt;- min(pbest_obj)\n    \n    # Store for visualization\n    iter &lt;- iter + 1\n    loc_df &lt;- rbind(loc_df, data.frame(X, iter = iter))\n  }\n  \n  return(list(X = loc_df, \n              obj = gbest_obj, \n              obj_loc = paste0(gbest, collapse = \",\")))\n}\n\nThe following code snippet applies the PSO algorithm to the Ackley function:\n\n# Run the PSO algorithm\nout &lt;- pso_optim(obj_func,\n                 c1 = 0.01,    # Low personal influence\n                 c2 = 0.05,    # Moderate social influence\n                 w = 0.5,      # Medium inertia\n                 n_particles = 50,\n                 init_fact = 0.1,\n                 n_iter = 200)\n\n# Check the result (global minimum should be at (1,1))\nout$obj_loc\n\n[1] \"0.999984750825776,1.00002173699751\""
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#visualising-swarm-behavior",
    "href": "posts/building-particle-swarm-optimizer.html#visualising-swarm-behavior",
    "title": "Implementing Particle Swarm Optimization from Scratch in R",
    "section": "Visualising Swarm Behavior",
    "text": "Visualising Swarm Behavior\nThe optimization process can be effectively visualized through animation, demonstrating the collective convergence behavior of the particle swarm:\n\n# Create animation of the optimization process\nggplot(out$X) +\n  geom_contour(data = grid, aes(x = Var1, y = Var2, z = z), color = \"black\") +\n  geom_point(aes(X1, X2)) +\n  labs(x = \"X\", y = \"Y\") +\n  transition_time(iter) +\n  ease_aes(\"linear\")"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#implementation-considerations",
    "href": "posts/building-particle-swarm-optimizer.html#implementation-considerations",
    "title": "Implementing Particle Swarm Optimization from Scratch in R",
    "section": "Implementation Considerations",
    "text": "Implementation Considerations\n\nBoundary Constraint Implementation: Enforce particle confinement within valid solution regions\nAdaptive Parameter Strategies: Implement dynamic parameter adjustment during optimization execution\nConvergence-Based Termination: Establish sophisticated stopping criteria based on solution convergence\nHigh-Dimensional Extension: Adapt the algorithm for complex, multi-dimensional optimization problems\n\n\nThe R package pso provides a comprehensive, production-ready implementation suitable for industrial applications."
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#introduction",
    "href": "posts/custom-charting-functions-ggplot2.html#introduction",
    "title": "Developing Custom Charting Functions with ggplot2",
    "section": "",
    "text": "While R provides numerous options for two-dimensional graphics and data visualization, ggplot2 offers great functionality, features, and visual quality. This tutorial shows how to develop customized charting functions for specific visualization types, utilizing ggplot2 as the foundational visualization engine. The approach enables the creation of reusable, standardized visualization components suitable for production environments and analytical workflows."
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#package-dependencies",
    "href": "posts/custom-charting-functions-ggplot2.html#package-dependencies",
    "title": "Developing Custom Charting Functions with ggplot2",
    "section": "Package Dependencies",
    "text": "Package Dependencies\n\n# Load required packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(stringr)"
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#dataset-acquisition-and-preparation",
    "href": "posts/custom-charting-functions-ggplot2.html#dataset-acquisition-and-preparation",
    "title": "Developing Custom Charting Functions with ggplot2",
    "section": "Dataset Acquisition and Preparation",
    "text": "Dataset Acquisition and Preparation\nThis tutorial utilizes a summarized version of the COVID-19 Data Repository maintained by Johns Hopkins University to demonstrate custom charting function development.\n\n# Load COVID-19 data\ndf &lt;- read.csv(\"https://bit.ly/3G8G63u\")\n\n# Get top 5 countries by death count\ntop_countries &lt;- df %&gt;% \n  group_by(country) %&gt;% \n  summarise(count = sum(deaths_daily)) %&gt;% \n  top_n(5) %&gt;% \n  .$country\n\nprint(top_countries)\n\n[1] \"Brazil\" \"India\"  \"Mexico\" \"Russia\" \"US\"    \n\n\nSubsequently, we prepare the dataset for visualization by calculating a 7-day centered moving average of daily confirmed cases for the identified top five countries:\n\n# Create a data frame with the required information\n# Note that a centered 7-day moving average is used\nplotdf &lt;- df %&gt;% \n  mutate(date = as.Date(date, format = \"%m/%d/%Y\")) %&gt;% \n  filter(country %in% top_countries) %&gt;% \n  group_by(country, date) %&gt;% \n  summarise(count = sum(confirmed_daily)) %&gt;%\n  arrange(country, date) %&gt;% \n  group_by(country) %&gt;% \n  mutate(MA = zoo::rollapply(count, FUN = mean, width = 7, by = 1, fill = NA, align = \"center\"))"
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#fundamental-line-chart-function-development",
    "href": "posts/custom-charting-functions-ggplot2.html#fundamental-line-chart-function-development",
    "title": "Developing Custom Charting Functions with ggplot2",
    "section": "Fundamental Line Chart Function Development",
    "text": "Fundamental Line Chart Function Development\nThe initial implementation demonstrates the creation of a basic line chart function. Note the utilization of aes_string() instead of aes(), which enables the provision of arguments to ggplot2 as string parameters, thereby enhancing function flexibility and programmability.\n\n# Function definition\nline_chart &lt;- function(df, \n                       x, \n                       y, \n                       group_color = NULL, \n                       line_width = 1, \n                       line_type = 1){\n  \n  ggplot(df, aes(x = !! sym(x), \n                 y = !! sym(y), \n                 color = !! sym(group_color))) + \n    geom_line(linewidth = line_width, \n              linetype = line_type)\n}\n\n# Test run\nline_chart(plotdf,\n           x = \"date\",\n           y = \"MA\",\n           group_color = \"country\", \n           line_type = 1, \n           line_width = 1.2)"
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#custom-theme-development",
    "href": "posts/custom-charting-functions-ggplot2.html#custom-theme-development",
    "title": "Developing Custom Charting Functions with ggplot2",
    "section": "Custom Theme Development",
    "text": "Custom Theme Development\nHaving established the methodology for encapsulating ggplot2 calls within intuitive function wrappers, we proceed to develop a customized theme framework for our visualizations. This approach ensures consistent styling across all chart types and can be universally applied to any ggplot2 object.\n\ncustom_theme &lt;- function(plt, \n                         base_size = 11, \n                         base_line_size = 1, \n                         palette = \"Set1\"){\n  \n  # Note the use of \"+\" and not \"%&gt;%\"\n  plt + \n    # Adjust overall font size\n    theme_minimal(base_size = base_size, \n                  base_line_size = base_line_size) + \n    \n    # Put legend at the bottom\n    theme(legend.position = \"bottom\") + \n    \n    # Different colour scale\n    scale_color_brewer(palette = palette)\n}\n\n# Test run\nline_chart(plotdf, \"date\", \"MA\", \"country\") %&gt;% custom_theme()"
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#advanced-function-enhancement",
    "href": "posts/custom-charting-functions-ggplot2.html#advanced-function-enhancement",
    "title": "Developing Custom Charting Functions with ggplot2",
    "section": "Advanced Function Enhancement",
    "text": "Advanced Function Enhancement\nThe following section demonstrates the expansion of the line_chart() function to incorporate additional features and parameters, thereby increasing its versatility and applicability across diverse visualization requirements:\n\nline_chart &lt;- function(df, \n                       x, y, \n                       group_color = NULL, \n                       line_width = 1, \n                       line_type = 1, \n                       xlab = NULL, \n                       ylab = NULL, \n                       title = NULL, \n                       subtitle = NULL, \n                       caption = NULL){\n  # Base plot\n  ggplot(df, aes(x = !! sym(x), \n                 y = !! sym(y), \n                 color = !! sym(group_color))) + \n    \n    # Line chart \n    geom_line(size = line_width, \n              linetype = line_type) + \n    \n    # Titles and subtitles\n    labs(x = xlab, \n         y = ylab, \n         title = title, \n         subtitle = subtitle, \n         caption = caption)\n}\n\nCorrespondingly, we enhance the custom_theme() function to accommodate diverse axis formatting options and advanced styling parameters:\n\ncustom_theme &lt;- function(plt, \n                         palette = \"Set1\", \n                         format_x_axis_as = NULL, \n                         format_y_axis_as = NULL, \n                         x_axis_scale = 1, \n                         y_axis_scale = 1, \n                         x_axis_text_size = 10, \n                         y_axis_text_size = 10, \n                         base_size = 11, \n                         base_line_size = 1, \n                         x_angle = 45){\n  \n  mappings &lt;- names(unlist(plt$mapping))\n  \n  p &lt;- plt + \n    \n    # Adjust overall font size\n    theme_minimal(base_size = base_size, \n                  base_line_size = base_line_size) + \n    \n    # Put legend at the bottom\n    theme(legend.position = \"bottom\", \n          axis.text.x = element_text(angle = x_angle)) + \n    \n    # Different colour palette\n    {if(\"colour\" %in% mappings) scale_color_brewer(palette = palette)}+\n    \n    {if(\"fill\" %in% mappings) scale_fill_brewer(palette = palette)}+\n    \n    # Change some theme options\n    theme(plot.background = element_rect(fill = \"#f7f7f7\"), \n          plot.subtitle = element_text(face = \"italic\"), \n          axis.title.x = element_text(face = \"bold\", \n                                      size = x_axis_text_size), \n          axis.title.y = element_text(face = \"bold\", \n                                      size = y_axis_text_size)) + \n    \n    # Change x-axis formatting\n    {if(!is.null(format_x_axis_as))\n      switch(format_x_axis_as, \n             \"date\" = scale_x_date(breaks = pretty_breaks(n = 12)), \n             \"number\" = scale_x_continuous(labels = number_format(accuracy = 0.1, \n                                                                  decimal.mark = \",\", \n                                                                  scale = x_axis_scale)), \n             \"percent\" = scale_x_continuous(labels = percent))} + \n    \n    # Change y-axis formatting\n    {if(!is.null(format_y_axis_as))\n      \n      switch(format_y_axis_as, \n             \"date\" = scale_y_date(breaks = pretty_breaks(n = 12)), \n             \"number\" = scale_y_continuous(labels = number_format(accuracy = 0.1, \n                                                                  decimal.mark = \",\", \n                                                                  scale = y_axis_scale)), \n             \"percent\" = scale_y_continuous(labels = percent))}\n  \n  # Capitalise all names\n  vec &lt;- lapply(p$labels, str_to_title)\n  names(vec) &lt;- names(p$labels)\n  p$labels &lt;- vec\n  \n  return(p)\n}"
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#integrated-function-implementation",
    "href": "posts/custom-charting-functions-ggplot2.html#integrated-function-implementation",
    "title": "Developing Custom Charting Functions with ggplot2",
    "section": "Integrated Function Implementation",
    "text": "Integrated Function Implementation\nThe following demonstration illustrates the coordinated application of our enhanced functions to generate a polished, publication-ready visualization:\n\nline_chart(plotdf,\n           x = \"date\", \n           y = \"MA\", \n           group_color = \"country\", \n           xlab = \"Date\", \n           ylab = \"Moving Avg. (in '000)\", \n           title = \"Daily COVID19 Case Load\", \n           subtitle = \"Top 5 countries by volume\") %&gt;% \n  \n  custom_theme(format_x_axis_as = \"date\", \n               format_y_axis_as = \"number\", \n               y_axis_scale = 0.001)"
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#cross-chart-type-theme-application",
    "href": "posts/custom-charting-functions-ggplot2.html#cross-chart-type-theme-application",
    "title": "Developing Custom Charting Functions with ggplot2",
    "section": "Cross-Chart Type Theme Application",
    "text": "Cross-Chart Type Theme Application\nThe architectural design of our custom_theme() function enables its universal application to any ggplot2 object, regardless of visualization type. The following example demonstrates this flexibility through bar chart implementation:\n\np &lt;- plotdf %&gt;%  \n  mutate(month = format(date, \"%m-%b\")) %&gt;% \n  ggplot(aes(x = month, y = MA, fill = country)) + \n  geom_col(position = \"dodge\") + \n  labs(title = \"Monthly COVID19 Case load trend\", \n       subtitle = \"Top 5 countries\", \n       x = \"Month\", \n       y = \"Moving Average ('000)\")\n\ncustom_theme(p, \n             palette = \"Set2\", \n             format_y_axis_as = \"number\", \n             y_axis_scale = 0.001)"
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#strategic-advantages-of-custom-charting-functions",
    "href": "posts/custom-charting-functions-ggplot2.html#strategic-advantages-of-custom-charting-functions",
    "title": "Developing Custom Charting Functions with ggplot2",
    "section": "Strategic Advantages of Custom Charting Functions",
    "text": "Strategic Advantages of Custom Charting Functions\nThe development of custom charting functions utilizing ggplot2 provides substantial advantages for analytical workflows:\n\nVisual Consistency: Ensures uniform appearance and styling across all visualizations within reports or analytical dashboards.\nDevelopment Efficiency: Significantly reduces code volume required for frequently utilized chart types and configurations.\nMaintenance Optimization: Facilitates centralized style updates through single function modifications, propagating changes across all implementations.\nAccessibility Enhancement: Abstracts ggplot2 complexity for team members with varying levels of package familiarity, democratizing visualization capabilities."
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#implementation-strategy-custom-functions-vs.-direct-ggplot2",
    "href": "posts/custom-charting-functions-ggplot2.html#implementation-strategy-custom-functions-vs.-direct-ggplot2",
    "title": "Developing Custom Charting Functions with ggplot2",
    "section": "Implementation Strategy: Custom Functions vs. Direct ggplot2",
    "text": "Implementation Strategy: Custom Functions vs. Direct ggplot2\nThe development of customized charting functions utilizing ggplot2 demonstrates optimal value when creating repetitive visualization types within structured analytical workflows. For exploratory data analysis activities, direct ggplot2 implementation often provides superior flexibility, enabling rapid prototyping and layered chart construction within integrated analytical pipelines."
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#key-takeaways",
    "href": "posts/custom-charting-functions-ggplot2.html#key-takeaways",
    "title": "Developing Custom Charting Functions with ggplot2",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nCustom function development enhances visualization consistency across analytical products and reporting frameworks\nString-based aesthetic mapping enables flexible parameter passing and programmatic chart generation\nModular theme design allows universal application across diverse chart types and visualization requirements\nEnhanced function parameters provide comprehensive control over styling, formatting, and axis presentation\nThe approach scales effectively for production environments where standardized visualization components are essential\nStrategic implementation decisions depend on use case: custom functions for repetitive tasks, direct ggplot2 for exploratory analysis\n\nThese principles establish a foundation for developing sophisticated visualization libraries and maintaining consistent analytical presentation standards across organizational reporting frameworks."
  },
  {
    "objectID": "posts/generating-correlated-random-numbers.html#introduction",
    "href": "posts/generating-correlated-random-numbers.html#introduction",
    "title": "Generating Correlated Random Numbers in R Using Matrix Methods",
    "section": "",
    "text": "The generation of random data with specified correlation patterns can be useful in statistical simulation and this tutorial provides a methodology for creating correlated random numbers in R. The techniques presented enable the development of realistic synthetic datasets with precisely controlled correlation structures, essential for robust statistical analysis and model validation."
  },
  {
    "objectID": "posts/generating-correlated-random-numbers.html#the-cholesky-decomposition-method-a-four-step-implementation",
    "href": "posts/generating-correlated-random-numbers.html#the-cholesky-decomposition-method-a-four-step-implementation",
    "title": "Generating Correlated Random Numbers in R Using Matrix Methods",
    "section": "The Cholesky Decomposition Method: A Four-Step Implementation",
    "text": "The Cholesky Decomposition Method: A Four-Step Implementation\n\n# 1. Define your target correlation matrix\ncor_mat &lt;- matrix(c(1, 0.3, \n                   0.3, 1), nrow = 2, byrow = TRUE)\n\n# 2. Apply Cholesky decomposition\nchol_mat &lt;- chol(cor_mat)\n\n# 3. Generate uncorrelated random numbers\nold_random &lt;- matrix(rnorm(2000), ncol = 2)\n\n# 4. Transform to create correlation\nnew_random &lt;- old_random %*% chol_mat\n\n# Verify the correlation\ncor(new_random)\n\n          [,1]      [,2]\n[1,] 1.0000000 0.3099169\n[2,] 0.3099169 1.0000000\n\n\nThe resulting new_random matrix contains values exhibiting approximately the target correlation structure. This technique employs Cholesky decomposition to construct a transformation matrix that induces the desired correlation when applied to uncorrelated input data."
  },
  {
    "objectID": "posts/generating-correlated-random-numbers.html#critical-implementation-considerations-and-common-pitfalls",
    "href": "posts/generating-correlated-random-numbers.html#critical-implementation-considerations-and-common-pitfalls",
    "title": "Generating Correlated Random Numbers in R Using Matrix Methods",
    "section": "Critical Implementation Considerations and Common Pitfalls",
    "text": "Critical Implementation Considerations and Common Pitfalls\n\nInput Data Independence Requirement\nThe input data must demonstrate statistical independence for the Cholesky method to function correctly. Pre-existing correlations in the input data compromise the method’s ability to achieve target correlation structures:\n\n# What happens with already correlated input?\nsimulate_correlation &lt;- function(input_correlation, target = 0.3) {\n  results &lt;- replicate(1000, {\n    # Create input with specified correlation\n    x &lt;- rnorm(1000)\n    y &lt;- input_correlation * x + rnorm(1000, sd = sqrt(1 - input_correlation^2))\n    \n    # Apply our method\n    old_random &lt;- cbind(x, y)\n    chol_mat &lt;- chol(matrix(c(1, target, target, 1), ncol = 2))\n    new_random &lt;- old_random %*% chol_mat\n    \n    # Return resulting correlation\n    cor(new_random)[1,2]\n  })\n  return(results)\n}\n\n# Compare results with different input correlations\npar(mfrow = c(1, 2))\nhist(simulate_correlation(0.8), main = \"Starting with Correlated Data\",\n     xlim = c(0, 1), col = \"salmon\")\nhist(simulate_correlation(0.001), main = \"Starting with Random Data\",     xlim = c(0, 1), col = \"lightblue\")\n\n\n\n\n\n\n\n\nWhen input data contains pre-existing correlation patterns, the Cholesky method cannot effectively override these relationships to establish the desired target correlation structure.\n\n\nDistribution Consistency Requirement\nOptimal results require consistent probability distributions across all variables in the transformation:\n\n# Different distributions cause problems\nset.seed(123)\nx1 &lt;- rchisq(1000, df = 3)  # Chi-squared (skewed)\ny1 &lt;- rnorm(1000)           # Normal (symmetric)\nold_mixed &lt;- cbind(x1, y1)\n\n# Same distribution works better\nx2 &lt;- rchisq(1000, df = 3)\ny2 &lt;- rchisq(1000, df = 3)\nold_same &lt;- cbind(x2, y2)\n\n# Apply the same transformation to both\nchol_mat &lt;- chol(matrix(c(1, 0.7, 0.7, 1), ncol = 2))\nnew_mixed &lt;- old_mixed %*% chol_mat\nnew_same &lt;- old_same %*% chol_mat\n\n# Compare results\ncat(\"Target correlation: 0.7\\n\")\n\nTarget correlation: 0.7\n\ncat(\"Mixed distributions result:\", round(cor(new_mixed)[1,2], 3), \"\\n\")\n\nMixed distributions result: 0.915 \n\ncat(\"Same distribution result:\", round(cor(new_same)[1,2], 3))\n\nSame distribution result: 0.699\n\n\nThe combination of different probability distributions (such as normal and chi-squared) can result in unexpected correlation patterns following the Cholesky transformation.\n\n\nDistribution Property Preservation Challenges\nThe Cholesky transformation may fundamentally alter the statistical properties of the original data:\n\n# Original positive-only distribution\nx &lt;- rchisq(1000, df = 3)  # Always positive\ny &lt;- rchisq(1000, df = 3)  # Always positive\nold_random &lt;- cbind(x, y)\n\n# Apply negative correlation\nchol_mat &lt;- chol(matrix(c(1, -0.7, -0.7, 1), ncol = 2))\nnew_random &lt;- old_random %*% chol_mat\n\n# Check what happened\ncat(\"Original data range:\", round(range(old_random), 2), \"\\n\")\n\nOriginal data range: 0.02 19.93 \n\ncat(\"Transformed data range:\", round(range(new_random), 2), \"\\n\")\n\nTransformed data range: -12.81 19.93 \n\ncat(\"Negative values in result:\", sum(new_random &lt; 0), \"out of\", length(new_random))\n\nNegative values in result: 488 out of 2000\n\n\nThe Cholesky transformation can fundamentally modify data characteristics, such as introducing negative values into previously positive-only distributions, thereby altering the fundamental nature of the data."
  },
  {
    "objectID": "posts/generating-correlated-random-numbers.html#alternative-implementation-the-mvtnorm-package-approach",
    "href": "posts/generating-correlated-random-numbers.html#alternative-implementation-the-mvtnorm-package-approach",
    "title": "Generating Correlated Random Numbers in R Using Matrix Methods",
    "section": "Alternative Implementation: The mvtnorm Package Approach",
    "text": "Alternative Implementation: The mvtnorm Package Approach\nFor practical applications requiring efficient implementation, the mvtnorm package provides a streamlined solution for generating multivariate normal distributions with specified correlation structures:\n\n# Load the package\nlibrary(mvtnorm)\n\n# Define means and covariance matrix\nmeans &lt;- c(10, 20)  # Mean for each variable\nsigma &lt;- matrix(c(4, 2,   # Covariance matrix\n                  2, 3), ncol = 2)\n\n# See the implied correlation\ncov2cor(sigma)\n\n          [,1]      [,2]\n[1,] 1.0000000 0.5773503\n[2,] 0.5773503 1.0000000\n\n# Generate correlated normal data in one step\nx &lt;- rmvnorm(n = 1000, mean = means, sigma = sigma)\n\n# Verify the result\nround(cor(x), 3)\n\n      [,1]  [,2]\n[1,] 1.000 0.613\n[2,] 0.613 1.000"
  },
  {
    "objectID": "posts/generating-correlated-random-numbers.html#methodological-selection-criteria",
    "href": "posts/generating-correlated-random-numbers.html#methodological-selection-criteria",
    "title": "Generating Correlated Random Numbers in R Using Matrix Methods",
    "section": "Methodological Selection Criteria",
    "text": "Methodological Selection Criteria\n\nCholesky Decomposition Method\nRecommended Applications: - Educational contexts requiring understanding of underlying mathematical principles - Non-normal distribution requirements - Custom correlation structure development - Theoretical research applications\n\n\nmvtnorm Package Implementation\nRecommended Applications: - Production environments requiring rapid multivariate normal data generation - Scenarios demanding precise control over means and variance parameters - High-dimensional variable systems - Operational analytical workflows"
  },
  {
    "objectID": "posts/generating-correlated-random-numbers.html#key-takeaways",
    "href": "posts/generating-correlated-random-numbers.html#key-takeaways",
    "title": "Generating Correlated Random Numbers in R Using Matrix Methods",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nCholesky decomposition provides a mathematical foundation for transforming uncorrelated data into correlated structures through matrix operations\nInput data independence is critical for successful correlation induction; pre-existing correlations compromise the transformation effectiveness\nDistribution consistency across variables ensures optimal results and prevents unexpected correlation artifacts\nThe transformation process can alter fundamental data properties, requiring careful consideration of distributional characteristics\nThe mvtnorm package offers production-ready solutions for multivariate normal data generation with specified correlation structures\nMethod selection depends on specific requirements: Cholesky for educational and custom applications, mvtnorm for operational efficiency"
  },
  {
    "objectID": "posts/generating-correlated-random-numbers.html#the-cholesky-decomposition-method",
    "href": "posts/generating-correlated-random-numbers.html#the-cholesky-decomposition-method",
    "title": "Generating Correlated Random Numbers in R Using Matrix Methods",
    "section": "The Cholesky Decomposition Method",
    "text": "The Cholesky Decomposition Method\n\n# Load required packages\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# 1. Define your target correlation matrix\ncor_mat &lt;- matrix(c(1, 0.3, \n                   0.3, 1), nrow = 2, byrow = TRUE)\n\n# 2. Apply Cholesky decomposition\nchol_mat &lt;- chol(cor_mat)\n\n# 3. Generate uncorrelated random numbers\nold_random &lt;- matrix(rnorm(2000), ncol = 2)\n\n# 4. Transform to create correlation\nnew_random &lt;- old_random %*% chol_mat\n\n# Verify the correlation\ncor(new_random)\n\n          [,1]      [,2]\n[1,] 1.0000000 0.3152987\n[2,] 0.3152987 1.0000000\n\n\nThe resulting new_random matrix contains values exhibiting approximately the target correlation structure."
  },
  {
    "objectID": "posts/generating-correlated-random-numbers.html#implementation-considerations-and-common-pitfalls",
    "href": "posts/generating-correlated-random-numbers.html#implementation-considerations-and-common-pitfalls",
    "title": "Generating Correlated Random Numbers in R Using Matrix Methods",
    "section": "Implementation Considerations and Common Pitfalls",
    "text": "Implementation Considerations and Common Pitfalls\n\nInput Data Independence Requirement\nThe input data must demonstrate statistical independence for the Cholesky method to function correctly. Pre-existing correlations in the input data compromise the method’s ability to achieve target correlation structures:\n\n# What happens with already correlated input?\nsimulate_correlation &lt;- function(input_correlation, target = 0.3) {\n  results &lt;- replicate(1000, {\n    # Create input with specified correlation\n    x &lt;- rnorm(1000)\n    y &lt;- input_correlation * x + rnorm(1000, sd = sqrt(1 - input_correlation^2))\n    \n    # Apply our method\n    old_random &lt;- cbind(x, y)\n    chol_mat &lt;- chol(matrix(c(1, target, target, 1), ncol = 2))\n    new_random &lt;- old_random %*% chol_mat\n    \n    # Return resulting correlation\n    cor(new_random)[1,2]\n  })\n  return(results)\n}\n\n# Compare results with different input correlations\npar(mfrow = c(1, 2))\nhist(simulate_correlation(0.8), main = \"Starting with Correlated Data\",\n     xlim = c(0, 1), col = \"salmon\")\nhist(simulate_correlation(0.001), main = \"Starting with Random Data\",     xlim = c(0, 1), col = \"lightblue\")\n\n\n\n\n\n\n\n\nWhen input data contains pre-existing correlation patterns, the Cholesky method cannot effectively override these relationships to establish the desired target correlation structure.\n\n\nDistribution Consistency Requirement\nOptimal results require consistent probability distributions across all variables in the transformation:\n\n# Different distributions cause problems\nset.seed(123)\nx1 &lt;- rchisq(1000, df = 3)  # Chi-squared (skewed)\ny1 &lt;- rnorm(1000)           # Normal (symmetric)\nold_mixed &lt;- cbind(x1, y1)\n\n# Same distribution works better\nx2 &lt;- rchisq(1000, df = 3)\ny2 &lt;- rchisq(1000, df = 3)\nold_same &lt;- cbind(x2, y2)\n\n# Apply the same transformation to both\nchol_mat &lt;- chol(matrix(c(1, 0.7, 0.7, 1), ncol = 2))\nnew_mixed &lt;- old_mixed %*% chol_mat\nnew_same &lt;- old_same %*% chol_mat\n\n# Compare results\ncat(\"Target correlation: 0.7\\n\")\n\nTarget correlation: 0.7\n\ncat(\"Mixed distributions result:\", round(cor(new_mixed)[1,2], 3), \"\\n\")\n\nMixed distributions result: 0.915 \n\ncat(\"Same distribution result:\", round(cor(new_same)[1,2], 3))\n\nSame distribution result: 0.699\n\n\nThe combination of different probability distributions (such as normal and chi-squared) can result in unexpected correlation patterns following the Cholesky transformation.\n\n\nDistribution Property Preservation Challenges\nThe Cholesky transformation may fundamentally alter the statistical properties of the original data:\n\n# Original positive-only distribution\nx &lt;- rchisq(1000, df = 3)  # Always positive\ny &lt;- rchisq(1000, df = 3)  # Always positive\nold_random &lt;- cbind(x, y)\n\n# Apply negative correlation\nchol_mat &lt;- chol(matrix(c(1, -0.7, -0.7, 1), ncol = 2))\nnew_random &lt;- old_random %*% chol_mat\n\n# Check what happened\ncat(\"Original data range:\", round(range(old_random), 2), \"\\n\")\n\nOriginal data range: 0.02 19.93 \n\ncat(\"Transformed data range:\", round(range(new_random), 2), \"\\n\")\n\nTransformed data range: -12.81 19.93 \n\ncat(\"Negative values in result:\", sum(new_random &lt; 0), \"out of\", length(new_random))\n\nNegative values in result: 488 out of 2000\n\n\nThe Cholesky transformation can fundamentally modify data characteristics, such as introducing negative values into previously positive-only distributions, thereby altering the fundamental nature of the data."
  },
  {
    "objectID": "posts/generating-correlated-random-numbers.html#implementation-considerations",
    "href": "posts/generating-correlated-random-numbers.html#implementation-considerations",
    "title": "Generating Correlated Random Numbers in R Using Matrix Methods",
    "section": "Implementation Considerations",
    "text": "Implementation Considerations\n\nIndependence\nThe input data must demonstrate statistical independence for the Cholesky method to function correctly. Pre-existing correlations in the input data compromise the method’s ability to achieve target correlation structures:\n\n# What happens with already correlated input?\nsimulate_correlation &lt;- function(input_correlation, target = 0.3) {\n  results &lt;- replicate(1000, {\n    # Create input with specified correlation\n    x &lt;- rnorm(1000)\n    y &lt;- input_correlation * x + rnorm(1000, sd = sqrt(1 - input_correlation^2))\n    \n    # Apply our method\n    old_random &lt;- cbind(x, y)\n    chol_mat &lt;- chol(matrix(c(1, target, target, 1), ncol = 2))\n    new_random &lt;- old_random %*% chol_mat\n    \n    # Return resulting correlation\n    cor(new_random)[1,2]\n  })\n  return(results)\n}\n\n# Compare results with different input correlations\ncorrelated_results &lt;- simulate_correlation(0.8, target = 0.3)\nuncorrelated_results &lt;- simulate_correlation(0.001, target = 0.3)\n\n# Create data frame for ggplot2\nplot_data &lt;- data.frame(\n  correlation = c(correlated_results, uncorrelated_results),\n  input_type = factor(rep(c(\"Correlated Input (0.8)\", \"Uncorrelated Input (0.001)\"), \n                         each = length(correlated_results)))\n)\n\n# Create density plot with ggplot2\nggplot(plot_data, aes(x = correlation, fill = input_type, color = input_type)) +\n  geom_density(alpha = 0.6) +\n  labs(title = \"Effect of Input Correlation on Target Correlation Achievement\",\n       subtitle = \"Target correlation = 0.3\",\n       x = \"Achieved Correlation\",\n       y = \"Density\",\n       fill = \"Input Data Type\",\n       color = \"Input Data Type\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_fill_manual(values = c(\"slateblue\", \"lightblue\")) +\n  scale_color_manual(values = c(\"darkblue\", \"darkblue\"))\n\n\n\n\n\n\n\n\nWhen input data contains pre-existing correlation patterns, the Cholesky method cannot effectively override these relationships to establish the desired target correlation structure.\n\n\nDistribution Consistency\nOptimal results require consistent probability distributions across all variables in the transformation:\n\n# Different distributions cause problems\nset.seed(123)\nx1 &lt;- rchisq(1000, df = 3)  # Chi-squared (skewed)\ny1 &lt;- rnorm(1000)           # Normal (symmetric)\nold_mixed &lt;- cbind(x1, y1)\n\n# Same distribution works better\nx2 &lt;- rchisq(1000, df = 3)\ny2 &lt;- rchisq(1000, df = 3)\nold_same &lt;- cbind(x2, y2)\n\n# Apply the same transformation to both\nchol_mat &lt;- chol(matrix(c(1, 0.7, 0.7, 1), ncol = 2))\nnew_mixed &lt;- old_mixed %*% chol_mat\nnew_same &lt;- old_same %*% chol_mat\n\n# Compare results\ncat(\"Target correlation: 0.7\\n\")\n\nTarget correlation: 0.7\n\ncat(\"Mixed distributions result:\", round(cor(new_mixed)[1,2], 3), \"\\n\")\n\nMixed distributions result: 0.915 \n\ncat(\"Same distribution result:\", round(cor(new_same)[1,2], 3))\n\nSame distribution result: 0.699\n\n\nThe combination of different probability distributions (such as normal and chi-squared) can result in unexpected correlation patterns following the Cholesky transformation.\n\n\nDistribution Properties\nThe Cholesky transformation may fundamentally alter the statistical properties of the original data:\n\n# Original positive-only distribution\nx &lt;- rchisq(1000, df = 3)  # Always positive\ny &lt;- rchisq(1000, df = 3)  # Always positive\nold_random &lt;- cbind(x, y)\n\n# Apply negative correlation\nchol_mat &lt;- chol(matrix(c(1, -0.7, -0.7, 1), ncol = 2))\nnew_random &lt;- old_random %*% chol_mat\n\n# Check what happened\ncat(\"Original data range:\", round(range(old_random), 2), \"\\n\")\n\nOriginal data range: 0.02 19.93 \n\ncat(\"Transformed data range:\", round(range(new_random), 2), \"\\n\")\n\nTransformed data range: -12.81 19.93 \n\ncat(\"Negative values in result:\", sum(new_random &lt; 0), \"out of\", length(new_random))\n\nNegative values in result: 488 out of 2000\n\n\nThe Cholesky transformation can fundamentally modify data characteristics, such as introducing negative values into previously positive-only distributions, thereby altering the fundamental nature of the data."
  },
  {
    "objectID": "posts/generating-correlated-random-numbers.html#alternate-implementation-mvtnorm",
    "href": "posts/generating-correlated-random-numbers.html#alternate-implementation-mvtnorm",
    "title": "Generating Correlated Random Numbers in R Using Matrix Methods",
    "section": "Alternate Implementation: mvtnorm",
    "text": "Alternate Implementation: mvtnorm\nFor practical applications requiring efficient implementation, the mvtnorm package provides a streamlined solution for generating multivariate normal distributions with specified correlation structures:\n\n# Load the package\nlibrary(mvtnorm)\n\n# Define means and covariance matrix\nmeans &lt;- c(10, 20)  # Mean for each variable\nsigma &lt;- matrix(c(4, 2,   # Covariance matrix\n                  2, 3), ncol = 2)\n\n# See the implied correlation\ncov2cor(sigma)\n\n          [,1]      [,2]\n[1,] 1.0000000 0.5773503\n[2,] 0.5773503 1.0000000\n\n# Generate correlated normal data in one step\nx &lt;- rmvnorm(n = 1000, mean = means, sigma = sigma)\n\n# Verify the result\nround(cor(x), 3)\n\n      [,1]  [,2]\n[1,] 1.000 0.613\n[2,] 0.613 1.000"
  },
  {
    "objectID": "posts/getting-started-with-reticulate.html#install-reticulate",
    "href": "posts/getting-started-with-reticulate.html#install-reticulate",
    "title": "Getting Started with Python using R and Reticulate",
    "section": "Install reticulate",
    "text": "Install reticulate\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)"
  },
  {
    "objectID": "posts/getting-started-with-reticulate.html#install-python-via-miniconda",
    "href": "posts/getting-started-with-reticulate.html#install-python-via-miniconda",
    "title": "Getting Started with Python using R and Reticulate",
    "section": "Install Python via Miniconda",
    "text": "Install Python via Miniconda\nThe easiest approach is to let reticulate handle Python installation for you:\n\ninstall_miniconda(path = \"c:/miniconda\")"
  },
  {
    "objectID": "posts/getting-started-with-reticulate.html#connect-to-python",
    "href": "posts/getting-started-with-reticulate.html#connect-to-python",
    "title": "Getting Started with Python using R and Reticulate",
    "section": "Connect to Python",
    "text": "Connect to Python\nReticulate creates a default environment called r-reticulate. Let’s connect to it:\n\n# Check available environments\nconda_list()\n\n# Connect to the default environment\nuse_condaenv(\"r-reticulate\")"
  },
  {
    "objectID": "posts/getting-started-with-reticulate.html#install-python-packages",
    "href": "posts/getting-started-with-reticulate.html#install-python-packages",
    "title": "Getting Started with Python using R and Reticulate",
    "section": "Install Python Packages",
    "text": "Install Python Packages\nNow you can install any Python packages you need:\n\npy_install(c(\"pandas\", \"scikit-learn\", \"matplotlib\"))"
  },
  {
    "objectID": "posts/getting-started-with-reticulate.html#ways-to-use-python-in-r",
    "href": "posts/getting-started-with-reticulate.html#ways-to-use-python-in-r",
    "title": "Getting Started with Python using R and reticulate",
    "section": "Ways to Use Python in R",
    "text": "Ways to Use Python in R\n\n1. Import Python Modules Directly\n\n# Import pandas and use it like any R package\npd &lt;- import(\"pandas\")\n\n# Create a pandas Series\npd$Series(c(1, 2, 3, 4, 5))\n\n# Import numpy for numerical operations\nnp &lt;- import(\"numpy\")\nnp$mean(c(1:100))  # Calculate mean using numpy\n\n\n\n2. Write Python Code in R Markdown\nYou can mix R and Python code in the same document by using Python code chunks:\n\n# This is Python code!\nimport pandas as pd\nimport numpy as np\n\n# Create a simple DataFrame\ndf = pd.DataFrame({\n    'A': np.random.randn(5),\n    'B': np.random.randn(5)\n})\n\nprint(df.describe())\n\n\n\n3. Use Python Libraries in R Workflows\nThe most powerful approach is using Python’s machine learning libraries within R:\n\n# Import scikit-learn\nsk &lt;- import(\"sklearn.linear_model\")\n\n# Create and fit a linear regression model\nmodel &lt;- sk$LinearRegression()\nmodel$fit(X = as.matrix(mtcars[, c(\"disp\", \"hp\", \"wt\")]), \n         y = mtcars$mpg)\n\n# Get predictions and coefficients\npredictions &lt;- model$predict(as.matrix(mtcars[, c(\"disp\", \"hp\", \"wt\")]))\ncoefficients &lt;- data.frame(\n  Feature = c(\"Intercept\", \"disp\", \"hp\", \"wt\"),\n  Coefficient = c(model$intercept_, model$coef_)\n)\n\ncoefficients"
  },
  {
    "objectID": "posts/getting-started-with-reticulate.html#different-ways-to-use-python-in-r",
    "href": "posts/getting-started-with-reticulate.html#different-ways-to-use-python-in-r",
    "title": "Getting Started with Python using R and Reticulate",
    "section": "Different Ways to Use Python in R",
    "text": "Different Ways to Use Python in R\n\n1. Import Python Modules Directly\n\n# Import pandas and use it like any R package\npd &lt;- import(\"pandas\")\n\n# Create a pandas Series\npd$Series(c(1, 2, 3, 4, 5))\n\n# Import numpy for numerical operations\nnp &lt;- import(\"numpy\")\nnp$mean(c(1:100))  # Calculate mean using numpy\n\n\n\n2. Write Python Code in R Markdown\nYou can mix R and Python code in the same document by using Python code chunks:\n\n# This is Python code!\nimport pandas as pd\nimport numpy as np\n\n# Create a simple DataFrame\ndf = pd.DataFrame({\n    'A': np.random.randn(5),\n    'B': np.random.randn(5)\n})\n\nprint(df.describe())\n\n\n\n3. Use Python Libraries in R Workflows\nThe most powerful approach is using Python’s machine learning libraries within R:\n\n# Import scikit-learn\nsk &lt;- import(\"sklearn.linear_model\")\n\n# Create and fit a linear regression model\nmodel &lt;- sk$LinearRegression()\nmodel$fit(X = as.matrix(mtcars[, c(\"disp\", \"hp\", \"wt\")]), \n         y = mtcars$mpg)\n\n# Get predictions and coefficients\npredictions &lt;- model$predict(as.matrix(mtcars[, c(\"disp\", \"hp\", \"wt\")]))\ncoefficients &lt;- data.frame(\n  Feature = c(\"Intercept\", \"disp\", \"hp\", \"wt\"),\n  Coefficient = c(model$intercept_, model$coef_)\n)\n\ncoefficients"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#introduction",
    "href": "posts/measuring-model-performance-gains-table.html#introduction",
    "title": "Evaluating Binary Classification Models Using Gains Tables",
    "section": "",
    "text": "In credit risk modeling and binary classification applications, analysts employ gains tables (also known as KS tables) as a fundamental tool for measuring and quantifying model performance. This tutorial dives into the construction and interpretion of gains tables using R. ## Theoretical Foundation: Understanding Gains Tables\nA gains table systematically discretizes the population (typically a validation or test dataset) into groups based on the model’s output predictions (probability scores, log odds, or risk scores). Each group conventionally represents 10% of the total population (deciles), though alternative binning strategies may be employed. The output presents summary statistics for each group and analyzes the cumulative distributions of events (defaults) and non-events to quantify the model’s discriminatory performance."
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#theoretical-foundation-understanding-gains-tables",
    "href": "posts/measuring-model-performance-gains-table.html#theoretical-foundation-understanding-gains-tables",
    "title": "Evaluating Binary Classification Models Using Gains Tables",
    "section": "Theoretical Foundation: Understanding Gains Tables",
    "text": "Theoretical Foundation: Understanding Gains Tables\nA gains table systematically discretizes the population (typically a validation or test dataset) into groups based on the model’s output predictions (probability scores, log odds, or risk scores). Each group conventionally represents 10% of the total population (deciles), though alternative binning strategies may be employed. The table presents comprehensive summary statistics for each group and analyzes the cumulative distributions of events (defaults) and non-events to quantify the model’s discriminatory performance."
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#package-dependencies",
    "href": "posts/measuring-model-performance-gains-table.html#package-dependencies",
    "title": "Evaluating Binary Classification Models Using Gains Tables",
    "section": "Package Dependencies",
    "text": "Package Dependencies\n\n# Load required packages\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(knitr)\nlibrary(scales)"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#dataset-acquisition-and-preparation",
    "href": "posts/measuring-model-performance-gains-table.html#dataset-acquisition-and-preparation",
    "title": "Evaluating Binary Classification Models Using Gains Tables",
    "section": "Dataset Acquisition and Preparation",
    "text": "Dataset Acquisition and Preparation\nThis tutorial utilizes a sample from the Lending Club dataset, which contains comprehensive loan information and associated outcomes suitable for credit risk modeling applications.\n\n# Load the sample data\nsample &lt;- read.csv(\"https://bit.ly/42ypcnJ\")\n\n# Check dimensions\ndim(sample)\n\n[1] 10000   153"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#target-variable-definition-and-event-classification",
    "href": "posts/measuring-model-performance-gains-table.html#target-variable-definition-and-event-classification",
    "title": "Evaluating Binary Classification Models Using Gains Tables",
    "section": "Target Variable Definition and Event Classification",
    "text": "Target Variable Definition and Event Classification\nThe initial step requires the creation of a binary target variable for modeling purposes. In this credit risk application, we identify borrowers who defaulted on their loan obligations.\n\n# Check unique loan statuses\nunique(sample$loan_status)\n\n[1] \"Fully Paid\"                                         \n[2] \"Current\"                                            \n[3] \"Charged Off\"                                        \n[4] \"Late (31-120 days)\"                                 \n[5] \"Late (16-30 days)\"                                  \n[6] \"In Grace Period\"                                    \n[7] \"Does not meet the credit policy. Status:Fully Paid\" \n[8] \"Does not meet the credit policy. Status:Charged Off\"\n\n# Define \"bad\" loans as those that are charged off\ncodes &lt;- c(\"Charged Off\", \"Does not meet the credit policy. Status:Charged Off\")\n\n# Create a binary flag for defaults\nsample %&lt;&gt;% mutate(bad_flag = ifelse(loan_status %in% codes, 1, 0))\n\n# Check overall event rates\nsample %&gt;% \n  summarise(events = sum(bad_flag == 1), \n            non_events = sum(bad_flag == 0)) %&gt;% \n  mutate(            event_rate = events/(events + non_events))\n\n  events non_events event_rate\n1   1162       8838     0.1162"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#predictive-model-development",
    "href": "posts/measuring-model-performance-gains-table.html#predictive-model-development",
    "title": "Evaluating Binary Classification Models Using Gains Tables",
    "section": "Predictive Model Development",
    "text": "Predictive Model Development\nSubsequently, we develop a logistic regression model to generate predictions that will serve as the foundation for gains table construction.\n\n# Replace NA values with a default value\nsample[is.na(sample)] &lt;- -1\n\n# Clean the data\nsample %&lt;&gt;% \n  # Remove cases where home ownership and payment plan are not reported\n  filter(!home_ownership %in% c(\"\", \"NONE\"),\n         pymnt_plan != \"\") %&gt;% \n  # Convert categorical variables to factors\n  mutate(home_ownership = factor(home_ownership), \n         pymnt_plan = factor(pymnt_plan))\n\n# Train-test split (70-30)\nidx &lt;- sample(1:nrow(sample), size = 0.7 * nrow(sample), replace = FALSE)\ntrain &lt;- sample[idx,]\ntest &lt;- sample[-idx,]\n\n\n# Build a logistic regression model\nmdl &lt;- glm(\n  formula = bad_flag ~ \n    loan_amnt + term + mths_since_last_delinq + total_pymnt + \n    home_ownership + acc_now_delinq + \n    inq_last_6mths + delinq_amnt + \n    mths_since_last_record + mths_since_recent_revol_delinq + \n    mths_since_last_major_derog + mths_since_recent_inq + \n    mths_since_recent_bc + num_accts_ever_120_pd,\n  family = \"binomial\", \n  data = train\n)\n\n# Generate predictions on the test set\ntest$pred &lt;- predict(mdl, newdata = test)"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#systematic-gains-table-construction",
    "href": "posts/measuring-model-performance-gains-table.html#systematic-gains-table-construction",
    "title": "Evaluating Binary Classification Models Using Gains Tables",
    "section": "Systematic Gains Table Construction",
    "text": "Systematic Gains Table Construction\nThe following section demonstrates the step-by-step construction of a comprehensive gains table through systematic binning and statistical analysis.\n\nPopulation Discretization into Risk Bins\n\n# Create deciles based on model predictions\nq &lt;- quantile(test$pred, probs = seq(0, 1, length.out = 11))\n\n# Add bins to test dataset\ntest$bins &lt;- cut(test$pred, breaks = q, include.lowest = TRUE, \n                right = TRUE, ordered_result = TRUE)\n\n# Check the bin levels (note they're in increasing order)\nlevels(test$bins)\n\n [1] \"[-5.42,-3.35]\" \"(-3.35,-2.91]\" \"(-2.91,-2.64]\" \"(-2.64,-2.39]\"\n [5] \"(-2.39,-2.21]\" \"(-2.21,-2.04]\" \"(-2.04,-1.85]\" \"(-1.85,-1.62]\"\n [9] \"(-1.62,-1.23]\" \"(-1.23,1.81]\" \n\n\n\n\nBasic Statistical Measures by Risk Segment\n\n# Create initial gains table with counts\ngains_table &lt;- test %&gt;% \n  group_by(bins) %&gt;% \n  summarise(total = n(), \n            events = sum(bad_flag == 1), \n            non_events = sum(bad_flag == 0))\n\n# Add event rate column\ngains_table %&lt;&gt;%\n  mutate(event_rate = percent(events / total, 0.1, 100))\n\n# Display the table\nkable(gains_table)\n\n\n\n\nbins\ntotal\nevents\nnon_events\nevent_rate\n\n\n\n\n[-5.42,-3.35]\n300\n5\n295\n1.7%\n\n\n(-3.35,-2.91]\n300\n10\n290\n3.3%\n\n\n(-2.91,-2.64]\n300\n12\n288\n4.0%\n\n\n(-2.64,-2.39]\n300\n16\n284\n5.3%\n\n\n(-2.39,-2.21]\n300\n22\n278\n7.3%\n\n\n(-2.21,-2.04]\n300\n35\n265\n11.7%\n\n\n(-2.04,-1.85]\n300\n49\n251\n16.3%\n\n\n(-1.85,-1.62]\n300\n48\n252\n16.0%\n\n\n(-1.62,-1.23]\n300\n56\n244\n18.7%\n\n\n(-1.23,1.81]\n300\n83\n217\n27.7%\n\n\n\n\n\n\n\nCumulative Distribution Analysis\n\n# Add population percentage and cumulative distributions\ngains_table %&lt;&gt;%\n  mutate(pop_pct = percent(total/sum(total), 0.1, 100), \n         \n         # Calculate cumulative percentages\n         c.events_pct = cumsum(events) / sum(events),\n         c.non_events_pct = cumsum(non_events) / sum(non_events))\n\n# Display the updated table\nkable(gains_table)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbins\ntotal\nevents\nnon_events\nevent_rate\npop_pct\nc.events_pct\nc.non_events_pct\n\n\n\n\n[-5.42,-3.35]\n300\n5\n295\n1.7%\n10.0%\n0.0148810\n0.1107357\n\n\n(-3.35,-2.91]\n300\n10\n290\n3.3%\n10.0%\n0.0446429\n0.2195946\n\n\n(-2.91,-2.64]\n300\n12\n288\n4.0%\n10.0%\n0.0803571\n0.3277027\n\n\n(-2.64,-2.39]\n300\n16\n284\n5.3%\n10.0%\n0.1279762\n0.4343093\n\n\n(-2.39,-2.21]\n300\n22\n278\n7.3%\n10.0%\n0.1934524\n0.5386637\n\n\n(-2.21,-2.04]\n300\n35\n265\n11.7%\n10.0%\n0.2976190\n0.6381381\n\n\n(-2.04,-1.85]\n300\n49\n251\n16.3%\n10.0%\n0.4434524\n0.7323574\n\n\n(-1.85,-1.62]\n300\n48\n252\n16.0%\n10.0%\n0.5863095\n0.8269520\n\n\n(-1.62,-1.23]\n300\n56\n244\n18.7%\n10.0%\n0.7529762\n0.9185435\n\n\n(-1.23,1.81]\n300\n83\n217\n27.7%\n10.0%\n1.0000000\n1.0000000\n\n\n\n\n\n\n\nAdvanced Performance Metrics Integration\n\n# Add KS statistic, capture rate, and cumulative event rate\ngains_table %&lt;&gt;%\n  mutate(\n    # KS statistic (difference between cumulative distributions)\n    ks = round(abs(c.events_pct - c.non_events_pct), 2), \n    \n    # Capture rate (percentage of total events captured)\n    cap_rate = percent(cumsum(events)/sum(events), 1, 100), \n    \n    # Cumulative event rate\n    c_event_rate = percent(cumsum(events)/cumsum(total), 0.1, 100), \n    \n    # Format percentage columns\n    c.events_pct = percent(c.events_pct, 0.1, 100),\n    c.non_events_pct = percent(c.non_events_pct, 0.1, 100))\n\n# Display the final table\nkable(gains_table)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbins\ntotal\nevents\nnon_events\nevent_rate\npop_pct\nc.events_pct\nc.non_events_pct\nks\ncap_rate\nc_event_rate\n\n\n\n\n[-5.42,-3.35]\n300\n5\n295\n1.7%\n10.0%\n1.5%\n11.1%\n0.10\n1%\n1.7%\n\n\n(-3.35,-2.91]\n300\n10\n290\n3.3%\n10.0%\n4.5%\n22.0%\n0.17\n4%\n2.5%\n\n\n(-2.91,-2.64]\n300\n12\n288\n4.0%\n10.0%\n8.0%\n32.8%\n0.25\n8%\n3.0%\n\n\n(-2.64,-2.39]\n300\n16\n284\n5.3%\n10.0%\n12.8%\n43.4%\n0.31\n13%\n3.6%\n\n\n(-2.39,-2.21]\n300\n22\n278\n7.3%\n10.0%\n19.3%\n53.9%\n0.35\n19%\n4.3%\n\n\n(-2.21,-2.04]\n300\n35\n265\n11.7%\n10.0%\n29.8%\n63.8%\n0.34\n30%\n5.6%\n\n\n(-2.04,-1.85]\n300\n49\n251\n16.3%\n10.0%\n44.3%\n73.2%\n0.29\n44%\n7.1%\n\n\n(-1.85,-1.62]\n300\n48\n252\n16.0%\n10.0%\n58.6%\n82.7%\n0.24\n59%\n8.2%\n\n\n(-1.62,-1.23]\n300\n56\n244\n18.7%\n10.0%\n75.3%\n91.9%\n0.17\n75%\n9.4%\n\n\n(-1.23,1.81]\n300\n83\n217\n27.7%\n10.0%\n100.0%\n100.0%\n0.00\n100%\n11.2%"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#reusable-function-development",
    "href": "posts/measuring-model-performance-gains-table.html#reusable-function-development",
    "title": "Evaluating Binary Classification Models Using Gains Tables",
    "section": "Reusable Function Development",
    "text": "Reusable Function Development\nThe following implementation encapsulates the gains table construction process within a comprehensive, reusable function suitable for any binary classification model evaluation:\n\ngains_table &lt;- function(act, pred, increasing = TRUE, nBins = 10) {\n  \n  # Create bins based on predictions\n  q &lt;- quantile(pred, probs = seq(0, 1, length.out = nBins + 1))\n  bins &lt;- cut(pred, breaks = q, include.lowest = TRUE, right = TRUE, ordered_result = TRUE)\n  \n  df &lt;- data.frame(act, pred, bins)\n  \n  df %&gt;% \n    # Group by bins and calculate statistics\n    group_by(bins) %&gt;% \n    summarise(total = n(), \n              events = sum(act == 1), \n              non_events = sum(act == 0)) %&gt;% \n    mutate(event_rate = percent(events / total, 0.1, 100)) %&gt;% \n    \n    # Sort the table based on the 'increasing' parameter\n    {if(increasing == TRUE) {\n      arrange(., bins)\n    } else {\n      arrange(., desc(bins))\n    }} %&gt;% \n    \n    # Add all performance metrics\n    mutate(pop_pct = percent(total/sum(total), 0.1, 100), \n           c.events_pct = cumsum(events) / sum(events),\n           c.non_events_pct = cumsum(non_events) / sum(non_events), \n           ks = round(abs(c.events_pct - c.non_events_pct), 2), \n           cap_rate = percent(cumsum(events)/sum(events), 1, 100), \n           c_event_rate = percent(cumsum(events)/cumsum(total), 0.1, 100), \n           c.events_pct = percent(c.events_pct, 0.1, 100),\n           c.non_events_pct = percent(c.non_events_pct, 0.1, 100))\n}\n\n\nFunction Implementation Demonstration\n\n# Generate a gains table with bins in descending order\ntab &lt;- gains_table(test$bad_flag, test$pred, FALSE, 10)\nkable(tab)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbins\ntotal\nevents\nnon_events\nevent_rate\npop_pct\nc.events_pct\nc.non_events_pct\nks\ncap_rate\nc_event_rate\n\n\n\n\n(-1.23,1.81]\n300\n83\n217\n27.7%\n10.0%\n24.7%\n8.1%\n0.17\n25%\n27.7%\n\n\n(-1.62,-1.23]\n300\n56\n244\n18.7%\n10.0%\n41.4%\n17.3%\n0.24\n41%\n23.2%\n\n\n(-1.85,-1.62]\n300\n48\n252\n16.0%\n10.0%\n55.7%\n26.8%\n0.29\n56%\n20.8%\n\n\n(-2.04,-1.85]\n300\n49\n251\n16.3%\n10.0%\n70.2%\n36.2%\n0.34\n70%\n19.7%\n\n\n(-2.21,-2.04]\n300\n35\n265\n11.7%\n10.0%\n80.7%\n46.1%\n0.35\n81%\n18.1%\n\n\n(-2.39,-2.21]\n300\n22\n278\n7.3%\n10.0%\n87.2%\n56.6%\n0.31\n87%\n16.3%\n\n\n(-2.64,-2.39]\n300\n16\n284\n5.3%\n10.0%\n92.0%\n67.2%\n0.25\n92%\n14.7%\n\n\n(-2.91,-2.64]\n300\n12\n288\n4.0%\n10.0%\n95.5%\n78.0%\n0.17\n96%\n13.4%\n\n\n(-3.35,-2.91]\n300\n10\n290\n3.3%\n10.0%\n98.5%\n88.9%\n0.10\n99%\n12.3%\n\n\n[-5.42,-3.35]\n300\n5\n295\n1.7%\n10.0%\n100.0%\n100.0%\n0.00\n100%\n11.2%"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#comprehensive-gains-table-interpretation",
    "href": "posts/measuring-model-performance-gains-table.html#comprehensive-gains-table-interpretation",
    "title": "Evaluating Binary Classification Models Using Gains Tables",
    "section": "Comprehensive Gains Table Interpretation",
    "text": "Comprehensive Gains Table Interpretation\nA properly constructed gains table provides multiple critical insights into model performance characteristics:\n\nMonotonicity Assessment: Event rates should demonstrate consistent increases (or decreases) across bins, confirming the model’s effectiveness in rank-ordering risk levels.\nPopulation Distribution Analysis: Consistent bin sizes (ideally ~10% each) indicate appropriate score distribution. Inconsistent sizes suggest score clustering, which may complicate threshold determination.\nKolmogorov-Smirnov (KS) Statistic: The maximum KS value represents the model’s discriminatory power. Higher values (approaching 1.0) indicate superior separation between positive and negative cases.\nCapture Rate Analysis: Demonstrates the percentage of total events captured at each threshold, essential for operational decision-making.\nCumulative Event Rate Evaluation: Indicates the event rate among all cases up to each bin, facilitating approval threshold establishment."
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#operational-applications-in-credit-risk-management",
    "href": "posts/measuring-model-performance-gains-table.html#operational-applications-in-credit-risk-management",
    "title": "Evaluating Binary Classification Models Using Gains Tables",
    "section": "Operational Applications in Credit Risk Management",
    "text": "Operational Applications in Credit Risk Management\nGains tables serve multiple critical functions in credit risk management environments:\n\nThreshold Optimization: Identification of appropriate score thresholds for automated approval or rejection decisions.\nTiered Strategy Development: Construction of multi-tier decision strategies (approve, manual review, decline) based on quantified risk levels.\nModel Performance Monitoring: Longitudinal tracking of model performance through comparison of actual versus expected distributions.\nComparative Model Evaluation: Systematic comparison of alternative models through KS statistics and capture rate analysis."
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#conclusion",
    "href": "posts/measuring-model-performance-gains-table.html#conclusion",
    "title": "Evaluating Binary Classification Models Using Gains Tables",
    "section": "Conclusion",
    "text": "Conclusion\nThe gains table represents a fundamental analytical tool for evaluating binary classification models, particularly within credit risk applications. By providing a structured analytical framework for assessing model discrimination across the complete score distribution, it enables practitioners to make informed decisions regarding model quality assessment and operational implementation strategies. The systematic approach demonstrated in this tutorial ensures consistent, reliable model evaluation suitable for production environments."
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#key-takeaways",
    "href": "posts/measuring-model-performance-gains-table.html#key-takeaways",
    "title": "Evaluating Binary Classification Models Using Gains Tables",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nThis tutorial has demonstrated the comprehensive construction and interpretation of gains tables for binary classification model evaluation. The essential insights include:\n\nGains tables provide systematic model evaluation through population discretization and cumulative distribution analysis\nThe KS statistic serves as a primary discriminatory power measure, with higher values indicating superior model performance\nMonotonicity assessment confirms effective risk rank-ordering across the complete score distribution\nCapture rate analysis enables operational threshold optimization for automated decision systems\nFunction encapsulation ensures reproducible analysis across different models and datasets\nThe methodology scales effectively for production environments requiring consistent model evaluation standards\n\nThese analytical techniques establish a foundation for robust model validation and operational decision-making in credit risk management and broader binary classification applications."
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#dataset-preparation",
    "href": "posts/measuring-model-performance-gains-table.html#dataset-preparation",
    "title": "Evaluating Binary Classification Models Using Gains Tables",
    "section": "Dataset Preparation",
    "text": "Dataset Preparation\nThis tutorial utilizes a sample from the Lending Club dataset, which contains comprehensive loan information and associated outcomes suitable for credit risk modeling applications.\n\n# Load the sample data\nsample &lt;- read.csv(\"https://bit.ly/42ypcnJ\")\n\n# Check dimensions\ndim(sample)\n\n[1] 10000   153"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#target-definition",
    "href": "posts/measuring-model-performance-gains-table.html#target-definition",
    "title": "Evaluating Binary Classification Models Using Gains Tables",
    "section": "Target Definition",
    "text": "Target Definition\nThe initial step requires the creation of a binary target variable for modeling purposes. In this credit risk application, we identify borrowers who defaulted on their loan obligations.\n\n# Check unique loan statuses\nunique(sample$loan_status)\n\n[1] \"Fully Paid\"                                         \n[2] \"Current\"                                            \n[3] \"Charged Off\"                                        \n[4] \"Late (31-120 days)\"                                 \n[5] \"Late (16-30 days)\"                                  \n[6] \"In Grace Period\"                                    \n[7] \"Does not meet the credit policy. Status:Fully Paid\" \n[8] \"Does not meet the credit policy. Status:Charged Off\"\n\n# Define \"bad\" loans as those that are charged off\ncodes &lt;- c(\"Charged Off\", \"Does not meet the credit policy. Status:Charged Off\")\n\n# Create a binary flag for defaults\nsample %&lt;&gt;% mutate(bad_flag = ifelse(loan_status %in% codes, 1, 0))\n\n# Check overall event rates\nsample %&gt;% \n  summarise(events = sum(bad_flag == 1), \n            non_events = sum(bad_flag == 0)) %&gt;% \n  mutate(            event_rate = events/(events + non_events))\n\n  events non_events event_rate\n1   1162       8838     0.1162"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#model-development",
    "href": "posts/measuring-model-performance-gains-table.html#model-development",
    "title": "Evaluating Binary Classification Models Using Gains Tables",
    "section": "Model Development",
    "text": "Model Development\nSubsequently, we develop a logistic regression model to generate predictions that will serve as the foundation for gains table construction.\n\n# Replace NA values with a default value\nsample[is.na(sample)] &lt;- -1\n\n# Clean the data\nsample %&lt;&gt;% \n  # Remove cases where home ownership and payment plan are not reported\n  filter(!home_ownership %in% c(\"\", \"NONE\"),\n         pymnt_plan != \"\") %&gt;% \n  # Convert categorical variables to factors\n  mutate(home_ownership = factor(home_ownership), \n         pymnt_plan = factor(pymnt_plan))\n\n# Train-test split (70-30)\nidx &lt;- sample(1:nrow(sample), size = 0.7 * nrow(sample), replace = FALSE)\ntrain &lt;- sample[idx,]\ntest &lt;- sample[-idx,]\n\n\n# Build a logistic regression model\nmdl &lt;- glm(\n  formula = bad_flag ~ \n    loan_amnt + term + mths_since_last_delinq + total_pymnt + \n    home_ownership + acc_now_delinq + \n    inq_last_6mths + delinq_amnt + \n    mths_since_last_record + mths_since_recent_revol_delinq + \n    mths_since_last_major_derog + mths_since_recent_inq + \n    mths_since_recent_bc + num_accts_ever_120_pd,\n  family = \"binomial\", \n  data = train\n)\n\n# Generate predictions on the test set\ntest$pred &lt;- predict(mdl, newdata = test)"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#gains-table-construction",
    "href": "posts/measuring-model-performance-gains-table.html#gains-table-construction",
    "title": "Evaluating Binary Classification Models Using Gains Tables",
    "section": "Gains Table Construction",
    "text": "Gains Table Construction\nThe following section demonstrates the step-by-step construction of a comprehensive gains table through systematic binning and statistical analysis.\n\nPopulation Discretization into Bins\n\n# Create deciles based on model predictions\nq &lt;- quantile(test$pred, probs = seq(0, 1, length.out = 11))\n\n# Add bins to test dataset\ntest$bins &lt;- cut(test$pred, breaks = q, include.lowest = TRUE, \n                right = TRUE, ordered_result = TRUE)\n\n# Check the bin levels (note they're in increasing order)\nlevels(test$bins)\n\n [1] \"[-5.33,-3.3]\"  \"(-3.3,-2.9]\"   \"(-2.9,-2.66]\"  \"(-2.66,-2.47]\"\n [5] \"(-2.47,-2.28]\" \"(-2.28,-2.1]\"  \"(-2.1,-1.91]\"  \"(-1.91,-1.65]\"\n [9] \"(-1.65,-1.24]\" \"(-1.24,2.03]\" \n\n\n\n\nBasic Statistical Measures by Segment\n\n# Create initial gains table with counts\ngains_table &lt;- test %&gt;% \n  group_by(bins) %&gt;% \n  summarise(total = n(), \n            events = sum(bad_flag == 1), \n            non_events = sum(bad_flag == 0))\n\n# Add event rate column\ngains_table %&lt;&gt;%\n  mutate(event_rate = percent(events / total, 0.1, 100))\n\n# Display the table\nkable(gains_table)\n\n\n\n\nbins\ntotal\nevents\nnon_events\nevent_rate\n\n\n\n\n[-5.33,-3.3]\n300\n2\n298\n0.7%\n\n\n(-3.3,-2.9]\n300\n9\n291\n3.0%\n\n\n(-2.9,-2.66]\n300\n10\n290\n3.3%\n\n\n(-2.66,-2.47]\n300\n20\n280\n6.7%\n\n\n(-2.47,-2.28]\n300\n37\n263\n12.3%\n\n\n(-2.28,-2.1]\n300\n42\n258\n14.0%\n\n\n(-2.1,-1.91]\n300\n48\n252\n16.0%\n\n\n(-1.91,-1.65]\n300\n53\n247\n17.7%\n\n\n(-1.65,-1.24]\n300\n79\n221\n26.3%\n\n\n(-1.24,2.03]\n300\n80\n220\n26.7%\n\n\n\n\n\n\n\nCumulative Distribution\n\n# Add population percentage and cumulative distributions\ngains_table %&lt;&gt;%\n  mutate(pop_pct = percent(total/sum(total), 0.1, 100), \n         \n         # Calculate cumulative percentages\n         c.events_pct = cumsum(events) / sum(events),\n         c.non_events_pct = cumsum(non_events) / sum(non_events))\n\n# Display the updated table\nkable(gains_table)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbins\ntotal\nevents\nnon_events\nevent_rate\npop_pct\nc.events_pct\nc.non_events_pct\n\n\n\n\n[-5.33,-3.3]\n300\n2\n298\n0.7%\n10.0%\n0.0052632\n0.1137405\n\n\n(-3.3,-2.9]\n300\n9\n291\n3.0%\n10.0%\n0.0289474\n0.2248092\n\n\n(-2.9,-2.66]\n300\n10\n290\n3.3%\n10.0%\n0.0552632\n0.3354962\n\n\n(-2.66,-2.47]\n300\n20\n280\n6.7%\n10.0%\n0.1078947\n0.4423664\n\n\n(-2.47,-2.28]\n300\n37\n263\n12.3%\n10.0%\n0.2052632\n0.5427481\n\n\n(-2.28,-2.1]\n300\n42\n258\n14.0%\n10.0%\n0.3157895\n0.6412214\n\n\n(-2.1,-1.91]\n300\n48\n252\n16.0%\n10.0%\n0.4421053\n0.7374046\n\n\n(-1.91,-1.65]\n300\n53\n247\n17.7%\n10.0%\n0.5815789\n0.8316794\n\n\n(-1.65,-1.24]\n300\n79\n221\n26.3%\n10.0%\n0.7894737\n0.9160305\n\n\n(-1.24,2.03]\n300\n80\n220\n26.7%\n10.0%\n1.0000000\n1.0000000\n\n\n\n\n\n\n\nPerformance Metrics\n\n# Add KS statistic, capture rate, and cumulative event rate\ngains_table %&lt;&gt;%\n  mutate(\n    # KS statistic (difference between cumulative distributions)\n    ks = round(abs(c.events_pct - c.non_events_pct), 2), \n    \n    # Capture rate (percentage of total events captured)\n    cap_rate = percent(cumsum(events)/sum(events), 1, 100), \n    \n    # Cumulative event rate\n    c_event_rate = percent(cumsum(events)/cumsum(total), 0.1, 100), \n    \n    # Format percentage columns\n    c.events_pct = percent(c.events_pct, 0.1, 100),\n    c.non_events_pct = percent(c.non_events_pct, 0.1, 100))\n\n# Display the final table\nkable(gains_table)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbins\ntotal\nevents\nnon_events\nevent_rate\npop_pct\nc.events_pct\nc.non_events_pct\nks\ncap_rate\nc_event_rate\n\n\n\n\n[-5.33,-3.3]\n300\n2\n298\n0.7%\n10.0%\n0.5%\n11.4%\n0.11\n1%\n0.7%\n\n\n(-3.3,-2.9]\n300\n9\n291\n3.0%\n10.0%\n2.9%\n22.5%\n0.20\n3%\n1.8%\n\n\n(-2.9,-2.66]\n300\n10\n290\n3.3%\n10.0%\n5.5%\n33.5%\n0.28\n6%\n2.3%\n\n\n(-2.66,-2.47]\n300\n20\n280\n6.7%\n10.0%\n10.8%\n44.2%\n0.33\n11%\n3.4%\n\n\n(-2.47,-2.28]\n300\n37\n263\n12.3%\n10.0%\n20.5%\n54.3%\n0.34\n21%\n5.2%\n\n\n(-2.28,-2.1]\n300\n42\n258\n14.0%\n10.0%\n31.6%\n64.1%\n0.33\n32%\n6.7%\n\n\n(-2.1,-1.91]\n300\n48\n252\n16.0%\n10.0%\n44.2%\n73.7%\n0.30\n44%\n8.0%\n\n\n(-1.91,-1.65]\n300\n53\n247\n17.7%\n10.0%\n58.2%\n83.2%\n0.25\n58%\n9.2%\n\n\n(-1.65,-1.24]\n300\n79\n221\n26.3%\n10.0%\n78.9%\n91.6%\n0.13\n79%\n11.1%\n\n\n(-1.24,2.03]\n300\n80\n220\n26.7%\n10.0%\n100.0%\n100.0%\n0.00\n100%\n12.7%"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#reusable-function",
    "href": "posts/measuring-model-performance-gains-table.html#reusable-function",
    "title": "Evaluating Binary Classification Models Using Gains Tables",
    "section": "Reusable Function",
    "text": "Reusable Function\nThe following implementation encapsulates the gains table construction process within a comprehensive, reusable function suitable for any binary classification model evaluation:\n\ngains_table &lt;- function(act, pred, increasing = TRUE, nBins = 10) {\n  \n  # Create bins based on predictions\n  q &lt;- quantile(pred, probs = seq(0, 1, length.out = nBins + 1))\n  bins &lt;- cut(pred, breaks = q, include.lowest = TRUE, right = TRUE, ordered_result = TRUE)\n  \n  df &lt;- data.frame(act, pred, bins)\n  \n  df %&gt;% \n    # Group by bins and calculate statistics\n    group_by(bins) %&gt;% \n    summarise(total = n(), \n              events = sum(act == 1), \n              non_events = sum(act == 0)) %&gt;% \n    mutate(event_rate = percent(events / total, 0.1, 100)) %&gt;% \n    \n    # Sort the table based on the 'increasing' parameter\n    {if(increasing == TRUE) {\n      arrange(., bins)\n    } else {\n      arrange(., desc(bins))\n    }} %&gt;% \n    \n    # Add all performance metrics\n    mutate(pop_pct = percent(total/sum(total), 0.1, 100), \n           c.events_pct = cumsum(events) / sum(events),\n           c.non_events_pct = cumsum(non_events) / sum(non_events), \n           ks = round(abs(c.events_pct - c.non_events_pct), 2), \n           cap_rate = percent(cumsum(events)/sum(events), 1, 100), \n           c_event_rate = percent(cumsum(events)/cumsum(total), 0.1, 100), \n           c.events_pct = percent(c.events_pct, 0.1, 100),\n           c.non_events_pct = percent(c.non_events_pct, 0.1, 100))\n}\n\n\nFunction Implementation\n\n# Generate a gains table with bins in descending order\ntab &lt;- gains_table(test$bad_flag, test$pred, FALSE, 10)\nkable(tab)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbins\ntotal\nevents\nnon_events\nevent_rate\npop_pct\nc.events_pct\nc.non_events_pct\nks\ncap_rate\nc_event_rate\n\n\n\n\n(-1.24,2.03]\n300\n80\n220\n26.7%\n10.0%\n21.1%\n8.4%\n0.13\n21%\n26.7%\n\n\n(-1.65,-1.24]\n300\n79\n221\n26.3%\n10.0%\n41.8%\n16.8%\n0.25\n42%\n26.5%\n\n\n(-1.91,-1.65]\n300\n53\n247\n17.7%\n10.0%\n55.8%\n26.3%\n0.30\n56%\n23.6%\n\n\n(-2.1,-1.91]\n300\n48\n252\n16.0%\n10.0%\n68.4%\n35.9%\n0.33\n68%\n21.7%\n\n\n(-2.28,-2.1]\n300\n42\n258\n14.0%\n10.0%\n79.5%\n45.7%\n0.34\n79%\n20.1%\n\n\n(-2.47,-2.28]\n300\n37\n263\n12.3%\n10.0%\n89.2%\n55.8%\n0.33\n89%\n18.8%\n\n\n(-2.66,-2.47]\n300\n20\n280\n6.7%\n10.0%\n94.5%\n66.5%\n0.28\n94%\n17.1%\n\n\n(-2.9,-2.66]\n300\n10\n290\n3.3%\n10.0%\n97.1%\n77.5%\n0.20\n97%\n15.4%\n\n\n(-3.3,-2.9]\n300\n9\n291\n3.0%\n10.0%\n99.5%\n88.6%\n0.11\n99%\n14.0%\n\n\n[-5.33,-3.3]\n300\n2\n298\n0.7%\n10.0%\n100.0%\n100.0%\n0.00\n100%\n12.7%"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#interpretation",
    "href": "posts/measuring-model-performance-gains-table.html#interpretation",
    "title": "Evaluating Binary Classification Models Using Gains Tables",
    "section": "Interpretation",
    "text": "Interpretation\nA properly constructed gains table provides multiple critical insights into model performance characteristics:\n\nMonotonicity Assessment: Event rates should demonstrate consistent increases (or decreases) across bins, confirming the model’s effectiveness in rank-ordering risk levels.\nPopulation Distribution: Consistent bin sizes (ideally ~10% each) indicate appropriate score distribution. Inconsistent sizes suggest score clustering, which may complicate threshold determination.\nKolmogorov-Smirnov (KS) Statistic: The maximum KS value represents the model’s discriminatory power. Higher values (approaching 1.0) indicate superior separation between positive and negative cases.\nCapture Rate: Demonstrates the percentage of total events captured at each threshold, essential for operational decision-making.\nCumulative Event Rate: Indicates the event rate among all cases up to each bin, facilitating approval threshold establishment."
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#applications-in-credit-risk-analytics",
    "href": "posts/measuring-model-performance-gains-table.html#applications-in-credit-risk-analytics",
    "title": "Evaluating Binary Classification Models Using Gains Tables",
    "section": "Applications in Credit Risk Analytics",
    "text": "Applications in Credit Risk Analytics\nGains tables serve multiple critical functions in credit risk management environments:\n\nThreshold Optimization: Identification of appropriate score thresholds for automated approval or rejection decisions.\nTiered Strategy Development: Construction of multi-tier decision strategies (approve, manual review, decline) based on quantified risk levels.\nModel Performance Monitoring: Longitudinal tracking of model performance through comparison of actual versus expected distributions.\nComparative Model Evaluation: Systematic comparison of alternative models through KS statistics and capture rate analysis."
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#introduction",
    "href": "posts/modeling-with-tidymodels.html#introduction",
    "title": "Comprehensive Credit Scoring Model Development Using tidymodels",
    "section": "",
    "text": "The tidymodels framework provides a comprehensive and cohesive ecosystem of packages for modeling and machine learning in R, adhering to tidy design principles. This tutorial demonstrates the development of a credit scoring model using the tidymodels framework, encompassing the complete machine learning workflow from data preparation through model evaluation.\nCredit scoring models serve as fundamental tools employed by financial institutions to assess borrower creditworthiness. These models predict default probability based on borrower characteristics and loan attributes. Effective credit scoring models must demonstrate strong discriminatory power between high-risk and low-risk borrowers, exhibit proper calibration, and provide interpretable insights for regulatory compliance and business decision-making."
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#package-dependencies-and-environment-setup",
    "href": "posts/modeling-with-tidymodels.html#package-dependencies-and-environment-setup",
    "title": "Comprehensive Credit Scoring Model Development Using tidymodels",
    "section": "Package Dependencies and Environment Setup",
    "text": "Package Dependencies and Environment Setup\nThis tutorial requires several specialized R packages for machine learning, data visualization, and model evaluation. The following packages must be installed and loaded prior to implementation:\n\nlibrary(tidymodels)\nlibrary(xgboost)\nlibrary(vip)        # For variable importance\nlibrary(stringr)    # For string manipulation functions\nlibrary(probably)   # For calibration plots\nlibrary(ROSE)       # For imbalanced data visualization\nlibrary(corrplot)   # For correlation visualization"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#synthetic-credit-dataset-generation",
    "href": "posts/modeling-with-tidymodels.html#synthetic-credit-dataset-generation",
    "title": "Comprehensive Credit Scoring Model Development Using tidymodels",
    "section": "Synthetic Credit Dataset Generation",
    "text": "Synthetic Credit Dataset Generation\nThis tutorial employs a simulated credit dataset incorporating realistic variable distributions commonly encountered in credit scoring applications. The synthetic data includes demographic information, loan characteristics, and credit history variables with appropriate statistical relationships.\n\nset.seed(123)\nn &lt;- 10000  # Larger sample size for more realistic modeling\n\n# Create base features with realistic distributions\ndata &lt;- tibble(\n  customer_id = paste0(\"CUS\", formatC(1:n, width = 6, format = \"d\", flag = \"0\")),\n  \n  # Demographics - with realistic age distribution for credit applicants\n  age = pmax(18, pmin(80, round(rnorm(n, 38, 13)))),\n  income = pmax(12000, round(rlnorm(n, log(52000), 0.8))),\n  employment_length = pmax(0, round(rexp(n, 1/6))),  # Exponential distribution for job tenure\n  home_ownership = sample(c(\"RENT\", \"MORTGAGE\", \"OWN\"), n, replace = TRUE, prob = c(0.45, 0.40, 0.15)),\n  \n  # Loan characteristics - with more realistic correlations\n  loan_amount = round(rlnorm(n, log(15000), 0.7) / 100) * 100,  # Log-normal for loan amounts\n  loan_term = sample(c(36, 60, 120), n, replace = TRUE, prob = c(0.6, 0.3, 0.1)),\n  \n  # Credit history - with more realistic distributions\n  credit_score = round(pmin(850, pmax(300, rnorm(n, 700, 90)))),\n  dti_ratio = pmax(0, pmin(65, rlnorm(n, log(20), 0.4))),  # Debt-to-income ratio\n  delinq_2yrs = rpois(n, 0.4),  # Number of delinquencies in past 2 years\n  inq_last_6mths = rpois(n, 0.7),  # Number of inquiries in last 6 months\n  open_acc = pmax(1, round(rnorm(n, 10, 4))),  # Number of open accounts\n  pub_rec = rbinom(n, 2, 0.06),  # Number of public records\n  revol_util = pmin(100, pmax(0, rnorm(n, 40, 20))),  # Revolving utilization\n  total_acc = pmax(open_acc, open_acc + round(rnorm(n, 8, 6)))  # Total accounts\n)\n\n# Add realistic correlations between variables\ndata &lt;- data %&gt;%\n  mutate(\n    # Interest rate depends on credit score and loan term\n    interest_rate = 25 - (credit_score - 300) * (15/550) + \n                    ifelse(loan_term == 36, -1, ifelse(loan_term == 60, 0, 1.5)) +\n                    rnorm(n, 0, 1.5),\n    \n    # Loan purpose with realistic probabilities\n    loan_purpose = sample(\n      c(\"debt_consolidation\", \"credit_card\", \"home_improvement\", \"major_purchase\", \"medical\", \"other\"), \n      n, replace = TRUE, \n      prob = c(0.45, 0.25, 0.10, 0.08, 0.07, 0.05)\n    ),\n    \n    # Add some derived features that have predictive power\n    payment_amount = (loan_amount * (interest_rate/100/12) * (1 + interest_rate/100/12)^loan_term) / \n                    ((1 + interest_rate/100/12)^loan_term - 1),\n    payment_to_income_ratio = (payment_amount * 12) / income\n  )\n\n# Create a more realistic default probability model with non-linear effects\nlogit_default &lt;- with(data, {\n  -4.5 +  # Base intercept for ~10% default rate\n    -0.03 * (age - 18) +  # Age effect (stronger for younger borrowers)\n    -0.2 * log(income/10000) +  # Log-transformed income effect\n    -0.08 * employment_length +  # Employment length effect\n    ifelse(home_ownership == \"OWN\", -0.7, ifelse(home_ownership == \"MORTGAGE\", -0.3, 0)) +  # Home ownership\n    0.3 * log(loan_amount/1000) +  # Log-transformed loan amount\n    ifelse(loan_term == 36, 0, ifelse(loan_term == 60, 0.4, 0.8)) +  # Loan term\n    0.15 * interest_rate +  # Interest rate effect\n    ifelse(loan_purpose == \"debt_consolidation\", 0.5, \n           ifelse(loan_purpose == \"credit_card\", 0.4, \n                  ifelse(loan_purpose == \"medical\", 0.6, 0))) +  # Loan purpose\n    -0.01 * (credit_score - 300) +  # Credit score (stronger effect at lower scores)\n    0.06 * dti_ratio +  # DTI ratio effect\n    0.4 * delinq_2yrs +  # Delinquencies effect (stronger effect for first delinquency)\n    0.3 * inq_last_6mths +  # Inquiries effect\n    -0.1 * log(open_acc + 1) +  # Open accounts (log-transformed)\n    0.8 * pub_rec +  # Public records (strong effect)\n    0.02 * revol_util +  # Revolving utilization\n    1.2 * payment_to_income_ratio +  # Payment to income ratio (strong effect)\n    rnorm(n, 0, 0.8)  # Add some noise for realistic variation\n})\n\n# Generate default flag with realistic default rate\nprob_default &lt;- plogis(logit_default)\ndata$default &lt;- factor(rbinom(n, 1, prob_default), levels = c(0, 1), labels = c(\"no\", \"yes\"))\n\n# Check class distribution\ntable(data$default)\n\n\n  no  yes \n9425  575 \n\n\n\nprop.table(table(data$default))\n\n\n    no    yes \n0.9425 0.0575 \n\n\n\n# Visualize the default rate\nggplot(data, aes(x = default, fill = default)) +\n  geom_bar(aes(y = ..prop.., group = 1)) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"Class Distribution in Credit Dataset\",\n       y = \"Percentage\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Visualize the relationship between key variables and default rate\nggplot(data, aes(x = credit_score, y = as.numeric(default) - 1)) +\n  geom_smooth(method = \"loess\") +\n  labs(title = \"Default Rate by Credit Score\", \n       x = \"Credit Score\", y = \"Default Probability\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Examine correlation between numeric predictors\ncredit_cors &lt;- data %&gt;%\n  select(age, income, employment_length, loan_amount, interest_rate, \n         credit_score, dti_ratio, delinq_2yrs, revol_util, payment_to_income_ratio) %&gt;%\n  cor()\n\ncorrplot(credit_cors, method = \"circle\", type = \"upper\", \n         tl.col = \"black\", tl.srt = 45, tl.cex = 0.7)"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#stratified-data-partitioning-strategy",
    "href": "posts/modeling-with-tidymodels.html#stratified-data-partitioning-strategy",
    "title": "Comprehensive Credit Scoring Model Development Using tidymodels",
    "section": "Stratified Data Partitioning Strategy",
    "text": "Stratified Data Partitioning Strategy\nCredit default datasets typically exhibit class imbalance, with default events representing the minority class. We implement stratified sampling to preserve the class distribution across training, validation, and test partitions.\n\n# Create initial train/test split (80/20)\nset.seed(456)\ninitial_split &lt;- initial_split(data, prop = 0.8, strata = default)\ntrain_data &lt;- training(initial_split)\ntest_data &lt;- testing(initial_split)\n\n# Create validation set from training data (75% train, 25% validation)\nset.seed(789)\nvalidation_split &lt;- initial_split(train_data, prop = 0.75, strata = default)\n\n# Check class imbalance in training data\ntrain_class_counts &lt;- table(training(validation_split)$default)\ntrain_class_props &lt;- prop.table(train_class_counts)\n\ncat(\"Training data class distribution:\\n\")\n\nTraining data class distribution:\n\nprint(train_class_counts)\n\n\n  no  yes \n5661  339 \n\ncat(\"\\nPercentage:\\n\")\n\n\nPercentage:\n\nprint(train_class_props * 100)\n\n\n   no   yes \n94.35  5.65 \n\n# Visualize class imbalance\nROSE::roc.curve(training(validation_split)$default == \"yes\", \n                training(validation_split)$credit_score,\n                plotit = TRUE,                main = \"ROC Curve for Credit Score Alone\")\n\n\n\n\n\n\n\n\nArea under the curve (AUC): 0.753"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#advanced-feature-engineering-and-preprocessing-pipeline",
    "href": "posts/modeling-with-tidymodels.html#advanced-feature-engineering-and-preprocessing-pipeline",
    "title": "Comprehensive Credit Scoring Model Development Using tidymodels",
    "section": "Advanced Feature Engineering and Preprocessing Pipeline",
    "text": "Advanced Feature Engineering and Preprocessing Pipeline\nThe following section demonstrates the development of a comprehensive preprocessing recipe incorporating domain-specific feature engineering techniques relevant to credit risk modeling, including class imbalance handling strategies.\n\n# Examine the distributions of key variables\npar(mfrow = c(2, 2))\nhist(training(validation_split)$credit_score, \n     main = \"Credit Score Distribution\", xlab = \"Credit Score\")\n\nhist(training(validation_split)$dti_ratio, \n     main = \"DTI Ratio Distribution\", xlab = \"DTI Ratio\")\n\nhist(training(validation_split)$payment_to_income_ratio, \n     main = \"Payment to Income Ratio\", \n     xlab = \"Payment to Income Ratio\")\n\nhist(log(training(validation_split)$income), \n     main = \"Log Income Distribution\", xlab = \"Log Income\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\n# Create a comprehensive recipe with domain knowledge\ncredit_recipe &lt;- recipe(default ~ ., data = training(validation_split)) %&gt;%\n  # Remove ID column\n  step_rm(customer_id) %&gt;%\n  \n  # Convert categorical variables to factors\n  step_string2factor(home_ownership, loan_purpose) %&gt;%\n  \n  # Create additional domain-specific features\n  step_mutate(\n    # We already have payment_to_income_ratio from data generation\n    # Add more credit risk indicators\n    credit_utilization = revol_util / 100,\n    acc_to_age_ratio = total_acc / age,\n    delinq_per_acc = ifelse(total_acc &gt; 0, delinq_2yrs / total_acc, 0),\n    inq_rate = inq_last_6mths / (open_acc + 0.1),  # Inquiry rate relative to open accounts\n    term_factor = loan_term / 12,  # Term in years\n    log_income = log(income),  # Log transform income\n    log_loan = log(loan_amount),  # Log transform loan amount\n    payment_ratio = payment_amount / (income / 12),  # Monthly payment to monthly income\n    util_to_income = (revol_util / 100) * (dti_ratio / 100)  # Interaction term\n  ) %&gt;%\n  \n  # Handle categorical variables\n  step_dummy(all_nominal_predictors()) %&gt;%\n  \n  # Impute missing values (if any)\n  step_impute_median(all_numeric_predictors()) %&gt;%\n  \n  # Transform highly skewed variables\n  step_YeoJohnson(income, loan_amount, payment_amount) %&gt;%\n  \n  # Remove highly correlated predictors\n  step_corr(all_numeric_predictors(), threshold = 0.85) %&gt;%\n  \n  # Normalize numeric predictors\n  step_normalize(all_numeric_predictors()) %&gt;%\n  \n  # Remove zero-variance predictors\n  step_zv(all_predictors())\n\n# Prep the recipe to examine the steps\nprepped_recipe &lt;- prep(credit_recipe)\nprepped_recipe\n\n# Check the transformed data\nrecipe_data &lt;- bake(prepped_recipe, new_data = NULL)\nglimpse(recipe_data)\n\nRows: 6,000\nColumns: 27\n$ age                             &lt;dbl&gt; -0.58938716, 1.60129128, 0.05970275, 0…\n$ employment_length               &lt;dbl&gt; 1.01545119, 0.17764322, 2.35594395, -0…\n$ loan_amount                     &lt;dbl&gt; -0.30772032, -1.64018559, 0.01897785, …\n$ credit_score                    &lt;dbl&gt; -1.66355858, -1.60583467, 0.02197934, …\n$ dti_ratio                       &lt;dbl&gt; -1.10745919, -0.21368748, -1.26746777,…\n$ delinq_2yrs                     &lt;dbl&gt; -0.6423758, -0.6423758, -0.6423758, -0…\n$ inq_last_6mths                  &lt;dbl&gt; -0.8324634, 1.5511111, -0.8324634, -0.…\n$ open_acc                        &lt;dbl&gt; -0.516984456, -1.282635289, -1.2826352…\n$ pub_rec                         &lt;dbl&gt; -0.3429372, -0.3429372, 2.7652549, -0.…\n$ total_acc                       &lt;dbl&gt; 0.39969129, 0.54625046, -0.47966374, 0…\n$ interest_rate                   &lt;dbl&gt; 1.47394527, 1.51546694, -0.11980960, 0…\n$ default                         &lt;fct&gt; no, no, no, no, no, no, no, no, yes, n…\n$ credit_utilization              &lt;dbl&gt; -1.841097091, 2.786415973, -1.09378697…\n$ acc_to_age_ratio                &lt;dbl&gt; 0.50174733, -0.54849606, -0.52980631, …\n$ delinq_per_acc                  &lt;dbl&gt; -0.44991964, -0.44991964, -0.44991964,…\n$ inq_rate                        &lt;dbl&gt; -0.520358012, 1.759991048, -0.52035801…\n$ term_factor                     &lt;dbl&gt; 0.3219163, -0.6274559, 0.3219163, -0.6…\n$ log_income                      &lt;dbl&gt; 2.44464243, 0.95097048, -0.59583257, 0…\n$ payment_ratio                   &lt;dbl&gt; -0.70055763, -0.66316187, -0.18762635,…\n$ util_to_income                  &lt;dbl&gt; -1.4271447, 1.7533431, -1.1717989, -1.…\n$ home_ownership_OWN              &lt;dbl&gt; -0.4238857, -0.4238857, -0.4238857, -0…\n$ home_ownership_RENT             &lt;dbl&gt; -0.8977787, -0.8977787, 1.1136746, 1.1…\n$ loan_purpose_debt_consolidation &lt;dbl&gt; 1.1204596, -0.8923421, -0.8923421, -0.…\n$ loan_purpose_home_improvement   &lt;dbl&gt; -0.3277222, -0.3277222, 3.0508567, -0.…\n$ loan_purpose_major_purchase     &lt;dbl&gt; -0.2968579, -0.2968579, -0.2968579, -0…\n$ loan_purpose_medical            &lt;dbl&gt; -0.299178, -0.299178, -0.299178, -0.29…\n$ loan_purpose_other              &lt;dbl&gt; -0.2208157, -0.2208157, -0.2208157, -0…\n\n# Verify class balance after SMOTE\ntable(recipe_data$default)\n\n\n  no  yes \n5661  339 \n\n\nThe feature engineering pipeline incorporates domain knowledge specific to credit risk modeling:\n\nDerived Risk Indicators: Creation of features capturing payment capacity, credit utilization, and borrower stability\nDistributional Transformations: Application of Yeo-Johnson transformations to address highly skewed variables\nStandardization: Normalization of all numeric predictors for consistent scaling across features"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#model-architecture-definition-and-hyperparameter-optimization",
    "href": "posts/modeling-with-tidymodels.html#model-architecture-definition-and-hyperparameter-optimization",
    "title": "Comprehensive Credit Scoring Model Development Using tidymodels",
    "section": "Model Architecture Definition and Hyperparameter Optimization",
    "text": "Model Architecture Definition and Hyperparameter Optimization\nThis section implements a comparative modeling approach, evaluating both traditional (logistic regression) and modern (XGBoost) algorithms. Logistic regression provides interpretability required for regulatory compliance, while XGBoost offers superior predictive performance. Both models undergo systematic hyperparameter tuning using cross-validation.\n\n# Define the logistic regression model\nlog_reg_spec &lt;- logistic_reg(penalty = tune(), mixture = tune()) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"classification\")\n\n# Create a logistic regression workflow\nlog_reg_workflow &lt;- workflow() %&gt;%\n  add_recipe(credit_recipe) %&gt;%\n  add_model(log_reg_spec)\n\n# Define the tuning grid for logistic regression\nlog_reg_grid &lt;- grid_regular(\n  penalty(range = c(-5, 0), trans = log10_trans()),\n  mixture(range = c(0, 1)),\n  levels = c(10, 5)\n)\n\n# Define the XGBoost model with tunable parameters\nxgb_spec &lt;- boost_tree(\n  trees = tune(),\n  tree_depth = tune(),\n  min_n = tune(),\n  loss_reduction = tune(),\n  mtry = tune(),\n  learn_rate = tune()\n) %&gt;%\n  set_engine(\"xgboost\", objective = \"binary:logistic\", scale_pos_weight = 5) %&gt;%\n  set_mode(\"classification\")\n\n# Create an XGBoost workflow\nxgb_workflow &lt;- workflow() %&gt;%\n  add_recipe(credit_recipe) %&gt;%\n  add_model(xgb_spec)\n\n# Define the tuning grid for XGBoost\nxgb_grid &lt;- grid_latin_hypercube(\n  trees(range = c(100, 500)),\n  tree_depth(range = c(3, 10)),\n  min_n(range = c(2, 20)),\n  loss_reduction(range = c(0.001, 1.0)),\n  mtry(range = c(5, 20)),\n  learn_rate(range = c(-4, -1), trans = log10_trans()),\n  size = 15\n)\n\n# Create cross-validation folds with stratification\nset.seed(234)\ncv_folds &lt;- vfold_cv(training(validation_split), v = 3, strata = default)\n\n# Define the metrics to evaluate\nclassification_metrics &lt;- metric_set(\n  roc_auc,  # Area under the ROC curve\n  pr_auc,    # Area under the precision-recall curve\n)\n\n# Tune the logistic regression model\nset.seed(345)\nlog_reg_tuned &lt;- tune_grid(\n  log_reg_workflow,\n  resamples = cv_folds,\n  grid = log_reg_grid,\n  metrics = classification_metrics,\n  control = control_grid(save_pred = TRUE, verbose = TRUE)\n)\n\n# Tune the XGBoost model\nset.seed(456)\nxgb_tuned &lt;- tune_grid(\n  xgb_workflow,\n  resamples = cv_folds,\n  grid = xgb_grid,\n  metrics = classification_metrics,\n  control = control_grid(save_pred = TRUE, verbose = TRUE)\n)\n\n# Collect and visualize logistic regression tuning results\nlog_reg_results &lt;- log_reg_tuned %&gt;% collect_metrics()\nlog_reg_results %&gt;% filter(.metric == \"roc_auc\") %&gt;% arrange(desc(mean)) %&gt;% head()\n\n# A tibble: 6 × 8\n   penalty mixture .metric .estimator  mean     n std_err .config              \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 0.00167     1    roc_auc binary     0.853     3 0.00841 Preprocessor1_Model45\n2 0.00167     0.75 roc_auc binary     0.853     3 0.00841 Preprocessor1_Model35\n3 0.00599     0.25 roc_auc binary     0.853     3 0.00869 Preprocessor1_Model16\n4 0.00167     0.5  roc_auc binary     0.853     3 0.00836 Preprocessor1_Model25\n5 0.000464    1    roc_auc binary     0.852     3 0.00817 Preprocessor1_Model44\n6 0.00167     0.25 roc_auc binary     0.852     3 0.00837 Preprocessor1_Model15\n\n\nThe hyperparameter optimization results demonstrate the performance characteristics of both modeling approaches across different parameter configurations. XGBoost typically achieves superior predictive performance metrics, while logistic regression provides enhanced interpretability essential for regulatory compliance and business understanding."
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#model-finalization-and-comprehensive-performance-evaluation",
    "href": "posts/modeling-with-tidymodels.html#model-finalization-and-comprehensive-performance-evaluation",
    "title": "Comprehensive Credit Scoring Model Development Using tidymodels",
    "section": "Model Finalization and Comprehensive Performance Evaluation",
    "text": "Model Finalization and Comprehensive Performance Evaluation\nWe proceed to finalize both models using their optimal hyperparameter configurations and conduct comprehensive evaluation on the validation dataset to assess generalization performance.\n\n# Select best hyperparameters based on ROC AUC\nbest_log_reg_params &lt;- select_best(log_reg_tuned, metric = \"roc_auc\")\nbest_xgb_params &lt;- select_best(xgb_tuned, metric = \"roc_auc\")\n\n# Finalize workflows with best parameters\nfinal_log_reg_workflow &lt;- log_reg_workflow %&gt;%\n  finalize_workflow(best_log_reg_params)\n\nfinal_xgb_workflow &lt;- xgb_workflow %&gt;%\n  finalize_workflow(best_xgb_params)\n\n# Fit the final models on the full training data\nfinal_log_reg_model &lt;- final_log_reg_workflow %&gt;%\n  fit(data = training(validation_split))\n\nfinal_xgb_model &lt;- final_xgb_workflow %&gt;%\n  fit(data = training(validation_split))\n\n# Make predictions on the validation set with both models\nlog_reg_val_results &lt;- final_log_reg_model %&gt;%\n  predict(testing(validation_split)) %&gt;%\n  bind_cols(predict(final_log_reg_model, testing(validation_split), type = \"prob\")) %&gt;%\n  bind_cols(testing(validation_split) %&gt;% select(default, customer_id))\n\nxgb_val_results &lt;- final_xgb_model %&gt;%\n  predict(testing(validation_split)) %&gt;%\n  bind_cols(predict(final_xgb_model, testing(validation_split), type = \"prob\")) %&gt;%\n  bind_cols(testing(validation_split) %&gt;% select(default, customer_id))\n\n# Evaluate model performance on validation set\nlog_reg_val_metrics &lt;- log_reg_val_results %&gt;%\n  metrics(truth = default, estimate = .pred_class, .pred_yes)\n\nxgb_val_metrics &lt;- xgb_val_results %&gt;%\n  metrics(truth = default, estimate = .pred_class, .pred_yes)\n\ncat(\"Logistic Regression Validation Metrics:\\n\")\n\nLogistic Regression Validation Metrics:\n\nprint(log_reg_val_metrics)\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.950\n2 kap         binary         0.145\n3 mn_log_loss binary         3.64 \n4 roc_auc     binary         0.157\n\ncat(\"\\nXGBoost Validation Metrics:\\n\")\n\n\nXGBoost Validation Metrics:\n\nprint(xgb_val_metrics)\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary        0.948 \n2 kap         binary        0.0339\n3 mn_log_loss binary        4.82  \n4 roc_auc     binary        0.158"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#model-interpretability-and-feature-importance-analysis",
    "href": "posts/modeling-with-tidymodels.html#model-interpretability-and-feature-importance-analysis",
    "title": "Comprehensive Credit Scoring Model Development Using tidymodels",
    "section": "Model Interpretability and Feature Importance Analysis",
    "text": "Model Interpretability and Feature Importance Analysis\nUnderstanding feature contributions to prediction outcomes represents a critical requirement for credit scoring models, particularly for regulatory compliance and business decision-making processes.\n#| label: feature-importance #| fig-width: 10 #| fig-height: 12"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#comprehensive-test-set-evaluation-and-performance-assessment",
    "href": "posts/modeling-with-tidymodels.html#comprehensive-test-set-evaluation-and-performance-assessment",
    "title": "Comprehensive Credit Scoring Model Development Using tidymodels",
    "section": "Comprehensive Test Set Evaluation and Performance Assessment",
    "text": "Comprehensive Test Set Evaluation and Performance Assessment\nThe final evaluation employs the optimal model (XGBoost) on the held-out test dataset to provide an unbiased assessment of generalization performance and operational effectiveness.\n#| label: final-evaluation"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#tutorial-summary-and-methodological-conclusions",
    "href": "posts/modeling-with-tidymodels.html#tutorial-summary-and-methodological-conclusions",
    "title": "Building Models in R with tidymodels",
    "section": "Tutorial Summary and Methodological Conclusions",
    "text": "Tutorial Summary and Methodological Conclusions\nThis tutorial has demonstrated the comprehensive development of a credit scoring model using the tidymodels framework, encompassing the complete machine learning workflow. The key components addressed include:\n\nSynthetic Dataset Generation: Creation of realistic credit data incorporating domain-specific variable relationships and statistical distributions\nAdvanced Feature Engineering: Implementation of comprehensive preprocessing pipelines with domain knowledge integration\nComparative Model Development: Training and optimization of both traditional and modern machine learning algorithms\nSystematic Performance Evaluation: Application of industry-standard metrics and validation techniques\nInterpretability Analysis: Development of model explanation visualizations for regulatory compliance\n\nThe tidymodels framework provides a systematic and modular approach to machine learning model development, facilitating experimentation with diverse algorithms and preprocessing strategies while maintaining rigorous statistical practices and reproducible workflows. This methodology ensures robust model development suitable for production deployment in regulated financial environments."
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#key-takeaways",
    "href": "posts/modeling-with-tidymodels.html#key-takeaways",
    "title": "Building Models in R with tidymodels",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nThe tidymodels ecosystem provides a unified approach to machine learning workflows, ensuring consistency and reproducibility across model development phases\nDomain knowledge integration is critical for effective feature engineering\nComparative modeling approaches enhance decision-making by balancing interpretability requirements with predictive performance needs\nSystematic hyperparameter optimization improves model performance while preventing overfitting through cross-validation techniques\nComprehensive evaluation frameworks ensure robust assessment of model generalization and operational effectiveness\nModel interpretability analysis supports regulatory compliance and business decision-making in financial applications"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#package-dependencies",
    "href": "posts/modeling-with-tidymodels.html#package-dependencies",
    "title": "Comprehensive Credit Scoring Model Development Using tidymodels",
    "section": "Package Dependencies",
    "text": "Package Dependencies\n\nlibrary(tidymodels)\nlibrary(xgboost)\nlibrary(vip)        # For variable importance\nlibrary(stringr)    # For string manipulation functions\nlibrary(probably)   # For calibration plots\nlibrary(ROSE)       # For imbalanced data visualization\nlibrary(corrplot)   # For correlation visualization"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#synthetic-data-generation",
    "href": "posts/modeling-with-tidymodels.html#synthetic-data-generation",
    "title": "Building Models in R with tidymodels",
    "section": "Synthetic Data Generation",
    "text": "Synthetic Data Generation\nThis tutorial uses a simulated credit dataset incorporating realistic variable distributions commonly encountered in credit scoring applications. The synthetic data includes demographic information, loan characteristics, and credit history variables with appropriate statistical relationships.\n\nset.seed(123)\nn &lt;- 10000  # Larger sample size for more realistic modeling\n\n# Create base features with realistic distributions\ndata &lt;- tibble(\n  customer_id = paste0(\"CUS\", formatC(1:n, width = 6, format = \"d\", flag = \"0\")),\n  \n  # Demographics - with realistic age distribution for credit applicants\n  age = pmax(18, pmin(80, round(rnorm(n, 38, 13)))),\n  income = pmax(12000, round(rlnorm(n, log(52000), 0.8))),\n  employment_length = pmax(0, round(rexp(n, 1/6))),  # Exponential distribution for job tenure\n  home_ownership = sample(c(\"RENT\", \"MORTGAGE\", \"OWN\"), n, replace = TRUE, prob = c(0.45, 0.40, 0.15)),\n  \n  # Loan characteristics - with more realistic correlations\n  loan_amount = round(rlnorm(n, log(15000), 0.7) / 100) * 100,  # Log-normal for loan amounts\n  loan_term = sample(c(36, 60, 120), n, replace = TRUE, prob = c(0.6, 0.3, 0.1)),\n  \n  # Credit history - with more realistic distributions\n  credit_score = round(pmin(850, pmax(300, rnorm(n, 700, 90)))),\n  dti_ratio = pmax(0, pmin(65, rlnorm(n, log(20), 0.4))),  # Debt-to-income ratio\n  delinq_2yrs = rpois(n, 0.4),  # Number of delinquencies in past 2 years\n  inq_last_6mths = rpois(n, 0.7),  # Number of inquiries in last 6 months\n  open_acc = pmax(1, round(rnorm(n, 10, 4))),  # Number of open accounts\n  pub_rec = rbinom(n, 2, 0.06),  # Number of public records\n  revol_util = pmin(100, pmax(0, rnorm(n, 40, 20))),  # Revolving utilization\n  total_acc = pmax(open_acc, open_acc + round(rnorm(n, 8, 6)))  # Total accounts\n)\n\n# Add realistic correlations between variables\ndata &lt;- data %&gt;%\n  mutate(\n    # Interest rate depends on credit score and loan term\n    interest_rate = 25 - (credit_score - 300) * (15/550) + \n      ifelse(loan_term == 36, -1, ifelse(loan_term == 60, 0, 1.5)) +\n      rnorm(n, 0, 1.5),\n    \n    # Loan purpose with realistic probabilities\n    loan_purpose = sample(\n      c(\"debt_consolidation\", \"credit_card\", \"home_improvement\", \"major_purchase\", \"medical\", \"other\"), \n      n, replace = TRUE, \n      prob = c(0.45, 0.25, 0.10, 0.08, 0.07, 0.05)\n    ),\n    \n    # Add some derived features that have predictive power\n    payment_amount = (loan_amount * (interest_rate/100/12) * (1 + interest_rate/100/12)^loan_term) / \n      ((1 + interest_rate/100/12)^loan_term - 1),\n    payment_to_income_ratio = (payment_amount * 12) / income\n  )\n\n# Create a more realistic default probability model with non-linear effects\nlogit_default &lt;- with(data, {\n  -4.5 +  # Base intercept for ~10% default rate\n    -0.03 * (age - 18) +  # Age effect (stronger for younger borrowers)\n    -0.2 * log(income/10000) +  # Log-transformed income effect\n    -0.08 * employment_length +  # Employment length effect\n    ifelse(home_ownership == \"OWN\", -0.7, ifelse(home_ownership == \"MORTGAGE\", -0.3, 0)) +  # Home ownership\n    0.3 * log(loan_amount/1000) +  # Log-transformed loan amount\n    ifelse(loan_term == 36, 0, ifelse(loan_term == 60, 0.4, 0.8)) +  # Loan term\n    0.15 * interest_rate +  # Interest rate effect\n    ifelse(loan_purpose == \"debt_consolidation\", 0.5, \n           ifelse(loan_purpose == \"credit_card\", 0.4, \n                  ifelse(loan_purpose == \"medical\", 0.6, 0))) +  # Loan purpose\n    -0.01 * (credit_score - 300) +  # Credit score (stronger effect at lower scores)\n    0.06 * dti_ratio +  # DTI ratio effect\n    0.4 * delinq_2yrs +  # Delinquencies effect (stronger effect for first delinquency)\n    0.3 * inq_last_6mths +  # Inquiries effect\n    -0.1 * log(open_acc + 1) +  # Open accounts (log-transformed)\n    0.8 * pub_rec +  # Public records (strong effect)\n    0.02 * revol_util +  # Revolving utilization\n    1.2 * payment_to_income_ratio +  # Payment to income ratio (strong effect)\n    rnorm(n, 0, 0.8)  # Add some noise for realistic variation\n})\n\n# Generate default flag with realistic default rate\nprob_default &lt;- plogis(logit_default)\ndata$default &lt;- factor(rbinom(n, 1, prob_default), levels = c(0, 1), labels = c(\"no\", \"yes\"))\n\n# Check class distribution\ntable(data$default)\n\n\n  no  yes \n9425  575 \n\n\n\nprop.table(table(data$default))\n\n\n    no    yes \n0.9425 0.0575 \n\n\n\n# Visualize the default rate\nggplot(data, aes(x = default, fill = default)) +\n  geom_bar(aes(y = ..prop.., group = 1)) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"Class Distribution in Credit Dataset\",\n       y = \"Percentage\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Visualize the relationship between key variables and default rate\nggplot(data, aes(x = credit_score, y = as.numeric(default) - 1)) +\n  geom_smooth(method = \"loess\") +\n  labs(title = \"Default Rate by Credit Score\", \n       x = \"Credit Score\", y = \"Default Probability\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Examine correlation between numeric predictors\ncredit_cors &lt;- data %&gt;%\n  select(age, income, employment_length, loan_amount, interest_rate, \n         credit_score, dti_ratio, delinq_2yrs, revol_util, payment_to_income_ratio) %&gt;%\n  cor()\n\ncorrplot(credit_cors, method = \"circle\", type = \"upper\", \n         tl.col = \"black\", tl.srt = 45, tl.cex = 0.7)"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#stratified-partitioning",
    "href": "posts/modeling-with-tidymodels.html#stratified-partitioning",
    "title": "Building Models in R with tidymodels",
    "section": "Stratified Partitioning",
    "text": "Stratified Partitioning\nCredit default datasets typically exhibit class imbalance, with default events representing the minority class. We implement stratified sampling to preserve the class distribution across training, validation, and test partitions.\n\n# Create initial train/test split (80/20)\nset.seed(456)\ninitial_split &lt;- initial_split(data, prop = 0.8, strata = default)\ntrain_data &lt;- training(initial_split)\ntest_data &lt;- testing(initial_split)\n\n# Create validation set from training data (75% train, 25% validation)\nset.seed(789)\nvalidation_split &lt;- initial_split(train_data, prop = 0.75, strata = default)\n\n# Check class imbalance in training data\ntrain_class_counts &lt;- table(training(validation_split)$default)\ntrain_class_props &lt;- prop.table(train_class_counts)\n\ncat(\"Training data class distribution:\\n\")\n\nTraining data class distribution:\n\nprint(train_class_counts)\n\n\n  no  yes \n5661  339 \n\ncat(\"\\nPercentage:\\n\")\n\n\nPercentage:\n\nprint(train_class_props * 100)\n\n\n   no   yes \n94.35  5.65 \n\n# Visualize class imbalance\nROSE::roc.curve(training(validation_split)$default == \"yes\", \n                training(validation_split)$credit_score,\n                plotit = TRUE,                main = \"ROC Curve for Credit Score Alone\")\n\n\n\n\n\n\n\n\nArea under the curve (AUC): 0.753"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#model-architecture-and-hyperparameter-optimization",
    "href": "posts/modeling-with-tidymodels.html#model-architecture-and-hyperparameter-optimization",
    "title": "Building Models in R with tidymodels",
    "section": "Model Architecture and Hyperparameter Optimization",
    "text": "Model Architecture and Hyperparameter Optimization\nThis section implements both traditional (logistic regression) and modern (XGBoost) algorithms. Logistic regression provides interpretability required for regulatory compliance, while XGBoost offers superior predictive performance. Both models undergo systematic hyperparameter tuning using cross-validation.\n\n# Define the logistic regression model\nlog_reg_spec &lt;- logistic_reg(penalty = tune(), mixture = tune()) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"classification\")\n\n# Create a logistic regression workflow\nlog_reg_workflow &lt;- workflow() %&gt;%\n  add_recipe(credit_recipe) %&gt;%\n  add_model(log_reg_spec)\n\n# Define the tuning grid for logistic regression\nlog_reg_grid &lt;- grid_regular(\n  penalty(range = c(-5, 0), trans = log10_trans()),\n  mixture(range = c(0, 1)),\n  levels = c(10, 5)\n)\n\n# Define the XGBoost model with tunable parameters\nxgb_spec &lt;- boost_tree(\n  trees = tune(),\n  tree_depth = tune(),\n  min_n = tune(),\n  loss_reduction = tune(),\n  mtry = tune(),\n  learn_rate = tune()\n) %&gt;%\n  set_engine(\"xgboost\", objective = \"binary:logistic\") %&gt;%\n  set_mode(\"classification\")\n\n# Create an XGBoost workflow\nxgb_workflow &lt;- workflow() %&gt;%\n  add_recipe(credit_recipe) %&gt;%\n  add_model(xgb_spec)\n\n# Define the tuning grid for XGBoost\nxgb_grid &lt;- grid_latin_hypercube(\n  trees(range = c(50, 100)),\n  tree_depth(range = c(3, 6)),\n  min_n(range = c(2, 8)),\n  loss_reduction(range = c(0.001, 1.0)),\n  mtry(range = c(5, 20)),\n  learn_rate(range = c(-4, -1), trans = log10_trans()),\n  size = 10\n)\n\n# Create cross-validation folds with stratification\nset.seed(234)\ncv_folds &lt;- vfold_cv(training(validation_split), v = 3, strata = default)\n\n# Define the metrics to evaluate\nclassification_metrics &lt;- metric_set(\n  roc_auc,  # Area under the ROC curve\n  pr_auc,    # Area under the precision-recall curve\n)\n\n# Tune the logistic regression model\nset.seed(345)\nlog_reg_tuned &lt;- tune_grid(\n  log_reg_workflow,\n  resamples = cv_folds,\n  grid = log_reg_grid,\n  metrics = classification_metrics,\n  control = control_grid(save_pred = TRUE, verbose = TRUE)\n)\n\n# Tune the XGBoost model\nset.seed(456)\nxgb_tuned &lt;- tune_grid(\n  xgb_workflow,\n  resamples = cv_folds,\n  grid = xgb_grid,\n  metrics = classification_metrics,\n  control = control_grid(save_pred = TRUE, verbose = TRUE)\n)\n\n# Collect and visualize logistic regression tuning results\nlog_reg_results &lt;- log_reg_tuned %&gt;% collect_metrics()\nlog_reg_results %&gt;% filter(.metric == \"roc_auc\") %&gt;% arrange(desc(mean)) %&gt;% head()\n\n# A tibble: 6 × 8\n   penalty mixture .metric .estimator  mean     n std_err .config              \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 0.00167     1    roc_auc binary     0.853     3 0.00841 Preprocessor1_Model45\n2 0.00167     0.75 roc_auc binary     0.853     3 0.00841 Preprocessor1_Model35\n3 0.00599     0.25 roc_auc binary     0.853     3 0.00869 Preprocessor1_Model16\n4 0.00167     0.5  roc_auc binary     0.853     3 0.00836 Preprocessor1_Model25\n5 0.000464    1    roc_auc binary     0.852     3 0.00817 Preprocessor1_Model44\n6 0.00167     0.25 roc_auc binary     0.852     3 0.00837 Preprocessor1_Model15"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#model-finalization-and-performance-evaluation",
    "href": "posts/modeling-with-tidymodels.html#model-finalization-and-performance-evaluation",
    "title": "Building Models in R with tidymodels",
    "section": "Model Finalization and Performance Evaluation",
    "text": "Model Finalization and Performance Evaluation\nThis section focuses on finalizing both models using their optimal hyperparameters and evaluating model performance on the validation dataset.\n\n# Select best hyperparameters based on ROC AUC\nbest_log_reg_params &lt;- select_best(log_reg_tuned, metric = \"roc_auc\")\nbest_xgb_params &lt;- select_best(xgb_tuned, metric = \"roc_auc\")\n\n# Finalize workflows with best parameters\nfinal_log_reg_workflow &lt;- log_reg_workflow %&gt;%\n  finalize_workflow(best_log_reg_params)\n\nfinal_xgb_workflow &lt;- xgb_workflow %&gt;%\n  finalize_workflow(best_xgb_params)\n\n# Fit the final models on the full training data\nfinal_log_reg_model &lt;- final_log_reg_workflow %&gt;%\n  fit(data = training(validation_split))\n\nfinal_xgb_model &lt;- final_xgb_workflow %&gt;%\n  fit(data = training(validation_split))\n\n# Make predictions on the validation set with both models\nlog_reg_val_results &lt;- final_log_reg_model %&gt;%\n  predict(testing(validation_split)) %&gt;%\n  bind_cols(predict(final_log_reg_model, testing(validation_split), type = \"prob\")) %&gt;%\n  bind_cols(testing(validation_split) %&gt;% select(default, customer_id))\n\nxgb_val_results &lt;- final_xgb_model %&gt;%\n  predict(testing(validation_split)) %&gt;%\n  bind_cols(predict(final_xgb_model, testing(validation_split), type = \"prob\")) %&gt;%\n  bind_cols(testing(validation_split) %&gt;% select(default, customer_id))\n\n# Evaluate model performance on validation set\nlog_reg_val_metrics &lt;- log_reg_val_results %&gt;%\n  metrics(truth = default, estimate = .pred_class, .pred_yes)\n\nxgb_val_metrics &lt;- xgb_val_results %&gt;%\n  metrics(truth = default, estimate = .pred_class, .pred_yes)\n\ncat(\"Logistic Regression Validation Metrics:\\n\")\n\nLogistic Regression Validation Metrics:\n\nprint(log_reg_val_metrics)\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.950\n2 kap         binary         0.145\n3 mn_log_loss binary         3.64 \n4 roc_auc     binary         0.157\n\ncat(\"\\nXGBoost Validation Metrics:\\n\")\n\n\nXGBoost Validation Metrics:\n\nprint(xgb_val_metrics)\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.948\n2 kap         binary         0.111\n3 mn_log_loss binary         3.24 \n4 roc_auc     binary         0.172"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#interpretability-and-feature-importance",
    "href": "posts/modeling-with-tidymodels.html#interpretability-and-feature-importance",
    "title": "Building Models in R with tidymodels",
    "section": "Interpretability and Feature Importance",
    "text": "Interpretability and Feature Importance\nUnderstanding feature contributions to prediction outcomes represents a critical requirement for credit scoring models, particularly for regulatory compliance and business decision-making.\n\n# Extract feature importance from XGBoost model\nxgb_importance &lt;- final_xgb_model %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vip(num_features = 15) +\n  labs(title = \"XGBoost Feature Importance\")\n\nxgb_importance\n\n\n\n\n\n\n\n# Extract coefficients from logistic regression model\nlog_reg_importance &lt;- final_log_reg_model %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vip(num_features = 15) +\n  labs(title = \"Logistic Regression Coefficient Importance\")\n\nlog_reg_importance\n\n\n\n\n\n\n\n# Create calibration plots for both models\nlog_reg_cal &lt;- log_reg_val_results %&gt;%\n  cal_plot_breaks(truth = default, estimate = .pred_yes, event_level = \"second\", num_breaks = 5) +\n  labs(title = \"Logistic Regression Probability Calibration\")\n\nxgb_cal &lt;- xgb_val_results %&gt;%\n  cal_plot_breaks(truth = default, estimate = .pred_yes, event_level = \"second\", num_breaks = 5) +\n  labs(title = \"XGBoost Probability Calibration\")\n\nlog_reg_cal\n\n\n\n\n\n\n\nxgb_cal"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#test-set-evaluation",
    "href": "posts/modeling-with-tidymodels.html#test-set-evaluation",
    "title": "Building Models in R with tidymodels",
    "section": "Test Set Evaluation",
    "text": "Test Set Evaluation\nThe final evaluation employs the optimal model (XGBoost) on the held-out test dataset to provide an unbiased assessment of generalization performance and operational effectiveness.\n\n# Make predictions on the test set with the XGBoost model\ntest_results &lt;- final_xgb_model %&gt;%\n  predict(test_data) %&gt;%\n  bind_cols(predict(final_xgb_model, test_data, type = \"prob\")) %&gt;%\n  bind_cols(test_data %&gt;% select(default, customer_id, credit_score))\n\n# Calculate performance metrics\ntest_metrics &lt;- test_results %&gt;%\n  metrics(truth = default, estimate = .pred_class, .pred_yes)\n\ncat(\"Final Test Set Performance Metrics:\\n\")\n\nFinal Test Set Performance Metrics:\n\nprint(test_metrics)\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary        0.932 \n2 kap         binary        0.0316\n3 mn_log_loss binary        3.21  \n4 roc_auc     binary        0.172 \n\n# Calculate AUC on test set\ntest_auc &lt;- test_results %&gt;%\n  roc_auc(truth = default, .pred_yes)\ncat(\"\\nTest Set ROC AUC: \", test_auc$.estimate, \"\\n\")\n\n\nTest Set ROC AUC:  0.1723324 \n\n# Create a gains table (commonly used in credit scoring)\ngains_table &lt;- test_results %&gt;%\n  mutate(risk_decile = ntile(.pred_yes, 10)) %&gt;%\n  group_by(risk_decile) %&gt;%\n  summarise(\n    total_accounts = n(),\n    defaults = sum(default == \"yes\"),\n    non_defaults = sum(default == \"no\"),\n    default_rate = mean(default == \"yes\"),\n    avg_score = mean(credit_score)) %&gt;% \n  arrange(desc(default_rate)) %&gt;% \n  mutate(\n    cumulative_defaults = cumsum(defaults),\n    pct_defaults_captured = cumulative_defaults / sum(defaults),\n    cumulative_accounts = cumsum(total_accounts),\n    pct_accounts = cumulative_accounts / sum(total_accounts),\n    lift = default_rate / (sum(defaults) / sum(total_accounts))\n  )\n\n# Display the gains table\ngains_table\n\n# A tibble: 10 × 11\n   risk_decile total_accounts defaults non_defaults default_rate avg_score\n         &lt;int&gt;          &lt;int&gt;    &lt;int&gt;        &lt;int&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n 1          10            200       58          142        0.29       577.\n 2           9            200       26          174        0.13       628.\n 3           8            200       19          181        0.095      640.\n 4           7            200        8          192        0.04       665.\n 5           6            200        7          193        0.035      682.\n 6           4            200        6          194        0.03       727.\n 7           2            200        2          198        0.01       776.\n 8           5            200        2          198        0.01       716.\n 9           1            200        1          199        0.005      790.\n10           3            200        1          199        0.005      757.\n# ℹ 5 more variables: cumulative_defaults &lt;int&gt;, pct_defaults_captured &lt;dbl&gt;,\n#   cumulative_accounts &lt;int&gt;, pct_accounts &lt;dbl&gt;, lift &lt;dbl&gt;\n\n# Create a lift chart\nggplot(gains_table, aes(x = pct_accounts, y = lift)) +\n  geom_line() +\n  geom_point() +\n  geom_hline(yintercept = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Lift Chart\",\n       x = \"Percentage of Accounts\",\n       y = \"Lift\") +  theme_minimal()"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#prerequisites-and-required-libraries",
    "href": "posts/monotonic-binning-using-xgboost.html#prerequisites-and-required-libraries",
    "title": "Monotonic Binning Using XGBoost",
    "section": "Prerequisites and Required Libraries",
    "text": "Prerequisites and Required Libraries\n\nlibrary(recipes)  # For data preprocessing\nlibrary(dplyr)    # For data manipulation\nlibrary(xgboost)  # For creating monotonic bins\nlibrary(ggplot2)  # For visualization"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#dataset-overview-and-loading",
    "href": "posts/monotonic-binning-using-xgboost.html#dataset-overview-and-loading",
    "title": "Monotonic Binning Using XGBoost",
    "section": "Dataset Overview and Loading",
    "text": "Dataset Overview and Loading\nThis tutorial demonstrates the methodology using a sample from the Lending Club dataset, which provides comprehensive loan information including default indicators.\n\n# Load sample data from Lending Club dataset\nsample &lt;- read.csv(\"https://bit.ly/42ypcnJ\")\n\n# Check dimensions of the dataset\ndim(sample)\n\n[1] 10000   153"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#step-1-creating-a-binary-target-variable",
    "href": "posts/monotonic-binning-using-xgboost.html#step-1-creating-a-binary-target-variable",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "The first step in our process involves constructing a binary target variable that clearly identifies loan defaults. This variable will serve as our outcome measure throughout the binning process:\n\n# Define loan statuses that represent defaults\ncodes &lt;- c(\"Charged Off\", \"Does not meet the credit policy. Status:Charged Off\")\n\n# Create binary target variable\nmodel_data &lt;- sample %&gt;%\n  mutate(bad_flag = ifelse(loan_status %in% codes, 1, 0))"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#step-2-data-preprocessing-and-preparation",
    "href": "posts/monotonic-binning-using-xgboost.html#step-2-data-preprocessing-and-preparation",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "Before proceeding with monotonic binning, the dataset must be prepared through systematic preprocessing. This step ensures data quality and compatibility with the XGBoost implementation. The process utilizes the recipes package to: 1. Filter and retain only numeric variables for analysis 2. Apply median imputation to handle missing values appropriately\n\n# Create a recipe for preprocessing\nrec &lt;- recipe(bad_flag ~ ., data = model_data) %&gt;%\n  step_select(where(is.numeric)) %&gt;%  # Keep only numeric variables\n  step_impute_median(all_predictors())  # Fill missing values with medians\n\n# Apply the preprocessing steps\nrec &lt;- prep(rec, training = model_data)\ntrain &lt;- bake(rec, new_data = model_data)"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#step-3-exploratory-analysis-of-variable-relationships",
    "href": "posts/monotonic-binning-using-xgboost.html#step-3-exploratory-analysis-of-variable-relationships",
    "title": "Monotonic Binning Using XGBoost",
    "section": "Step 3: Exploratory Analysis of Variable Relationships",
    "text": "Step 3: Exploratory Analysis of Variable Relationships\nIt is crucial to examine the raw relationship between predictor variables and the target outcome. This analysis provides insights into underlying data patterns and validates the need for monotonic transformation.\nThis example analyzes the relationship between credit inquiries in the past 6 months and default rates:\n\n# Create dataframe with inquiries and default flag\ndata.frame(x = model_data$inq_last_6mths,\n           y = model_data$bad_flag) %&gt;%\n  filter(x &lt;= 5) %&gt;%  # Focus on 0-5 inquiries for clarity\n  group_by(x) %&gt;% \n  summarise(count = n(),  # Count observations in each group\n            events = sum(y)) %&gt;%  # Count defaults in each group\n  mutate(pct = events/count) %&gt;%  # Calculate default rate\n  ggplot(aes(x = factor(x), y = pct)) + \n  geom_col() + \n  theme_minimal() + \n  labs(x = \"# of inquiries in past 6 months\", \n       y = \"Default rate\",\n       title = \"Default rate vs number of inquiries\")\n\n\n\n\n\n\n\n\nWhile the data exhibits a general upward trend (indicating that increased inquiries correlate with higher default rates), the relationship lacks perfect monotonicity. This validates the necessity of the monotonic binning approach to establish consistent patterns."
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#step-4-implementing-monotonic-binning-with-xgboost",
    "href": "posts/monotonic-binning-using-xgboost.html#step-4-implementing-monotonic-binning-with-xgboost",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "The implementation of the core monotonic binning methodology uses XGBoost’s advanced monotonicity constraints. The fundamental component of this approach is the monotone_constraints = 1 parameter, which enforces the model to generate splits that preserve a positive monotonic relationship with the target variable:\n\n# Train XGBoost model with monotonicity constraint\nmdl &lt;- xgboost(\n  data = train %&gt;%\n    select(inq_last_6mths) %&gt;%  # Use only the inquiries variable\n    as.matrix(),  \n  label = train[[\"bad_flag\"]],  # Target variable\n  nrounds = 5,  # Number of boosting rounds\n  params = list(\n    booster = \"gbtree\",\n    objective = \"binary:logistic\",\n    monotone_constraints = 1,  # Force positive relationship\n    max_depth = 1  # Simple trees with single splits\n  ),\n  verbose = 0  # Suppress output\n)"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#step-5-extracting-split-points-and-constructing-final-bins",
    "href": "posts/monotonic-binning-using-xgboost.html#step-5-extracting-split-points-and-constructing-final-bins",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "Following model training, the process extracts the optimal split points identified by XGBoost and utilizes them to construct the final monotonic bin structure:\n\n# Extract split points from the model\nsplits &lt;- xgb.model.dt.tree(model = mdl)  \n\n# Create bin boundaries including -Inf and Inf for complete coverage\ncuts &lt;- c(-Inf, unique(sort(splits$Split)), Inf)\n\n# Create and visualize the monotonic bins\ndata.frame(target = train$bad_flag,\n           buckets = cut(train$inq_last_6mths, \n                         breaks = cuts, \n                         include.lowest = TRUE, \n                         right = TRUE)) %&gt;% \n  group_by(buckets) %&gt;%\n  summarise(total = n(),  # Count observations in each bin\n            events = sum(target == 1)) %&gt;%  # Count defaults in each bin\n  mutate(pct = events/total) %&gt;%  # Calculate default rate\n  ggplot(aes(x = buckets, y = pct)) + \n  geom_col() + \n  theme_minimal() + \n  labs(x = \"Bins\", \n       y = \"Default rate\",\n       title = \"Monotonic Bins for Inquiries\")\n\n\n\n\n\n\n\n\nObserve how the default rates now demonstrate perfect monotonic behavior across all bins, creating a clearer and more interpretable relationship compared to the raw data patterns examined previously. This transformation enhances both model performance and stakeholder comprehension."
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#step-6-developing-a-generalized-function-for-production-use",
    "href": "posts/monotonic-binning-using-xgboost.html#step-6-developing-a-generalized-function-for-production-use",
    "title": "Monotonic Binning Using XGBoost",
    "section": "Step 6: Developing a Generalized Function for Production Use",
    "text": "Step 6: Developing a Generalized Function for Production Use\nTo maximize efficiency and enable scalable implementation across multiple variables, the next step involves constructing a comprehensive, reusable function that encapsulates the entire monotonic binning workflow:\n\ncreate_bins &lt;- function(var, outcome, max_depth = 10, plot = TRUE){\n  # Determine relationship direction automatically\n  corr &lt;- cor(var, outcome, method = \"spearman\")\n  direction &lt;- ifelse(corr &gt; 0, 1, -1)  # 1 for positive, -1 for negative correlation\n  \n  # Build XGBoost model with appropriate monotonicity constraint\n  mdl &lt;- xgboost(\n    verbose = 0,\n    data = as.matrix(var),\n    label = outcome,\n    nrounds = 100,  # Single round is sufficient for binning\n    params = list(objective = \"binary:logistic\",\n                  monotone_constraints = direction,  # Apply constraint based on correlation\n                  max_depth = max_depth))  # Control tree complexity\n  \n  # Extract and return split points\n  splits &lt;- xgb.model.dt.tree(model = mdl)\n  cuts &lt;- c(-Inf, sort(unique(splits$Split)), Inf)  # Include boundaries for complete coverage\n  \n  # Optionally visualize the bins\n  if(plot) {\n    data.frame(target = outcome,\n               buckets = cut(var, \n                            breaks = cuts, \n                            include.lowest = TRUE, \n                            right = TRUE)) %&gt;% \n      group_by(buckets) %&gt;%\n      summarise(total = n(),\n                events = sum(target == 1)) %&gt;%\n      mutate(pct = events/total) %&gt;%\n      ggplot(aes(x = buckets, y = pct)) + \n      geom_col() + \n      theme_minimal() + \n      labs(x = \"Bins\", \n           y = \"Default rate\",\n           title = \"Monotonic Bins\")\n  }\n  \n  return(cuts)  # Return the bin boundaries\n}"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#practical-application-example",
    "href": "posts/monotonic-binning-using-xgboost.html#practical-application-example",
    "title": "Monotonic Binning Using XGBoost",
    "section": "Practical Application Example",
    "text": "Practical Application Example\nThe following demonstrates how to apply the generalized function to create monotonic bins for any numeric variable in a dataset. This approach can be readily implemented across an entire feature set:\n\n# Example: Create monotonic bins for annual income\nincome_bins &lt;- create_bins(\n  var = train$annual_inc,\n  outcome = train$bad_flag,\n  max_depth = 5\n)"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#creating-a-binary-target-variable",
    "href": "posts/monotonic-binning-using-xgboost.html#creating-a-binary-target-variable",
    "title": "Monotonic Binning Using XGBoost",
    "section": "Creating a Binary Target Variable",
    "text": "Creating a Binary Target Variable\nThe first step involves constructing a binary target variable that clearly identifies loan defaults. This variable will serve as our outcome throughout the binning process:\n\n# Define loan statuses that represent defaults\ncodes &lt;- c(\"Charged Off\", \"Does not meet the credit policy. Status:Charged Off\")\n\n# Create binary target variable\nmodel_data &lt;- sample %&gt;%\n  mutate(bad_flag = ifelse(loan_status %in% codes, 1, 0))"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#data-preprocessing",
    "href": "posts/monotonic-binning-using-xgboost.html#data-preprocessing",
    "title": "Monotonic Binning Using XGBoost",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nBefore proceeding, the dataset must be prepared through systematic preprocessing. This step ensures data quality and compatibility with the XGBoost implementation. The process utilizes the recipes package to:\n\nFilter and retain only numeric variables for analysis\nApply median imputation to handle missing values appropriately\n\n\n# Create a recipe for preprocessing\nrec &lt;- recipe(bad_flag ~ ., data = model_data) %&gt;%\n  step_select(where(is.numeric)) %&gt;%  # Keep only numeric variables\n  step_impute_median(all_predictors())  # Fill missing values with medians\n\n# Apply the preprocessing steps\nrec &lt;- prep(rec, training = model_data)\ntrain &lt;- bake(rec, new_data = model_data)"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#implementing-monotonic-binning-with-xgboost",
    "href": "posts/monotonic-binning-using-xgboost.html#implementing-monotonic-binning-with-xgboost",
    "title": "Monotonic Binning Using XGBoost",
    "section": "Implementing Monotonic Binning with XGBoost",
    "text": "Implementing Monotonic Binning with XGBoost\nThe implementation uses XGBoost’s monotonicity constraints which enforces the model to generate splits that preserve a monotonic relationship with the target variable:\n\n# Train XGBoost model with monotonicity constraint\nmdl &lt;- xgboost(\n  data = train %&gt;%\n    select(inq_last_6mths) %&gt;%  # Use only the inquiries variable\n    as.matrix(),  \n  label = train[[\"bad_flag\"]],  # Target variable\n  nrounds = 5,  # Number of boosting rounds\n  params = list(\n    booster = \"gbtree\",\n    objective = \"binary:logistic\",\n    monotone_constraints = 1,  # Force positive relationship\n    max_depth = 1  # Simple trees with single splits\n  ),\n  verbose = 0  # Suppress output\n)"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#extracting-split-points-and-constructing-final-bins",
    "href": "posts/monotonic-binning-using-xgboost.html#extracting-split-points-and-constructing-final-bins",
    "title": "Monotonic Binning Using XGBoost",
    "section": "Extracting Split Points and Constructing Final Bins",
    "text": "Extracting Split Points and Constructing Final Bins\nFollowing model training, the process extracts the optimal split points identified by XGBoost and utilizes them to construct the final bin structure:\n\n# Extract split points from the model\nsplits &lt;- xgb.model.dt.tree(model = mdl)  \n\n# Create bin boundaries including -Inf and Inf for complete coverage\ncuts &lt;- c(-Inf, unique(sort(splits$Split)), Inf)\n\n# Create and visualize the monotonic bins\ndata.frame(target = train$bad_flag,\n           buckets = cut(train$inq_last_6mths, \n                         breaks = cuts, \n                         include.lowest = TRUE, \n                         right = TRUE)) %&gt;% \n  group_by(buckets) %&gt;%\n  summarise(total = n(),  # Count observations in each bin\n            events = sum(target == 1)) %&gt;%  # Count defaults in each bin\n  mutate(pct = events/total) %&gt;%  # Calculate default rate\n  ggplot(aes(x = buckets, y = pct)) + \n  geom_col() + \n  theme_minimal() + \n  labs(x = \"Bins\", \n       y = \"Default rate\",\n       title = \"Monotonic Bins for Inquiries\")\n\n\n\n\n\n\n\n\nThe default rates now demonstrate perfect monotonic behavior across all bins, creating a clearer and more interpretable relationship compared to the raw data."
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#developing-a-generalized-function",
    "href": "posts/monotonic-binning-using-xgboost.html#developing-a-generalized-function",
    "title": "Monotonic Binning Using XGBoost",
    "section": "Developing a Generalized Function",
    "text": "Developing a Generalized Function\nTo enable implementation across multiple variables, a reusable function that encapsulates the entire monotonic binning workflow would be very useful here. Note the use of ranked correlation to identify the appropriate direction to be used inside the Xgboost call.\n\ncreate_bins &lt;- function(var, outcome, max_depth = 10, plot = TRUE){\n  # Determine relationship direction automatically\n  corr &lt;- cor(var, outcome, method = \"spearman\")\n  direction &lt;- ifelse(corr &gt; 0, 1, -1)  # 1 for positive, -1 for negative correlation\n  \n  # Build XGBoost model with appropriate monotonicity constraint\n  mdl &lt;- xgboost(\n    verbose = 0,\n    data = as.matrix(var),\n    label = outcome,\n    nrounds = 100,  # Single round is sufficient for binning\n    params = list(objective = \"binary:logistic\",\n                  monotone_constraints = direction,  # Apply constraint based on correlation\n                  max_depth = max_depth))  # Control tree complexity\n  \n  # Extract and return split points\n  splits &lt;- xgb.model.dt.tree(model = mdl)\n  cuts &lt;- c(-Inf, sort(unique(splits$Split)), Inf)  # Include boundaries for complete coverage\n  \n  # Optionally visualize the bins\n  if(plot) {\n    data.frame(target = outcome,\n               buckets = cut(var, \n                            breaks = cuts, \n                            include.lowest = TRUE, \n                            right = TRUE)) %&gt;% \n      group_by(buckets) %&gt;%\n      summarise(total = n(),\n                events = sum(target == 1)) %&gt;%\n      mutate(pct = events/total) %&gt;%\n      ggplot(aes(x = buckets, y = pct)) + \n      geom_col() + \n      theme_minimal() + \n      labs(x = \"Bins\", \n           y = \"Default rate\",\n           title = \"Monotonic Bins\")\n  }\n  \n  return(cuts)  # Return the bin boundaries\n}\n\n# Example: Create monotonic bins for annual income\nincome_bins &lt;- create_bins(\n  var = train$annual_inc,\n  outcome = train$bad_flag,\n  max_depth = 5\n)"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#generalised-function",
    "href": "posts/monotonic-binning-using-xgboost.html#generalised-function",
    "title": "Monotonic Binning Using XGBoost",
    "section": "Generalised Function",
    "text": "Generalised Function\nTo enable implementation across multiple variables, a reusable function that encapsulates the entire monotonic binning workflow would be very useful here. Note the use of ranked correlation to identify the appropriate direction to be used inside the Xgboost call.\n\ncreate_bins &lt;- function(var, outcome, max_depth = 10, plot = TRUE){\n  # Determine relationship direction automatically\n  corr &lt;- cor(var, outcome, method = \"spearman\")\n  direction &lt;- ifelse(corr &gt; 0, 1, -1)  # 1 for positive, -1 for negative correlation\n  \n  # Build XGBoost model with appropriate monotonicity constraint\n  mdl &lt;- xgboost(\n    verbose = 0,\n    data = as.matrix(var),\n    label = outcome,\n    nrounds = 100,  # Single round is sufficient for binning\n    params = list(objective = \"binary:logistic\",\n                  monotone_constraints = direction,  # Apply constraint based on correlation\n                  max_depth = max_depth))  # Control tree complexity\n  \n  # Extract and return split points\n  splits &lt;- xgb.model.dt.tree(model = mdl)\n  cuts &lt;- c(-Inf, sort(unique(splits$Split)), Inf)  # Include boundaries for complete coverage\n  \n  # Optionally visualize the bins\n  if(plot) {\n    data.frame(target = outcome,\n               buckets = cut(var, \n                            breaks = cuts, \n                            include.lowest = TRUE, \n                            right = TRUE)) %&gt;% \n      group_by(buckets) %&gt;%\n      summarise(total = n(),\n                events = sum(target == 1)) %&gt;%\n      mutate(pct = events/total) %&gt;%\n      ggplot(aes(x = buckets, y = pct)) + \n      geom_col() + \n      theme_minimal() + \n      labs(x = \"Bins\", \n           y = \"Default rate\",\n           title = \"Monotonic Bins\")\n  }\n  \n  return(cuts)  # Return the bin boundaries\n}\n\n# Example: Create monotonic bins for annual income\nincome_bins &lt;- create_bins(\n  var = train$annual_inc,\n  outcome = train$bad_flag,\n  max_depth = 5\n)"
  },
  {
    "objectID": "posts/multi-task-learning-with-torch.html#prerequisites-and-installation",
    "href": "posts/multi-task-learning-with-torch.html#prerequisites-and-installation",
    "title": "Multi-Task Learning with torch in R",
    "section": "Prerequisites and Installation",
    "text": "Prerequisites and Installation\nThe implementation requires the following R packages to be installed and loaded:\n\n# install.packages(c(\"torch\", \"tidyverse\", \"corrplot\"))\nlibrary(torch)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/multi-task-learning-with-torch.html#introduction",
    "href": "posts/multi-task-learning-with-torch.html#introduction",
    "title": "Multi-Task Learning with torch in R",
    "section": "Introduction",
    "text": "Introduction\nMulti-task learning operates by sharing representations between related tasks, enabling models to generalize more effectively. Instead of training separate models for each task, this approach develops a single model with:\n\nShared layers that learn common features across tasks\nTask-specific layers that specialize for each individual task\n\nMultiple loss functions, one for each task\n\nThis approach is particularly valuable when dealing with related prediction problems that can benefit from shared feature representations."
  },
  {
    "objectID": "posts/multi-task-learning-with-torch.html#prerequisites",
    "href": "posts/multi-task-learning-with-torch.html#prerequisites",
    "title": "Multi-Task Learning with torch in R",
    "section": "Prerequisites",
    "text": "Prerequisites\n\n# install.packages(c(\"torch\", \"tidyverse\", \"corrplot\"))\nlibrary(torch)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/multi-task-learning-with-torch.html#anti-overfitting-strategies-summary",
    "href": "posts/multi-task-learning-with-torch.html#anti-overfitting-strategies-summary",
    "title": "Multi-Task Learning with torch in R",
    "section": "Anti-Overfitting Strategies Summary",
    "text": "Anti-Overfitting Strategies Summary\nThe tutorial demonstrates several key techniques for preventing overfitting in multi-task learning:\n\nDropout: Applied in shared layers (20% rate) to prevent co-adaptation of neurons\nWeight Decay: L2 regularization in optimizer (1e-4) to prevent large weights\nEarly Stopping: Monitoring validation loss with patience to stop at optimal point\nGradient Clipping: Preventing exploding gradients (max norm = 1.0)\nValidation Split: Proper train/validation/test separation for unbiased evaluation\nModel Selection: Saving and loading best performing model state based on validation performance"
  },
  {
    "objectID": "posts/simple-neural-net-with-torch.html#prerequisites-and-installation",
    "href": "posts/simple-neural-net-with-torch.html#prerequisites-and-installation",
    "title": "Building a Simple Neural Network in R with torch",
    "section": "",
    "text": "To begin, the torch package must be installed from CRAN and the backend configured:\n\n# install.packages(\"torch\")\nlibrary(torch)\n# torch::install_torch()"
  },
  {
    "objectID": "posts/simple-neural-net-with-torch.html#a-simple-neural-network",
    "href": "posts/simple-neural-net-with-torch.html#a-simple-neural-network",
    "title": "Building a Simple Neural Network in R with torch",
    "section": "A Simple Neural Network",
    "text": "A Simple Neural Network\nThis section focuses on the creation of a neural network to perform a simple regression task.\n\n1. Sample Data\n\n# Set seed for reproducibility\nset.seed(42)\n\n# Generate training data: y = 3x + 2 + noise\nx &lt;- torch_randn(100, 1)\ny &lt;- 3 * x + 2 + torch_randn(100, 1) * 0.3\n\n# Display the first few data points\nhead(\n  data.frame(\n    x = as.numeric(x$squeeze()),\n    y = as.numeric(y$squeeze())\n  ))\n\n           x          y\n1  0.3017566  2.5305436\n2 -0.7544662 -0.1921384\n3 -0.5905746  0.7500853\n4  1.2793002  5.8569264\n5 -0.4106060  1.2982252\n6  0.6766803  4.2061977\n\n\n\n\n2. Neural Network Module\nThe next step involves defining the neural network architecture using torch’s module system:\n\n# Define a simple feedforward neural network\nnnet &lt;- nn_module(\n  initialize = function() {\n    # Define layers\n    self$layer1 &lt;- nn_linear(1, 8)  # Input layer to hidden layer (1 -&gt; 8 neurons)\n    self$layer2 &lt;- nn_linear(8, 1)  # Hidden layer to output layer (8 -&gt; 1 neuron)\n  },\n  forward = function(x) {\n    # Define forward pass\n    x %&gt;% \n      self$layer1() %&gt;%     # First linear transformation\n      nnf_relu() %&gt;%     # ReLU activation function\n      self$layer2()         # Second linear transformation\n  }\n)\n\n# Instantiate the model\nmodel &lt;- nnet()\n\n# Display model structure\nprint(model)\n\nAn `nn_module` containing 25 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────\n• layer1: &lt;nn_linear&gt; #16 parameters\n• layer2: &lt;nn_linear&gt; #9 parameters\n\n\n\n\n3. Set Up the Optimizer and Loss Function\nThe training process requires defining how the model will learn from the data:\n\n# Set up optimizer (Adam optimizer with learning rate 0.02)\noptimizer &lt;- optim_adam(model$parameters, lr = 0.02)\n\n# Define loss function (Mean Squared Error for regression)\nloss_fn &lt;- nnf_mse_loss\n\n\n\n4. Training Loop\nThe neural network training process proceeds as follows:\n\n# Store loss values for plotting\nloss_history &lt;- numeric(300)\n\n# Training loop\nfor(epoch in 1:300) {\n  \n  # Set model to training mode\n  model$train()\n  \n  # Reset gradients\n  optimizer$zero_grad()\n  \n  # Forward pass\n  y_pred &lt;- model(x)\n  \n  # Calculate loss\n  loss &lt;- loss_fn(y_pred, y)\n  \n  # Backward pass\n  loss$backward()\n  \n  # Update parameters\n  optimizer$step()\n  \n  # Store loss for plotting\n  loss_history[epoch] &lt;- loss$item()\n}\n\n\n\n5. Visualize the Training Progress\nThe following visualization demonstrates how the loss decreased during training:\n\n# Create a data frame for plotting\ntraining_df &lt;- data.frame(\n  epoch = 1:300,\n  loss = loss_history\n)\n\n# Plot training loss\nggplot(training_df, aes(x = epoch, y = loss)) +\n  geom_line(color = \"#2c3e50\", size = 1) +\n  labs(\n    title = \"Training Loss Over Time\",\n    subtitle = \"Neural Network Learning Progress\",\n    x = \"Epoch\",\n    y = \"Mean Squared Error Loss\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12, color = \"gray60\")\n  )\n\n\n\n\n\n\n\n\n\n\n6. Visualize the Results\nThe following analysis demonstrates how well the trained model performs:\n\n# Set model to evaluation mode\nmodel$eval()\n\n# Generate predictions\nwith_no_grad({\n  y_pred &lt;- model(x)\n})\n\n# Convert to R vectors for plotting\nx_np &lt;- as.numeric(x$squeeze())\ny_np &lt;- as.numeric(y$squeeze())\ny_pred_np &lt;- as.numeric(y_pred$squeeze())\n\n# Create data frame for ggplot\nplot_df &lt;- data.frame(\n  x = x_np,\n  y_actual = y_np,\n  y_predicted = y_pred_np\n)\n\n# Create the plot\nggplot(plot_df, aes(x = x)) +\n  geom_point(aes(y = y_actual, color = \"Actual\"), alpha = 0.7, size = 2) +\n  geom_point(aes(y = y_predicted, color = \"Predicted\"), alpha = 0.7, size = 2) +\n  geom_smooth(aes(y = y_predicted), method = \"loess\", se = FALSE, \n              color = \"#e74c3c\", linetype = \"dashed\") +\n  labs(\n    title = \"Neural Network Regression Results\",\n    subtitle = \"Comparing actual vs predicted values\",\n    x = \"Input (x)\",\n    y = \"Output (y)\",\n    color = \"Data Type\"\n  ) +\n  scale_color_manual(values = c(\"Actual\" = \"#3498db\", \"Predicted\" = \"#e74c3c\")) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12, color = \"gray60\"),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\n\n\n\n7. Model Performance Analysis\nThe following analysis examines how well the model learned the underlying pattern:\n\n# Calculate performance metrics\nmse &lt;- mean((y_pred_np - y_np)^2)\nrmse &lt;- sqrt(mse)\nmae &lt;- mean(abs(y_pred_np - y_np))\nr_squared &lt;- cor(y_pred_np, y_np)^2\n\n# Create performance summary\nperformance_summary &lt;- data.frame(\n  Metric = c(\"Mean Squared Error\", \"Root Mean Squared Error\", \n             \"Mean Absolute Error\", \"R-squared\"),\n  Value = c(mse, rmse, mae, r_squared)\n)\n\nprint(performance_summary)\n\n                   Metric      Value\n1      Mean Squared Error 0.09657499\n2 Root Mean Squared Error 0.31076516\n3     Mean Absolute Error 0.24404984\n4               R-squared 0.99224673\n\n# Compare with true relationship (y = 3x + 2)\n# Generate predictions on a grid for comparison\nx_grid &lt;- torch_linspace(-3, 3, 100)$unsqueeze(2)\nwith_no_grad({\n  y_grid_pred &lt;- model(x_grid)\n})\n\nx_grid_np &lt;- as.numeric(x_grid$squeeze())\ny_grid_pred_np &lt;- as.numeric(y_grid_pred$squeeze())\ny_grid_true &lt;- 3 * x_grid_np + 2\n\n# Plot comparison\ncomparison_df &lt;- data.frame(\n  x = x_grid_np,\n  y_true = y_grid_true,\n  y_predicted = y_grid_pred_np\n)\n\nggplot(comparison_df, aes(x = x)) +\n  geom_line(aes(y = y_true, color = \"True Function\"), size = 2) +\n  geom_line(aes(y = y_predicted, color = \"Neural Network\"), size = 2, linetype = \"dashed\") +\n  geom_point(data = plot_df, aes(y = y_actual), alpha = 0.3, color = \"gray50\") +  labs(\n    title = \"Neural Network vs True Function\",\n    subtitle = \"Model learning assessment against the underlying pattern\",\n    x = \"Input (x)\",\n    y = \"Output (y)\",\n    color = \"Function Type\"\n  ) +\n  scale_color_manual(values = c(\"True Function\" = \"#2c3e50\", \"Neural Network\" = \"#e74c3c\")) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12, color = \"gray60\"),\n    legend.position = \"top\"\n  )"
  },
  {
    "objectID": "posts/simple-neural-net-with-torch.html#experimenting-with-different-architectures",
    "href": "posts/simple-neural-net-with-torch.html#experimenting-with-different-architectures",
    "title": "Building a Simple Neural Network in R with torch",
    "section": "Experimenting with Different Architectures",
    "text": "Experimenting with Different Architectures\nThe following section analyzes the simple network against different architectures:\n\n# Define different network architectures\ncreate_network &lt;- function(hidden_sizes) {\n  nn_module(\n    initialize = function(hidden_sizes) {\n      self$layers &lt;- nn_module_list()\n      \n      # Input layer\n      prev_size &lt;- 1\n      \n      for(i in seq_along(hidden_sizes)) {\n        self$layers$append(nn_linear(prev_size, hidden_sizes[i]))\n        prev_size &lt;- hidden_sizes[i]\n      }\n      # Output layer\n      self$layers$append(nn_linear(prev_size, 1))\n    },\n    forward = function(x) {\n      for(i in 1:(length(self$layers) - 1)) {\n        x &lt;- nnf_relu(self$layers[[i]](x))\n      }\n      # No activation on output layer\n      self$layers[[length(self$layers)]](x)\n    }\n  )\n}\n\n# Train different architectures\narchitectures &lt;- list(\n  \"Simple (8)\" = c(8),\n  \"Deep (16-8)\" = c(16, 8),\n  \"Wide (32)\" = c(32),\n  \"Very Deep (16-16-8)\" = c(16, 16, 8)\n)\n\nresults &lt;- list()\n\nfor(arch_name in names(architectures)) {\n\n  # Create and train model\n  net_class &lt;- create_network(architectures[[arch_name]])\n  model_temp &lt;- net_class(architectures[[arch_name]])\n  optimizer_temp &lt;- optim_adam(model_temp$parameters, lr = 0.01)\n  \n  # Quick training (fewer epochs for comparison)\n  for(epoch in 1:200) {\n    model_temp$train()\n    optimizer_temp$zero_grad()\n    y_pred_temp &lt;- model_temp(x)\n    loss_temp &lt;- loss_fn(y_pred_temp, y)\n    loss_temp$backward()\n    optimizer_temp$step()\n  }\n  \n  # Generate predictions\n  model_temp$eval()\n  with_no_grad({\n    y_pred_arch &lt;- model_temp(x_grid)\n  })\n  \n  results[[arch_name]] &lt;- data.frame(\n    x = x_grid_np,\n    y_pred = as.numeric(y_pred_arch$squeeze()),\n    architecture = arch_name\n  )\n}\n\n# Combine results\nall_results &lt;- do.call(rbind, results)\n\n# Plot comparison\nggplot(all_results, aes(x = x, y = y_pred, color = architecture)) +\n  geom_line(size = 1.2) +\n  geom_line(data = comparison_df, aes(y = y_true, color = \"True Function\"), \n            size = 2, linetype = \"solid\") +\n  geom_point(data = plot_df, aes(x = x, y = y_actual), \n             color = \"gray50\", alpha = 0.3, inherit.aes = FALSE) +  labs(\n               title = \"Comparison of Different Neural Network Architectures\",\n               subtitle = \"Effects of network depth and width on learning performance\",\n               x = \"Input (x)\",\n               y = \"Output (y)\",\n               color = \"Architecture\"\n             ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12, color = \"gray60\"),\n    legend.position = \"top\"\n  )"
  },
  {
    "objectID": "posts/particle-swarm-optimization.html#prerequisites-and-required-libraries",
    "href": "posts/particle-swarm-optimization.html#prerequisites-and-required-libraries",
    "title": "Portfolio Optimization Using PSO",
    "section": "Prerequisites and Required Libraries",
    "text": "Prerequisites and Required Libraries\nThe analysis requires the following R packages to be loaded:\n\n# Load required packages\nlibrary(pso)       # For PSO implementation (provides psoptim function)\nlibrary(ggplot2)   # For data visualization\nlibrary(dplyr)     # For data manipulation and transformation\nlibrary(quantmod)  # For downloading financial data\nlibrary(tidyr)     # For reshaping data (pivot_wider, gather functions)\nlibrary(plotly)    # For creating interactive 3D visualizations"
  },
  {
    "objectID": "posts/particle-swarm-optimization.html#libraries",
    "href": "posts/particle-swarm-optimization.html#libraries",
    "title": "Portfolio Optimization Using PSO",
    "section": "Libraries",
    "text": "Libraries\n\n# Load required packages\nlibrary(pso)       # For PSO implementation (provides psoptim function)\nlibrary(ggplot2)   # For data visualization\nlibrary(dplyr)     # For data manipulation and transformation\nlibrary(quantmod)  # For downloading financial data\nlibrary(tidyr)     # For reshaping data (pivot_wider, gather functions)\nlibrary(plotly)    # For creating interactive 3D visualizations"
  },
  {
    "objectID": "posts/particle-swarm-optimization.html#data-collection",
    "href": "posts/particle-swarm-optimization.html#data-collection",
    "title": "Portfolio Optimization Using PSO",
    "section": "Data Collection",
    "text": "Data Collection\nThe first step in portfolio optimization involves gathering the necessary data. Historical price data is required to calculate returns and risk metrics.\n\nGetting Stock Tickers\nThis demonstration utilizes stocks from the NIFTY50 index, which includes the 50 largest Indian companies by market capitalization:\n\n# Read ticker list from NSE (National Stock Exchange of India) website\nticker_list &lt;- read.csv(\"https://raw.githubusercontent.com/royr2/datasets/refs/heads/main/ind_nifty50list.csv\")\n\n# View the first few rows to understand the data structure\nhead(ticker_list[,1:3], 5)\n\n                                Company.Name           Industry     Symbol\n1                     Adani Enterprises Ltd.    Metals & Mining   ADANIENT\n2 Adani Ports and Special Economic Zone Ltd.           Services ADANIPORTS\n3           Apollo Hospitals Enterprise Ltd.         Healthcare APOLLOHOSP\n4                          Asian Paints Ltd.  Consumer Durables ASIANPAINT\n5                             Axis Bank Ltd. Financial Services   AXISBANK\n\n\n\n\nDownloading Historical Price Data\nThe next step involves downloading historical price data for these stocks using the quantmod package, which provides an interface to Yahoo Finance:\n\n# Append \".NS\" to tickers for Yahoo Finance format (NS = National Stock Exchange)\ntickers &lt;- paste0(ticker_list$Symbol, \".NS\")\ntickers &lt;- tickers[!tickers %in% c(\"ETERNAL.NS\", \"JIOFIN.NS\")]\n\n# Initialize empty dataframe to store all ticker data\nticker_df &lt;- data.frame()\n\n# Create a progress bar to monitor the download process\n# pb &lt;- txtProgressBar(min = 1, max = length(tickers), style = 3)\n\n# Loop through each ticker and download its historical data\nfor(nms in tickers){\n  # Download data from Yahoo Finance\n  df &lt;- getSymbols(Symbols = nms, verbose = FALSE, auto.assign = FALSE)\n  \n  # Rename columns for clarity\n  colnames(df) &lt;- c(\"open\", \"high\", \"low\", \"close\", \"volume\", \"adjusted\")\n  df$date = rownames(df)\n  \n  # Convert to dataframe and add ticker and date information\n  df &lt;- data.frame(df)\n  df$ticker &lt;- nms\n  df$date &lt;- rownames(df)\n  \n  # Append to the main dataframe\n  ticker_df &lt;- rbind(ticker_df, df)\n  \n  Sys.sleep(0.2)\n  \n  # Update progress bar\n  # setTxtProgressBar(pb, which(tickers == nms))\n}\n\n# Reshape data to wide format with dates as rows and tickers as columns\n# This format facilitates the calculation of returns across all stocks\nprices_df &lt;- pivot_wider(data = ticker_df, id_cols = \"date\", names_from = \"ticker\", values_from = \"close\")\n\n# Remove rows with missing values to ensure complete data\nprices_df &lt;- na.omit(prices_df)\n\n# Check the date range of our data\nrange(prices_df$date)\n\n[1] \"2017-11-17\" \"2025-06-20\"\n\n# Check dimensions (number of trading days × number of stocks + date column)\ndim(prices_df)\n\n[1] 1874   49\n\n\n\n\nVisualizing the Data\nBefore proceeding with analysis, examining the data through visualization helps identify anomalies and understand general trends. The following visualization displays the price data for a subset of stocks (focusing on the metals industry):\n\n# Plot closing prices for metal stocks\nprices_df %&gt;% \n  # Convert from wide to long format for easier plotting with ggplot2\n  pivot_longer(-date, names_to = \"ticker\", values_to = \"price\") %&gt;% \n  \n  # Attach industry information from our original ticker list\n  left_join(ticker_list %&gt;% \n              mutate(ticker = paste0(Symbol, \".NS\")) %&gt;% \n              select(ticker, industry = Industry),\n            by = \"ticker\") %&gt;% \n  \n  # Convert date strings to Date objects\n  mutate(date = as.Date(date)) %&gt;% \n  \n  # Filter to show only metal industry stocks for clarity\n  filter(stringr::str_detect(tolower(industry), \"metal\")) %&gt;% \n  \n  # Create the line plot\n  ggplot(aes(x = date, y = price, color = ticker)) + \n  geom_line(linewidth = 0.8) + \n  theme_minimal() + \n  scale_color_brewer(palette = \"RdBu\") +  # Use a color-blind friendly palette\n  labs(title = \"Closing Prices\", \n       subtitle = \"Nifty 50 metal stocks\",\n       x = \"Date\", \n       y = \"Closing Price\") + \n  theme(legend.position = \"top\", \n        legend.title = element_text(colour = \"transparent\"), \n        axis.title.x = element_text(face = \"bold\"), \n        axis.title.y = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\nClosing prices for metal stocks\n\n\nThe visualization demonstrates the price movements of metal stocks over time. The data reveals periods of both correlation and divergence between different stocks, highlighting the importance of diversification in portfolio construction.\n\n\nCalculating Returns\nFor portfolio optimization, we need to work with returns rather than prices. Returns better represent the investment performance and have more desirable statistical properties (like stationarity):\n\n# Calculate daily returns for all stocks\n# Formula: (Price_today / Price_yesterday) - 1\nreturns_df &lt;- apply(prices_df[,-1], 2, function(vec){\n  ret &lt;- vec/lag(vec) - 1  # Simple returns calculation\n  return(ret)\n})\n\n# Convert to dataframe for easier manipulation\nreturns_df &lt;- as.data.frame(returns_df)\n\n# Remove first row which contains NA values (no previous day to calculate return)\nreturns_df &lt;- returns_df[-1,]  \n\n# Pre-compute average returns and covariance matrix for optimization\n# These constitute key inputs to the mean-variance optimization\nmean_returns &lt;- sapply(returns_df, mean)  # Expected returns\ncov_mat &lt;- cov(returns_df)  # Risk (covariance) matrix\n\nThe mean returns represent expectations for each asset’s performance, while the covariance matrix captures both the individual volatilities and the relationships between assets. These serve as the primary inputs to the optimization process."
  },
  {
    "objectID": "posts/portfolio_optimisation_pso.html",
    "href": "posts/portfolio_optimisation_pso.html",
    "title": "Portfolio Optimization Using PSO",
    "section": "",
    "text": "Portfolio optimization represents a critical task in investment management, where the goal involves allocating capital across different assets to maximize returns while controlling risk. This post explores how to use Particle Swarm Optimization (PSO) to perform mean-variance portfolio optimization with various constraints."
  },
  {
    "objectID": "posts/portfolio_optimisation_pso.html#libraries",
    "href": "posts/portfolio_optimisation_pso.html#libraries",
    "title": "Portfolio Optimization Using PSO",
    "section": "Libraries",
    "text": "Libraries\n\n# Load required packages\nlibrary(pso)       # For PSO implementation (provides psoptim function)\nlibrary(ggplot2)   # For data visualization\nlibrary(dplyr)     # For data manipulation and transformation\nlibrary(quantmod)  # For downloading financial data\nlibrary(tidyr)     # For reshaping data (pivot_wider, gather functions)\nlibrary(plotly)    # For creating interactive 3D visualizations"
  },
  {
    "objectID": "posts/portfolio_optimisation_pso.html#data-collection",
    "href": "posts/portfolio_optimisation_pso.html#data-collection",
    "title": "Portfolio Optimization Using PSO",
    "section": "Data Collection",
    "text": "Data Collection\nThe first step is to gather historical price data to calculate returns and risk metrics.\n\nGetting Stock Tickers\nThis post uses stocks from the NIFTY50 index, which includes the 50 largest Indian companies by market capitalisation:\n\n# Read ticker list from NSE (National Stock Exchange of India) website\nticker_list &lt;- read.csv(\"https://raw.githubusercontent.com/royr2/datasets/refs/heads/main/ind_nifty50list.csv\")\n\n# View the first few rows \nhead(ticker_list[,1:3], 5)\n\n                                Company.Name           Industry     Symbol\n1                     Adani Enterprises Ltd.    Metals & Mining   ADANIENT\n2 Adani Ports and Special Economic Zone Ltd.           Services ADANIPORTS\n3           Apollo Hospitals Enterprise Ltd.         Healthcare APOLLOHOSP\n4                          Asian Paints Ltd.  Consumer Durables ASIANPAINT\n5                             Axis Bank Ltd. Financial Services   AXISBANK\n\n\n\n\nDownloading Historical Price Data\nThe next step involves downloading historical price data for these stocks using the quantmod package, which provides an interface to Yahoo Finance:\n\n# Append \".NS\" to tickers for Yahoo Finance format (NS = National Stock Exchange)\ntickers &lt;- paste0(ticker_list$Symbol, \".NS\")\ntickers &lt;- tickers[!tickers %in% c(\"ETERNAL.NS\", \"JIOFIN.NS\")]\n\n# Initialize empty dataframe to store all ticker data\nticker_df &lt;- data.frame()\n\n# Create a progress bar to monitor the download process\n# pb &lt;- txtProgressBar(min = 1, max = length(tickers), style = 3)\n\n# Loop through each ticker and download its historical data\nfor(nms in tickers){\n  # Download data from Yahoo Finance\n  df &lt;- getSymbols(Symbols = nms, verbose = FALSE, auto.assign = FALSE)\n  \n  # Rename columns for clarity\n  colnames(df) &lt;- c(\"open\", \"high\", \"low\", \"close\", \"volume\", \"adjusted\")\n  df$date = rownames(df)\n  \n  # Convert to dataframe and add ticker and date information\n  df &lt;- data.frame(df)\n  df$ticker &lt;- nms\n  df$date &lt;- rownames(df)\n  \n  # Append to the main dataframe\n  ticker_df &lt;- rbind(ticker_df, df)\n  \n  Sys.sleep(0.2)\n  \n  # Update progress bar\n  # setTxtProgressBar(pb, which(tickers == nms))\n}\n\n# Reshape data to wide format with dates as rows and tickers as columns\n# This format facilitates the calculation of returns across all stocks\nprices_df &lt;- pivot_wider(data = ticker_df, id_cols = \"date\", names_from = \"ticker\", values_from = \"close\")\n\n# Remove rows with missing values to ensure complete data\nprices_df &lt;- na.omit(prices_df)\n\n# Check the date range of our data\nrange(prices_df$date)\n\n[1] \"2017-11-17\" \"2025-06-20\"\n\n# Check dimensions (number of trading days × number of stocks + date column)\ndim(prices_df)\n\n[1] 1874   49\n\n\n\n\nVisualizing the Data\n\n# Plot closing prices for metal stocks\nprices_df %&gt;% \n  # Convert from wide to long format for easier plotting with ggplot2\n  pivot_longer(-date, names_to = \"ticker\", values_to = \"price\") %&gt;% \n  \n  # Attach industry information from our original ticker list\n  left_join(ticker_list %&gt;% \n              mutate(ticker = paste0(Symbol, \".NS\")) %&gt;% \n              select(ticker, industry = Industry),\n            by = \"ticker\") %&gt;% \n  \n  # Convert date strings to Date objects\n  mutate(date = as.Date(date)) %&gt;% \n  \n  # Filter to show only metal industry stocks for clarity\n  filter(stringr::str_detect(tolower(industry), \"metal\")) %&gt;% \n  \n  # Create the line plot\n  ggplot(aes(x = date, y = price, color = ticker)) + \n  geom_line(linewidth = 0.8) + \n  theme_minimal() + \n  scale_color_brewer(palette = \"RdBu\") +  # Use a color-blind friendly palette\n  labs(title = \"Closing Prices\", \n       subtitle = \"Nifty 50 metal stocks\",\n       x = \"Date\", \n       y = \"Closing Price\") + \n  theme(legend.position = \"top\", \n        legend.title = element_text(colour = \"transparent\"), \n        axis.title.x = element_text(face = \"bold\"), \n        axis.title.y = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\nThe visualization demonstrates the price movements of metal stocks over time. The data reveals periods of both correlation and divergence between different stocks, highlighting the importance of diversification in portfolio construction.\n\n\nCalculating Returns\nFor portfolio optimization, we need to work with returns rather than prices. Returns better represent the investment performance and have more desirable statistical properties (like stationarity):\n\n# Calculate daily returns for all stocks\n# Formula: (Price_today / Price_yesterday) - 1\nreturns_df &lt;- apply(prices_df[,-1], 2, function(vec){\n  ret &lt;- vec/lag(vec) - 1  # Simple returns calculation\n  return(ret)\n})\n\n# Convert to dataframe for easier manipulation\nreturns_df &lt;- as.data.frame(returns_df)\n\n# Remove first row which contains NA values (no previous day to calculate return)\nreturns_df &lt;- returns_df[-1,]  \n\n# Pre-compute average returns and covariance matrix for optimization\n# These constitute key inputs to the mean-variance optimization\nmean_returns &lt;- sapply(returns_df, mean)  # Expected returns\ncov_mat &lt;- cov(returns_df)  # Risk (covariance) matrix\n\nThe mean returns represent expectations for each asset’s performance, while the covariance matrix captures both the individual volatilities and the relationships between assets. These serve as inputs to the optimization process."
  },
  {
    "objectID": "posts/portfolio_optimisation_pso.html#portfolio-optimization-framework",
    "href": "posts/portfolio_optimisation_pso.html#portfolio-optimization-framework",
    "title": "Portfolio Optimization Using PSO",
    "section": "Portfolio Optimization Framework",
    "text": "Portfolio Optimization Framework\n\nObjective Function\nIn mean-variance optimisation the objective function, which defines the optimization target balances three key components:\n\nExpected returns (reward): The weighted average of expected returns for each asset\nPortfolio variance (risk): A measure of the portfolio’s volatility, calculated using the covariance matrix\nRisk aversion parameter: Controls the trade-off between risk and return (higher values prioritize risk reduction)\n\nThe implementation also incorporates constraints through penalty terms:\n\nobj_func &lt;- function(wts, \n                     risk_av = 10,  # Risk aversion parameter\n                     lambda1 = 10,  # Penalty weight for full investment constraint\n                     lambda2 = 1e2,  # Reserved for additional constraints\n                     ret_vec, cov_mat){\n  \n  # Calculate expected portfolio return (weighted average of asset returns)\n  port_returns &lt;- ret_vec %*% wts\n  \n  # Calculate portfolio risk (quadratic form using covariance matrix)\n  port_risk &lt;- t(wts) %*% cov_mat %*% wts\n  \n  # Mean-variance utility function: return - risk_aversion * risk\n  # This is the core Markowitz portfolio optimization formula\n  obj &lt;- port_returns - risk_av * port_risk\n  \n  # Add penalty for violating the full investment constraint (sum of weights = 1)\n  # The squared term ensures the penalty increases quadratically with violation size\n  obj &lt;- obj - lambda1 * (sum(wts) - 1)^2\n    # Return negative value since PSO minimizes by default, but the goal is to maximize\n  # the objective (higher returns, lower risk)\n  return(-obj)\n}\n\nThis objective function implements the classic mean-variance utility with a quadratic penalty for the full investment constraint. The risk aversion parameter allows us to move along the efficient frontier to find portfolios with different risk-return profiles."
  },
  {
    "objectID": "posts/portfolio_optimisation_pso.html#two-asset-example",
    "href": "posts/portfolio_optimisation_pso.html#two-asset-example",
    "title": "Portfolio Optimization Using PSO",
    "section": "Two-Asset Example",
    "text": "Two-Asset Example\nBefore tackling the full portfolio optimization problem, this section begins with a simple two-asset example. This approach helps visualize how PSO works and validates the methodology:\n\n# Use only the first two assets for this example\n# Calculate their average returns and covariance matrix\nmean_returns_small &lt;- apply(returns_df[,1:2], 2, mean)\ncov_mat_small &lt;- cov(returns_df[,1:2])\n\n# Define a custom PSO optimizer function to track the optimization process\npso_optim &lt;- function(obj_func,\n                      c1 = 0.05,      # Cognitive parameter (personal best influence)\n                      c2 = 0.05,      # Social parameter (global best influence)\n                      w = 0.8,        # Inertia weight (controls momentum)\n                      init_fact = 0.1, # Initial velocity factor\n                      n_particles = 20, # Number of particles in the swarm\n                      n_dim = 2,       # Dimensionality (number of assets)\n                      n_iter = 50,     # Maximum iterations\n                      upper = 1,       # Upper bound for weights\n                      lower = 0,       # Lower bound for weights (no short selling)\n                      n_avg = 10,      # Number of iterations for averaging\n                      ...){\n  \n  # Initialize particle positions randomly within bounds\n  X &lt;- matrix(runif(n_particles * n_dim), nrow = n_particles)\n  X &lt;- X * (upper - lower) + lower  # Scale to fit within bounds\n  \n  # Initialize particle velocities (movement speeds)\n  dX &lt;- matrix(runif(n_particles * n_dim) * init_fact, ncol = n_dim)\n  dX &lt;- dX * (upper - lower) + lower\n  \n  # Initialize personal best positions and objective values\n  pbest &lt;- X  # Each particle's best position so far\n  pbest_obj &lt;- apply(X, 1, obj_func, ...)  # Objective value at personal best\n  \n  # Initialize global best position and objective value\n  gbest &lt;- pbest[which.min(pbest_obj),]  # Best position across all particles\n  gbest_obj &lt;- min(pbest_obj)  # Best objective value found\n  \n  # Store initial positions for visualization\n  loc_df &lt;- data.frame(X, iter = 0, obj = pbest_obj)\n  iter &lt;- 1\n  \n  # Main PSO loop\n  while(iter &lt; n_iter){\n    \n    # Update velocities using PSO formula:\n    # New velocity = inertia + cognitive component + social component\n    dX &lt;- w * dX +                         # Inertia (continue in same direction)\n          c1*runif(1)*(pbest - X) +        # Pull toward personal best\n          c2*runif(1)*t(gbest - t(X))      # Pull toward global best\n    \n    # Update positions based on velocities\n    X &lt;- X + dX\n    \n    # Evaluate objective function at new positions\n    obj &lt;- apply(X, 1, obj_func, ...)\n    \n    # Update personal bests if new positions are better\n    idx &lt;- which(obj &lt;= pbest_obj)\n    pbest[idx,] &lt;- X[idx,]\n    pbest_obj[idx] &lt;- obj[idx]\n    \n    # Update global best if a better solution is found\n    idx &lt;- which.min(pbest_obj)\n    gbest &lt;- pbest[idx,]\n    gbest_obj &lt;- min(pbest_obj)\n    \n    # Store current state for visualization\n    iter &lt;- iter + 1\n    loc_df &lt;- rbind(loc_df, data.frame(X, iter = iter, obj = pbest_obj))\n  }\n  \n  # Return optimization results\n  lst &lt;- list(X = loc_df,          # All particle positions throughout optimization\n              obj = gbest_obj,     # Best objective value found\n              obj_loc = gbest)     # Weights that achieved the best objective\n  return(lst)\n}\n\n# Run the optimization for our two-asset portfolio\nout &lt;- pso_optim(obj_func,\n                 ret_vec = mean_returns_small,  # Expected returns\n                 cov_mat = cov_mat_small,       # Covariance matrix\n                 lambda1 = 10, risk_av = 100,    # Constraint and risk parameters\n                 n_particles = 100,              # Use 100 particles for better coverage\n                 n_dim = 2,                      # Two-asset portfolio\n                 n_iter = 200,                   # Run for 200 iterations\n                 upper = 1, lower = 0,           # Bounds for weights\n                 c1 = 0.02, c2 = 0.02,           # Lower influence parameters for stability\n                 w = 0.05, init_fact = 0.01)     # Low inertia for better convergence\n\n# Verify that the weights sum to approximately 1 (full investment constraint)\nsum(out$obj_loc)\n\n[1] 0.9906321\n\n\nIn this implementation, the tracking of all particle movements throughout the optimization process occurs. This enables visualization of how the swarm converges toward the optimal solution.\n\nVisualizing the Optimization Process\nOne advantage of starting with a two-asset example is the ability to visualize the entire search space and observe how the PSO algorithm explores it. The following creates a 3D visualization of the objective function landscape and the path each particle took during optimization:\n\n# Create a fine grid of points covering the feasible region (all possible weight combinations)\ngrid &lt;- expand.grid(x = seq(0, 1, by = 0.01),  # First asset weight from 0 to 1\n                    y = seq(0, 1, by = 0.01))   # Second asset weight from 0 to 1\n\n# Evaluate the objective function at each grid point to create the landscape\ngrid$obj &lt;- apply(grid, 1, obj_func, \n                  ret_vec = mean_returns_small, \n                  cov_mat = cov_mat_small, \n                  lambda1 = 10, risk_av = 100)\n\n# Create an interactive 3D plot showing both the objective function surface\n# and the particle trajectories throughout the optimization\np &lt;- plot_ly() %&gt;% \n  # Add the objective function surface as a mesh\n  add_mesh(data = grid, x = ~x, y = ~y, z = ~obj, \n           inherit = FALSE, color = \"red\") %&gt;% \n  \n  # Add particles as markers, colored by iteration to show progression\n  add_markers(data = out$X, x = ~X1, y = ~X2, z = ~obj, \n              color = ~ iter, inherit = FALSE, \n              marker = list(size = 2))\n\nThis visualization demonstrates:\n\nThe objective function landscape as a 3D surface\nThe particles (small dots) exploring the search space\nHow the swarm converges toward the optimal solution over iterations (color gradient)\n\nThe concentration of particles in certain regions indicates where the algorithm found promising solutions. The global best solution represents where the particles ultimately converge."
  },
  {
    "objectID": "posts/portfolio_optimisation_pso.html#multi-asset-portfolio-optimization",
    "href": "posts/portfolio_optimisation_pso.html#multi-asset-portfolio-optimization",
    "title": "Portfolio Optimization Using PSO",
    "section": "Multi-Asset Portfolio Optimization",
    "text": "Multi-Asset Portfolio Optimization\nTo scale the problem to multiple assets, instead of using the custom PSO implementation, this section leverages the psoptim function from the pso package:\n\n# Get the number of stocks in the dataset\nn_stocks &lt;- ncol(returns_df)\n\n# Run the PSO optimization for the full portfolio\nopt &lt;- psoptim(\n  # Initial particle positions (starting with equal weights)\n  par = rep(0, n_stocks),\n  \n  # Objective function to minimize\n  fn = obj_func,\n  \n  # Pass the expected returns and covariance matrix\n  ret_vec = mean_returns, \n  cov_mat = cov_mat,\n  \n  # Set constraint parameters\n  lambda1 = 10,  # Weight for full investment constraint\n  risk_av = 1000,  # Higher risk aversion for a more conservative portfolio\n  \n  # Set bounds for weights (no short selling allowed)\n  lower = rep(0, n_stocks),\n  upper = rep(1, n_stocks),\n  \n  # Configure the PSO algorithm\n  control = list(\n    maxit = 200,          # Maximum iterations\n    s = 100,               # Swarm size (number of particles)\n    maxit.stagnate = 500   # Stop if no improvement after this many iterations\n  )\n)\n\n# Calculate and display the expected return of the optimized portfolio\npaste(\"Portfolio returns:\", round(opt$par %*% mean_returns, 5))\n\n[1] \"Portfolio returns: 0.00061\"\n\n# Calculate and display the standard deviation (risk) of the optimized portfolio\npaste(\"Portfolio Std dev:\", round(sqrt(opt$par %*% cov_mat %*% opt$par), 5))\n\n[1] \"Portfolio Std dev: 0.00879\"\n\n# Verify that the weights sum to approximately 1 (full investment constraint)\nsum(opt$par)\n\n[1] 0.9950411"
  },
  {
    "objectID": "posts/portfolio_optimisation_pso.html#adding-tracking-error-constraint",
    "href": "posts/portfolio_optimisation_pso.html#adding-tracking-error-constraint",
    "title": "Portfolio Optimization Using PSO",
    "section": "Adding Tracking Error Constraint",
    "text": "Adding Tracking Error Constraint\nTracking error measures how closely a portfolio follows a benchmark.\n\n# Define benchmark portfolio (equally weighted across all stocks)\nbench_wts &lt;- rep(1/n_stocks, n_stocks)\n\n# Calculate the time series of benchmark returns\nbench_returns &lt;- as.matrix(returns_df) %*% t(t(bench_wts))\n\n# Create a new objective function that includes tracking error\nobj_func_TE &lt;- function(wts,  \n                        risk_av = 10,     # Risk aversion parameter\n                        lambda1 = 10,    # Full investment constraint weight\n                        lambda2 = 50,    # Tracking error constraint weight\n                        ret_vec, cov_mat){\n  \n  # Calculate portfolio metrics\n  port_returns &lt;- ret_vec %*% wts                      # Expected portfolio return\n  port_risk &lt;- t(wts) %*% cov_mat %*% wts             # Portfolio variance\n  port_returns_ts &lt;- as.matrix(returns_df) %*% t(t(wts))  # Time series of portfolio returns\n  \n  # Original mean-variance objective\n  obj &lt;- port_returns - risk_av * port_risk\n  \n  # Full investment constraint (weights sum to 1)\n  obj &lt;- obj - lambda1 * (sum(wts) - 1)^2\n  \n  # Tracking error constraint (penalize deviation from benchmark)\n  # Tracking error is measured as the standard deviation of the difference\n  # between portfolio returns and benchmark returns\n  obj &lt;- obj - lambda2 * sd(port_returns_ts - bench_returns)\n  \n  return(-obj)  # Return negative for minimization\n}\n\n# Run optimization with the tracking error constraint\nopt &lt;- psoptim(\n  # Initial particle positions\n  par = rep(0, n_stocks),\n  \n  # Use our new objective function with tracking error\n  fn = obj_func_TE,\n  \n  # Pass the expected returns and covariance matrix\n  ret_vec = mean_returns, \n  cov_mat = cov_mat,\n  \n  # Set constraint parameters\n  lambda1 = 10,    # Weight for full investment constraint\n  risk_av = 1000,  # Risk aversion parameter\n  \n  # Set bounds for weights\n  lower = rep(0, n_stocks),\n  upper = rep(1, n_stocks),\n  \n  # Configure the PSO algorithm\n  control = list(\n    maxit = 200,          # Maximum iterations\n    s = 100,               # Swarm size\n    maxit.stagnate = 500   # Stop if no improvement after this many iterations\n  )\n)\n\n# Calculate and display the expected return of the optimized portfolio\npaste(\"Portfolio returns:\", round(opt$par %*% mean_returns, 5))\n\n[1] \"Portfolio returns: 0.00075\"\n\n# Calculate and display the standard deviation (risk) of the optimized portfolio\npaste(\"Portfolio Std dev:\", round(sqrt(opt$par %*% cov_mat %*% opt$par), 5))\n\n[1] \"Portfolio Std dev: 0.01087\"\n\n# Verify that the weights sum to approximately 1\nsum(opt$par)\n\n[1] 0.9938126\n\n\nAdding the tracking error constraint, not only balances risk and return but also tracks the performance of an equally-weighted benchmark. The lambda2 parameter controls the closeness of benchmark tracking - higher values result in portfolios that more closely resemble the benchmark."
  },
  {
    "objectID": "posts/portfolio_optimisation_pso.html#advantages-and-limitations-of-pso",
    "href": "posts/portfolio_optimisation_pso.html#advantages-and-limitations-of-pso",
    "title": "Portfolio Optimization Using PSO",
    "section": "Advantages and Limitations of PSO",
    "text": "Advantages and Limitations of PSO\n\nAdvantages:\n\nFlexibility: PSO can handle non-convex, non-differentiable objective functions, making it suitable for complex portfolio constraints that traditional optimizers struggle with\nSimplicity: The algorithm is intuitive and relatively easy to implement compared to other global optimization techniques\nConstraints: Various constraints can be easily incorporated through penalty functions without reformulating the entire problem\nGlobal Search: PSO explores the search space more thoroughly and is less likely to get stuck in local optima compared to gradient-based methods\nParallelization: The algorithm is naturally parallelizable, as particles can be evaluated independently\n\n\n\nLimitations:\n\nVariability: Results can vary between runs due to the stochastic nature of the algorithm, potentially leading to inconsistent portfolio recommendations\nParameter Tuning: Performance significantly depends on parameters like inertia weight and acceleration coefficients, which may require careful tuning\nConvergence: There’s no mathematical guarantee of convergence to the global optimum, unlike some convex optimization methods\nComputational Cost: Can be computationally intensive for high-dimensional problems with many assets\nConstraint Handling: While flexible, the penalty function approach may not always satisfy constraints exactly"
  },
  {
    "objectID": "posts/portfolio_optimisation_pso.html#practical-applications",
    "href": "posts/portfolio_optimisation_pso.html#practical-applications",
    "title": "Portfolio Optimization Using PSO",
    "section": "Practical Applications",
    "text": "Practical Applications\nPSO-based portfolio optimization proves particularly valuable in scenarios where:\n\nTraditional quadratic programming approaches fail due to complex constraints\nThe objective function includes non-linear terms like higher moments (skewness, kurtosis)\nMultiple competing objectives need to be balanced\nThe portfolio needs to satisfy regulatory or client-specific constraints\n\nThe approach demonstrated in this post can be extended to include additional constraints such as:\n\nSector or industry exposure limits\nMaximum position sizes\nTurnover or transaction cost constraints\nRisk factor exposures and limits\nCardinality constraints (limiting the number of assets)"
  },
  {
    "objectID": "posts/multi-task-learning-with-torch.html#packages",
    "href": "posts/multi-task-learning-with-torch.html#packages",
    "title": "Multi-Task Learning with torch in R",
    "section": "Packages",
    "text": "Packages\n\n# install.packages(c(\"torch\", \"tidyverse\", \"corrplot\"))\nlibrary(torch)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/multi-task-learning-with-torch.html#creating-a-mtl-model",
    "href": "posts/multi-task-learning-with-torch.html#creating-a-mtl-model",
    "title": "Multi-Task Learning with torch in R",
    "section": "Creating a MTL Model",
    "text": "Creating a MTL Model\nThe implementation will construct a model that simultaneously performs two related tasks:\n\nRegression: Predicting a continuous value\nClassification: Predicting a binary outcome\n\n\nSample Data\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Number of samples\nn &lt;- 1000\n\n# Create a dataset with 5 features\nx &lt;- torch_randn(n, 5)\n\n# Task 1 (Regression): Predict continuous value\n# Create a target that's a function of the input features plus some noise\ny_regression &lt;- x[, 1] * 0.7 + x[, 2] * 0.3 - x[, 3] * 0.5 + torch_randn(n) * 0.2\n\n# Task 2 (Classification): Predict binary outcome\n# Create a classification target based on a nonlinear combination of features\nlogits &lt;- x[, 1] * 0.8 - x[, 4] * 0.4 + x[, 5] * 0.6\ny_classification &lt;- (logits &gt; 0)$to(torch_float())\n\n# Split into training (70%) and testing (30%) sets\ntrain_idx &lt;- 1:round(0.7 * n)\ntest_idx &lt;- (round(0.7 * n) + 1):n\n\n# Training data\nx_train &lt;- x[train_idx, ]\ny_reg_train &lt;- y_regression[train_idx]\ny_cls_train &lt;- y_classification[train_idx]\n\n# Testing data\nx_test &lt;- x[test_idx, ]\ny_reg_test &lt;- y_regression[test_idx]\ny_cls_test &lt;- y_classification[test_idx]\n\n\n\nDefine the Multi-Task Neural Network\nThe architecture design creates a neural network with shared layers and task-specific branches:\n\n# Define the multi-task neural network\nmulti_task_net &lt;- nn_module(\n  \"MultiTaskNet\",\n  \n  initialize = function(input_size, \n                        hidden_size, \n                        reg_output_size = 1, \n                        cls_output_size = 1) {\n    \n    self$input_size &lt;- input_size\n    self$hidden_size &lt;- hidden_size\n    self$reg_output_size &lt;- reg_output_size\n    self$cls_output_size &lt;- cls_output_size\n    \n    # Shared layers - these learn representations useful for both tasks\n    self$shared_layer1 &lt;- nn_linear(input_size, hidden_size)\n    self$shared_layer2 &lt;- nn_linear(hidden_size, hidden_size)\n    \n    # Task-specific layers\n    # Regression branch\n    self$regression_layer &lt;- nn_linear(hidden_size, reg_output_size)\n    \n    # Classification branch\n    self$classification_layer &lt;- nn_linear(hidden_size, cls_output_size)\n  },\n  \n  forward = function(x) {\n    # Shared feature extraction\n    shared_features &lt;- x %&gt;%\n      self$shared_layer1() %&gt;%\n      nnf_relu() %&gt;%\n      self$shared_layer2() %&gt;%\n      nnf_relu()\n    \n    # Task-specific predictions\n    regression_output &lt;- self$regression_layer(shared_features)\n    classification_logits &lt;- self$classification_layer(shared_features)\n    \n    list(\n      regression = regression_output,\n      classification = classification_logits\n    )\n  }\n)\n\n# Create model instance\nmodel &lt;- multi_task_net(\n  input_size = 5,\n  hidden_size = 10\n)\n\n# Print model architecture\nprint(model)\n\nAn `nn_module` containing 192 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────\n• shared_layer1: &lt;nn_linear&gt; #60 parameters\n• shared_layer2: &lt;nn_linear&gt; #110 parameters\n• regression_layer: &lt;nn_linear&gt; #11 parameters\n• classification_layer: &lt;nn_linear&gt; #11 parameters\n\n\n\n\n4. Define Loss Functions and Optimizer\nMulti-task learning requires separate loss functions for each task.\n\n# Loss functions\nregression_loss_fn &lt;- nnf_mse_loss  # Mean squared error for regression\nclassification_loss_fn &lt;- nnf_binary_cross_entropy_with_logits  # Binary cross-entropy for classification\n\n# Optimizer with weight decay for L2 regularization\noptimizer &lt;- optim_adam(model$parameters, lr = 0.01)\n\n# Task weights - these control the relative importance of each task\ntask_weights &lt;- c(regression = 0.5, classification = 0.5)\n\n# Validation split from training data\nval_size &lt;- round(0.2 * length(train_idx))\nval_indices &lt;- sample(train_idx, val_size)\ntrain_indices &lt;- setdiff(train_idx, val_indices)\n\n# Create validation sets\nx_val &lt;- x[val_indices, ]\ny_reg_val &lt;- y_regression[val_indices]\ny_cls_val &lt;- y_classification[val_indices]\n\n# Update training sets\nx_train &lt;- x[train_indices, ]\ny_reg_train &lt;- y_regression[train_indices]\ny_cls_train &lt;- y_classification[train_indices]\n\n\n\nTraining Loop\n\n# Hyperparameters\nepochs &lt;- 100  # Increased epochs since we have early stopping\n\n# Enhanced training history tracking\ntraining_history &lt;- data.frame(\n  epoch = integer(),\n  train_reg_loss = numeric(),\n  train_cls_loss = numeric(),\n  train_total_loss = numeric(),\n  val_reg_loss = numeric(),\n  val_cls_loss = numeric(),\n  val_total_loss = numeric(),\n  val_accuracy = numeric()\n)\n\nfor (epoch in 1:epochs) {\n  # Training phase\n  model$train()\n  optimizer$zero_grad()\n  \n  # Forward pass on training data\n  outputs &lt;- model(x_train)\n  \n  # Calculate training loss for each task\n  train_reg_loss &lt;- regression_loss_fn(\n    outputs$regression$squeeze(), \n    y_reg_train\n  )\n  \n  train_cls_loss &lt;- classification_loss_fn(\n    outputs$classification$squeeze(), \n    y_cls_train\n  )\n  \n  # Weighted combined training loss\n  train_total_loss &lt;- task_weights[\"regression\"] * train_reg_loss + \n    task_weights[\"classification\"] * train_cls_loss\n  \n  # Backward pass and optimize\n  train_total_loss$backward()\n  \n  # Gradient clipping to prevent exploding gradients\n  nn_utils_clip_grad_norm_(model$parameters, max_norm = 1.0)\n  \n  optimizer$step()\n  \n  # Validation phase\n  model$eval()\n  \n  with_no_grad({\n    val_outputs &lt;- model(x_val)\n    \n    # Calculate validation losses\n    val_reg_loss &lt;- regression_loss_fn(\n      val_outputs$regression$squeeze(), \n      y_reg_val\n    )\n    \n    val_cls_loss &lt;- classification_loss_fn(\n      val_outputs$classification$squeeze(), \n      y_cls_val\n    )\n    \n    val_total_loss &lt;- task_weights[\"regression\"] * val_reg_loss + task_weights[\"classification\"] * val_cls_loss\n    \n    # Calculate validation accuracy\n    val_cls_probs &lt;- nnf_sigmoid(val_outputs$classification$squeeze())\n    val_cls_preds &lt;- (val_cls_probs &gt; 0.5)$to(torch_int())\n    val_accuracy &lt;- (val_cls_preds == y_cls_val$to(torch_int()))$sum()$item() / length(val_indices)\n  })\n  \n  # Record history\n  training_history &lt;- rbind(\n    training_history,\n    data.frame(\n      epoch = epoch,\n      train_reg_loss = as.numeric(train_reg_loss$item()),\n      train_cls_loss = as.numeric(train_cls_loss$item()),\n      train_total_loss = as.numeric(train_total_loss$item()),\n      val_reg_loss = as.numeric(val_reg_loss$item()),\n      val_cls_loss = as.numeric(val_cls_loss$item()),\n      val_total_loss = as.numeric(val_total_loss$item()),\n      val_accuracy = val_accuracy\n    )\n  )\n  \n  # Print progress every 25 epochs\n  if (epoch %% 25 == 0 || epoch == 1) {\n    cat(sprintf(\"Epoch %d - Train Loss: %.4f, Val Loss: %.4f, Val Acc: %.3f\\n\", \n                epoch, \n                train_total_loss$item(), \n                val_total_loss$item(), \n                val_accuracy))\n  }\n\n}\n\nEpoch 1 - Train Loss: 0.8108, Val Loss: 0.7960, Val Acc: 0.457\nEpoch 25 - Train Loss: 0.3391, Val Loss: 0.3333, Val Acc: 0.793\nEpoch 50 - Train Loss: 0.1598, Val Loss: 0.1629, Val Acc: 0.921\nEpoch 75 - Train Loss: 0.0751, Val Loss: 0.0821, Val Acc: 0.971\nEpoch 100 - Train Loss: 0.0490, Val Loss: 0.0587, Val Acc: 0.979\n\n\n\n\nModel Evaluation\n\n# Set model to evaluation mode\nmodel$eval()\n\n# Make predictions on test set\nwith_no_grad({\n  outputs &lt;- model(x_test)\n  \n  # Regression evaluation\n  reg_preds &lt;- outputs$regression$squeeze()\n  reg_test_loss &lt;- regression_loss_fn(reg_preds, y_reg_test)\n  \n  # Classification evaluation\n  cls_preds &lt;- outputs$classification$squeeze()\n  cls_probs &lt;- nnf_sigmoid(cls_preds)\n  cls_test_loss &lt;- classification_loss_fn(cls_preds, y_cls_test)\n  \n  # Convert predictions to binary (threshold = 0.5)\n  cls_pred_labels &lt;- (cls_probs &gt; 0.5)$to(torch_int())\n  \n  # Calculate accuracy\n  accuracy &lt;- (cls_pred_labels == y_cls_test$to(torch_int()))$sum()$item() / length(test_idx)\n})\n\n# Calculate additional metrics\nreg_preds_r &lt;- as.numeric(reg_preds)\ny_reg_test_r &lt;- as.numeric(y_reg_test)\ncls_probs_r &lt;- as.numeric(cls_probs)\ny_cls_test_r &lt;- as.numeric(y_cls_test)\n\n# Regression metrics\nrmse &lt;- sqrt(mean((reg_preds_r - y_reg_test_r)^2))\nmae &lt;- mean(abs(reg_preds_r - y_reg_test_r))\nr_squared &lt;- cor(reg_preds_r, y_reg_test_r)^2\n\n# Classification metrics\nauc &lt;- pROC::auc(pROC::roc(y_cls_test_r, cls_probs_r, quiet = TRUE))\n\n# Display results\nperformance_results &lt;- data.frame(\n  Task = c(\"Regression\", \"Regression\", \"Regression\", \"Classification\", \"Classification\", \"Classification\"),\n  Metric = c(\"Test Loss (MSE)\", \"RMSE\", \"R-squared\", \"Test Loss (BCE)\", \"Accuracy\", \"AUC\"),\n  Value = c(\n    round(reg_test_loss$item(), 4),\n    round(rmse, 4),\n    round(r_squared, 4),\n    round(cls_test_loss$item(), 4),\n    round(accuracy * 100, 2), \n    round(auc * 100, 2)\n  )\n)\n\nprint(performance_results)\n\n            Task          Metric   Value\n1     Regression Test Loss (MSE)  0.0569\n2     Regression            RMSE  0.2386\n3     Regression       R-squared  0.9436\n4 Classification Test Loss (BCE)  0.0707\n5 Classification        Accuracy 98.6700\n6 Classification             AUC 99.9800\n\n\n\n\nVisualization and Overfitting Analysis\n\n# Plot enhanced training history with overfitting detection\np1 &lt;- training_history %&gt;%\n  select(epoch, train_total_loss, val_total_loss) %&gt;%\n  pivot_longer(cols = c(train_total_loss, val_total_loss), \n               names_to = \"split\", values_to = \"loss\") %&gt;%\n  mutate(split = case_when(\n    split == \"train_total_loss\" ~ \"Training\",\n    split == \"val_total_loss\" ~ \"Validation\"\n  )) %&gt;%\n  ggplot(aes(x = epoch, y = loss, color = split)) +\n  geom_line(size = 1) +\n  geom_vline(xintercept = which.min(training_history$val_total_loss), \n             linetype = \"dashed\", color = \"red\", alpha = 0.7) +\n  labs(title = \"Training vs Validation Loss\",\n       subtitle = \"Red line shows optimal stopping point\",\n       x = \"Epoch\", y = \"Total Loss\", color = \"Dataset\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n# Separate task losses\np2 &lt;- training_history %&gt;%\n  select(epoch, train_reg_loss, val_reg_loss, train_cls_loss, val_cls_loss) %&gt;%\n  pivot_longer(cols = -epoch, names_to = \"metric\", values_to = \"loss\") %&gt;%\n  separate(metric, into = c(\"split\", \"task\", \"loss_type\"), sep = \"_\") %&gt;%\n  mutate(\n    split = ifelse(split == \"train\", \"Training\", \"Validation\"),\n    task = ifelse(task == \"reg\", \"Regression\", \"Classification\"),\n    metric_name = paste(split, task)\n  ) %&gt;%\n  ggplot(aes(x = epoch, y = loss, color = metric_name)) +\n  geom_line(size = 1) +\n  facet_wrap(~task, scales = \"free_y\") +\n  labs(title = \"Task-Specific Loss Curves\",\n       subtitle = \"Monitoring overfitting in individual tasks\",\n       x = \"Epoch\", y = \"Loss\", color = \"Split & Task\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set2\")\n\n# Validation accuracy progression\np3 &lt;- ggplot(training_history, aes(x = epoch, y = val_accuracy)) +\n  geom_line(color = \"#2c3e50\", size = 1) +\n  geom_hline(yintercept = max(training_history$val_accuracy), \n             linetype = \"dashed\", color = \"red\", alpha = 0.7) +\n  labs(title = \"Validation Accuracy Progression\",\n       subtitle = paste(\"Peak accuracy:\", round(max(training_history$val_accuracy), 3)),\n       x = \"Epoch\", y = \"Validation Accuracy\") +\n  theme_minimal()\n\n# Overfitting analysis\ntraining_history$overfitting_gap &lt;- training_history$train_total_loss - training_history$val_total_loss\n\np4 &lt;- ggplot(training_history, aes(x = epoch, y = overfitting_gap)) +\n  geom_line(color = \"#e74c3c\", size = 1) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  labs(title = \"Overfitting Gap Analysis\",\n       subtitle = \"Difference between training and validation loss\",\n       x = \"Epoch\", y = \"Training Loss - Validation Loss\") +\n  theme_minimal()\n\n# Regression predictions vs actual values\nregression_results &lt;- data.frame(\n  Actual = y_reg_test_r,\n  Predicted = reg_preds_r\n)\n\np5 &lt;- ggplot(regression_results, aes(x = Actual, y = Predicted)) +\n  geom_point(alpha = 0.6, color = \"#2c3e50\") +\n  geom_abline(slope = 1, intercept = 0, color = \"#e74c3c\", linetype = \"dashed\", size = 1) +\n  geom_smooth(method = \"lm\", color = \"#3498db\", se = TRUE) +\n  labs(title = \"Regression Task: Actual vs Predicted Values\",\n       subtitle = paste(\"R² =\", round(r_squared, 3), \", RMSE =\", round(rmse, 3)),\n       x = \"Actual Values\", y = \"Predicted Values\") +\n  theme_minimal()\n\n# Classification probability distribution\ncls_results &lt;- data.frame(\n  Probability = cls_probs_r,\n  Actual_Class = factor(y_cls_test_r, labels = c(\"Class 0\", \"Class 1\"))\n)\n\np6 &lt;- ggplot(cls_results, aes(x = Probability, fill = Actual_Class)) +\n  geom_histogram(alpha = 0.7, bins = 20, position = \"identity\") +\n  geom_vline(xintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Classification Task: Predicted Probabilities\",\n       subtitle = paste(\"Accuracy =\", round(accuracy * 100, 1), \"%\"),\n       x = \"Predicted Probability\", y = \"Count\", fill = \"Actual Class\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\")\n\n# Combine plots\nlibrary(patchwork)\n(p1 | p3) / (p2) / (p4) / (p5 | p6)"
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#detecting-available-cpu-cores",
    "href": "get-started/parallel-computing-in-r.html#detecting-available-cpu-cores",
    "title": "Parallel Computing in R",
    "section": "Detecting Available CPU Cores",
    "text": "Detecting Available CPU Cores\nThe first step involves checking how many CPU cores are available on the system:\n\n# Detect the number of CPU cores\ndetectCores()\n\n[1] 16\n\n\nGood practice dictates leaving one core free for the operating system, so typically detectCores() - 1 is used for parallel operations."
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#fundamental-parallel-computing-the-basics",
    "href": "get-started/parallel-computing-in-r.html#fundamental-parallel-computing-the-basics",
    "title": "Parallel Computing in R",
    "section": "Fundamental Parallel Computing: The Basics",
    "text": "Fundamental Parallel Computing: The Basics\nThis section demonstrates creating a simple function that takes time to execute, then compares sequential versus parallel execution times:\n\n# A function that takes time to execute\nslow_function &lt;- function(x) {\n  Sys.sleep(0.5)  # Simulate computation time (half a second)\n  return(x^2)     # Return the square of x\n}\n\n# Create a list of numbers to process\nnumbers &lt;- 1:10\n\n\nMethod 1: Using parLapply (Works on All Systems)\nThis method works on all operating systems including Windows:\n\n# Step 1: Create a cluster of workers\ncl &lt;- makeCluster(detectCores() - 1)\n\n# Step 2: Export any functions our workers need\nclusterExport(cl, \"slow_function\")\n\n# Run the sequential version and time it\ntic(\"Sequential version\")\nresult_sequential &lt;- lapply(numbers, slow_function)\ntoc()\n\nSequential version: 5.09 sec elapsed\n\n# Run the parallel version and time it\ntic(\"Parallel version\")\nresult_parallel &lt;- parLapply(cl, numbers, slow_function)\ntoc()\n\nParallel version: 0.52 sec elapsed\n\n# Step 3: Always stop the cluster when finished!\nstopCluster(cl)\n\n# Verify both methods give the same results\nall.equal(result_sequential, result_parallel)\n\n[1] TRUE\n\n\n\n\nMethod 2: Using mclapply (Unix/Mac Only)\nFor Mac or Linux systems, this simpler approach can be utilized:\n\n# For Mac/Linux users only\ntic(\"Parallel mclapply (Mac/Linux only)\")\nresult_parallel &lt;- mclapply(numbers, slow_function, mc.cores = detectCores() - 1)\ntoc()"
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#practical-application-guidelines",
    "href": "get-started/parallel-computing-in-r.html#practical-application-guidelines",
    "title": "Parallel Computing in R",
    "section": "Practical Application Guidelines",
    "text": "Practical Application Guidelines\nParallel computing isn’t always the optimal choice. Here are some considerations:\n✅ Good for parallelization: - Independent calculations (like applying the same function to different data chunks) - Computationally intensive tasks (simulations, bootstrap resampling) - Tasks that take more than a few seconds to run sequentially\n❌ Not good for parallelization: - Very quick operations (parallelization overhead may exceed the time saved) - Tasks with heavy dependencies between steps - I/O-bound operations (reading/writing files)"
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#implementation-best-practices",
    "href": "get-started/parallel-computing-in-r.html#implementation-best-practices",
    "title": "Parallel Computing in R",
    "section": "Implementation Best Practices",
    "text": "Implementation Best Practices\n\nAlways stop clusters with stopCluster(cl) when processing is complete\nLeave one core free for the operating system\nStart small and test with a subset of data\nMonitor memory usage - each worker needs its own copy of the data"
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#further-learning-opportunities",
    "href": "get-started/parallel-computing-in-r.html#further-learning-opportunities",
    "title": "Parallel Computing in R",
    "section": "Further Learning Opportunities",
    "text": "Further Learning Opportunities\nOnce practitioners are comfortable with these basics, they can explore:\n• The future package for more advanced parallel computing • The furrr package for parallel versions of purrr functions\n• Parallel computing with large datasets using data.table or dplyr • Distributed computing across multiple machines\n\nAdvanced Example: Bootstrap Sampling\nHere’s a more advanced example using bootstrap sampling, which is a perfect use case for parallel computing:\n\n# Generate sample data\nset.seed(123)\nsample_data &lt;- rnorm(1000, mean = 100, sd = 15)\n\n# Function to perform bootstrap sampling\nbootstrap_mean &lt;- function(data, n_bootstrap = 1000) {\n  bootstrap_samples &lt;- replicate(n_bootstrap, {\n    sample_indices &lt;- sample(length(data), replace = TRUE)\n    mean(data[sample_indices])\n  })\n  return(bootstrap_samples)\n}\n\n# Sequential bootstrap\ntic(\"Sequential bootstrap\")\nsequential_bootstrap &lt;- bootstrap_mean(sample_data, 5000)\ntoc()\n\nSequential bootstrap: 0.67 sec elapsed\n\n# Parallel bootstrap using foreach\ncl &lt;- makeCluster(detectCores() - 1)\nregisterDoParallel(cl)\n\ntic(\"Parallel bootstrap\")\nparallel_bootstrap &lt;- foreach(i = 1:5000, .combine = c) %dopar% {\n  sample_indices &lt;- sample(length(sample_data), replace = TRUE)\n  mean(sample_data[sample_indices])\n}\ntoc()\n\nParallel bootstrap: 2.17 sec elapsed\n\nstopCluster(cl)\n\n# Compare results\ncat(\"Sequential mean:\", mean(sequential_bootstrap), \"\\n\")\n\nSequential mean: 100.2462 \n\ncat(\"Parallel mean:\", mean(parallel_bootstrap), \"\\n\")\n\nParallel mean: 100.2482 \n\ncat(\"Original data mean:\", mean(sample_data), \"\\n\")\n\nOriginal data mean: 100.2419 \n\n\n\n\nMemory Usage Considerations\n\n# Check memory usage\ncat(\"Available memory info:\\n\")\n\nAvailable memory info:\n\ncat(\"Total RAM:\", round(as.numeric(system(\"wmic OS get TotalVisibleMemorySize /value\", intern = TRUE)[2]) / 1024^2, 1), \"GB\\n\")\n\nTotal RAM: NA GB\n\n# Guidelines for managing memory in parallel computing:\n# 1. Use smaller chunks of data\n# 2. Return only necessary results from workers\n# 3. Clear unnecessary objects before parallel processing\n# 4. Monitor memory usage with tools like 'top' or Task Manager\n\nThis tutorial provides a solid foundation for implementing parallel computing in R. Practitioners should always test parallel code with small datasets first and be mindful of the overhead that comes with setting up parallel workers!"
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#packages",
    "href": "get-started/parallel-computing-in-r.html#packages",
    "title": "Parallel Computing in R",
    "section": "",
    "text": "# Install packages if needed (uncomment to run)\n# install.packages(c(\"parallel\", \"foreach\", \"doParallel\", \"tictoc\"))\n\n# Load the essential packages\nlibrary(parallel)    # Base R parallel functions\nlibrary(foreach)     # For parallel loops\nlibrary(doParallel)  # Backend for foreach\nlibrary(tictoc)      # For timing comparisons"
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#the-basics",
    "href": "get-started/parallel-computing-in-r.html#the-basics",
    "title": "Parallel Computing in R",
    "section": "The Basics",
    "text": "The Basics\nThis section demonstrates creating a simple function that takes time to execute, then compares sequential versus parallel execution times:\n\n# A function that takes time to execute\nslow_function &lt;- function(x) {\n  Sys.sleep(0.5)  # Simulate computation time (half a second)\n  return(x^2)     # Return the square of x\n}\n\n# Create a list of numbers to process\nnumbers &lt;- 1:10\n\n\nMethod 1: Using parLapply (Works on All Systems)\nThis method works on all operating systems including Windows:\n\n# Step 1: Create a cluster of workers\ncl &lt;- makeCluster(detectCores() - 1)\n\n# Step 2: Export any functions our workers need\nclusterExport(cl, \"slow_function\")\n\n# Run the sequential version and time it\ntic(\"Sequential version\")\nresult_sequential &lt;- lapply(numbers, slow_function)\ntoc()\n\nSequential version: 5.05 sec elapsed\n\n# Run the parallel version and time it\ntic(\"Parallel version\")\nresult_parallel &lt;- parLapply(cl, numbers, slow_function)\ntoc()\n\nParallel version: 0.5 sec elapsed\n\n# Step 3: Always stop the cluster when finished!\nstopCluster(cl)\n\n# Verify both methods give the same results\nall.equal(result_sequential, result_parallel)\n\n[1] TRUE\n\n\n\n\nMethod 2: Using mclapply (Unix/Mac Only)\nFor Mac or Linux systems, this simpler approach can be utilized:\n\n# For Mac/Linux users only\ntic(\"Parallel mclapply (Mac/Linux only)\")\nresult_parallel &lt;- mclapply(numbers, slow_function, mc.cores = detectCores() - 1)\ntoc()"
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#matrix-operations",
    "href": "get-started/parallel-computing-in-r.html#matrix-operations",
    "title": "Parallel Computing in R",
    "section": "Matrix Operations",
    "text": "Matrix Operations\nLet’s try something more realistic. Matrix operations are perfect for parallelization:\n\n# A more computationally intensive function\nmatrix_function &lt;- function(n) {\n  # Create a random n×n matrix\n  m &lt;- matrix(rnorm(n*n), ncol = n)\n  # Calculate eigenvalues (computationally expensive)\n  eigen(m)\n  return(sum(diag(m)))\n}\n\n# Let's process 8 matrices of size 300×300\nmatrix_sizes &lt;- rep(300, 8)\n\n\nPerformance Comparison\nLet’s compare how different methods perform:\n\n# Sequential execution\ntic(\"Sequential\")\nsequential_result &lt;- lapply(matrix_sizes, matrix_function)\nsequential_time &lt;- toc(quiet = TRUE)\nsequential_time &lt;- sequential_time$toc - sequential_time$tic\n\n# Parallel with parLapply\ncl &lt;- makeCluster(detectCores() - 1)\nclusterExport(cl, \"matrix_function\")\ntic(\"parLapply\")\nparlapply_result &lt;- parLapply(cl, matrix_sizes, matrix_function)\nparlapply_time &lt;- toc(quiet = TRUE)\nparlapply_time &lt;- parlapply_time$toc - parlapply_time$tic\nstopCluster(cl)\n\n# Parallel with foreach\ncl &lt;- makeCluster(detectCores() - 1)\nregisterDoParallel(cl)\ntic(\"foreach\")\nforeach_result &lt;- foreach(s = matrix_sizes) %dopar% {\n  matrix_function(s)\n}\nforeach_time &lt;- toc(quiet = TRUE)\nforeach_time &lt;- foreach_time$toc - foreach_time$tic\nstopCluster(cl)\n\n# Create a results table\nresults &lt;- data.frame(\n  Method = c(\"Sequential\", \"parLapply\", \"foreach\"),\n  Time = c(sequential_time, parlapply_time, foreach_time),\n  Speedup = c(1, sequential_time/parlapply_time, sequential_time/foreach_time)\n)\n\n# Display the results\nresults\n\n      Method Time  Speedup\n1 Sequential 1.99 1.000000\n2  parLapply 0.22 9.045455\n3    foreach 0.27 7.370370\n\n\n\n\nVisualizing Results\n\n# Load ggplot2 for visualization\nlibrary(ggplot2)\n\n# Plot execution times\nggplot(results, aes(x = reorder(Method, -Time), y = Time, fill = Method)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Execution Time Comparison\",\n       x = \"Method\", y = \"Time (seconds)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n# Plot speedup\nggplot(results, aes(x = reorder(Method, Speedup), y = Speedup, fill = Method)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Speedup Comparison\",\n       x = \"Method\", y = \"Times faster than sequential\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#practical-implementation",
    "href": "get-started/parallel-computing-in-r.html#practical-implementation",
    "title": "Parallel Computing in R",
    "section": "Practical Implementation",
    "text": "Practical Implementation\nParallel computing isn’t always the optimal choice. Here are some considerations:\n✅ Good for parallelization: - Independent calculations (like applying the same function to different data chunks) - Computationally intensive tasks (simulations, bootstrap resampling) - Tasks that take more than a few seconds to run sequentially\n❌ Not good for parallelization: - Very quick operations (parallelization overhead may exceed the time saved) - Tasks with heavy dependencies between steps - I/O-bound operations (reading/writing files)"
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#best-practices",
    "href": "get-started/parallel-computing-in-r.html#best-practices",
    "title": "Parallel Computing in R",
    "section": "Best Practices",
    "text": "Best Practices\n\nAlways stop clusters with stopCluster(cl) when processing is complete\nLeave one core free for the operating system\nStart small and test with a subset of data\nMonitor memory usage - each worker needs its own copy of the data"
  }
]