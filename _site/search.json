[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "This page lists all the blog posts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Variability in Credit Score Predictions\n\n\n\n\n\n\nR\n\n\nCredit Risk Analytics\n\n\nBootstrapping\n\n\n\n\n\n\n\n\n\nNov 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Models in R with tidymodels\n\n\n\n\n\n\nR\n\n\nMachine Learning\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\nOct 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Bayesian Optimization to Tune XGBoost Models in R\n\n\n\n\n\n\nR\n\n\nAnalytics\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nSep 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Particle Swarm Optimizer from Scratch in R\n\n\n\n\n\n\nR\n\n\nOptimization\n\n\nVisualization\n\n\n\n\n\n\n\n\n\nJul 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCustom Charting Functions Using ggplot2\n\n\n\n\n\n\nR\n\n\nData Visualization\n\n\nggplot2\n\n\n\n\n\n\n\n\n\nMay 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating Correlated Random Numbers in R from Scratch\n\n\n\n\n\n\nR\n\n\nStatistics\n\n\nSimulation\n\n\n\n\n\n\n\n\n\nMar 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMeasuring Model Performance Using a Gains Table\n\n\n\n\n\n\nR\n\n\nCredit Risk Analytics\n\n\nModel Evaluation\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPortfolio Optimization Using Particle Swarm Optimization in R\n\n\n\n\n\n\nR\n\n\nFinance\n\n\nOptimization\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to R for Analytics\n\n\n\n\n\n\nR\n\n\nAnalytics\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\nMar 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMonotonic Binning Using XGBoost\n\n\n\n\n\n\nR\n\n\nCredit Risk Analytics\n\n\nXGBoost\n\n\n\n\n\n\n\n\n\nJan 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with Python using R and reticulate\n\n\n\n\n\n\nR\n\n\nPython\n\n\nreticulate\n\n\n\n\n\n\n\n\n\nJan 15, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html",
    "href": "posts/monotonic-binning-using-xgboost.html",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "Monotonic binning is a technique where variable values are grouped into bins such that event rates increase or decrease consistently across these bins. This approach is particularly valuable in credit risk modeling for two key reasons:\n\nModel Stability: Monotonic relationships add robustness to models, making them less susceptible to overfitting and more reliable when deployed in production\nInterpretability: Monotonic relationships are easier to explain to stakeholders and regulators, as they ensure consistent and logical relationships between variables and outcomes\n\n\n\nWe’ll use the following R packages for this demonstration:\n\nlibrary(recipes)  # For data preprocessing\nlibrary(dplyr)    # For data manipulation\nlibrary(xgboost)  # For creating monotonic bins\nlibrary(ggplot2)  # For visualization\n\n\n\n\nFor this demonstration, we’ll use a sample from the Lending Club dataset, which contains loan information including whether loans defaulted:\n\n# Load sample data from Lending Club dataset\nsample &lt;- read.csv(\"https://bit.ly/42ypcnJ\")\n\n# Check dimensions of the dataset\ndim(sample)\n\n[1] 10000   153\n\n\n\n\n\nFirst, we need to create a binary target variable that indicates whether a loan defaulted (1) or not (0):\n\n# Define loan statuses that represent defaults\ncodes &lt;- c(\"Charged Off\", \"Does not meet the credit policy. Status:Charged Off\")\n\n# Create binary target variable\nmodel_data &lt;- sample %&gt;%\n  mutate(bad_flag = ifelse(loan_status %in% codes, 1, 0))\n\n\n\n\nNext, we’ll preprocess the data using the recipes package to: 1. Select only numeric variables 2. Impute missing values with median values\n\n# Create a recipe for preprocessing\nrec &lt;- recipe(bad_flag ~ ., data = model_data) %&gt;%\n  step_select(where(is.numeric)) %&gt;%  # Keep only numeric variables\n  step_impute_median(all_predictors())  # Fill missing values with medians\n\n# Apply the preprocessing steps\nrec &lt;- prep(rec, training = model_data)\ntrain &lt;- bake(rec, new_data = model_data)\n\n\n\n\nBefore creating monotonic bins, it’s helpful to visualize the raw relationship between a predictor variable and the target. Let’s examine how the number of credit inquiries in the past 6 months relates to default rates:\n\n# Create dataframe with inquiries and default flag\ndata.frame(x = model_data$inq_last_6mths,\n           y = model_data$bad_flag) %&gt;%\n  filter(x &lt;= 5) %&gt;%  # Focus on 0-5 inquiries for clarity\n  group_by(x) %&gt;% \n  summarise(count = n(),  # Count observations in each group\n            events = sum(y)) %&gt;%  # Count defaults in each group\n  mutate(pct = events/count) %&gt;%  # Calculate default rate\n  ggplot(aes(x = factor(x), y = pct)) + \n  geom_col() + \n  theme_minimal() + \n  labs(x = \"# of inquiries in past 6 months\", \n       y = \"Default rate\",\n       title = \"Default rate vs number of inquiries\")\n\n\n\n\n\n\n\n\nNotice that while there’s a general upward trend (more inquiries correlate with higher default rates), the relationship isn’t perfectly monotonic. This is where our binning approach will help.\n\n\n\nNow we’ll leverage XGBoost’s monotonicity constraints to create bins that have a strictly increasing relationship with default rates. The key parameter is monotone_constraints = 1, which forces the model to create splits that maintain a positive relationship with the target:\n\n# Train XGBoost model with monotonicity constraint\nmdl &lt;- xgboost(\n  data = train %&gt;%\n    select(inq_last_6mths) %&gt;%  # Use only the inquiries variable\n    as.matrix(),  \n  label = train[[\"bad_flag\"]],  # Target variable\n  nrounds = 5,  # Number of boosting rounds\n  params = list(\n    booster = \"gbtree\",\n    objective = \"binary:logistic\",\n    monotone_constraints = 1,  # Force positive relationship\n    max_depth = 1  # Simple trees with single splits\n  ),\n  verbose = 0  # Suppress output\n)\n\n\n\n\nAfter training the model, we can extract the split points that XGBoost identified and use them to create our monotonic bins:\n\n# Extract split points from the model\nsplits &lt;- xgb.model.dt.tree(model = mdl)  \n\n# Create bin boundaries including -Inf and Inf for complete coverage\ncuts &lt;- c(-Inf, unique(sort(splits$Split)), Inf)\n\n# Create and visualize the monotonic bins\ndata.frame(target = train$bad_flag,\n           buckets = cut(train$inq_last_6mths, \n                         breaks = cuts, \n                         include.lowest = TRUE, \n                         right = TRUE)) %&gt;% \n  group_by(buckets) %&gt;%\n  summarise(total = n(),  # Count observations in each bin\n            events = sum(target == 1)) %&gt;%  # Count defaults in each bin\n  mutate(pct = events/total) %&gt;%  # Calculate default rate\n  ggplot(aes(x = buckets, y = pct)) + \n  geom_col() + \n  theme_minimal() + \n  labs(x = \"Bins\", \n       y = \"Default rate\",\n       title = \"Monotonic Bins for Inquiries\")\n\n\n\n\n\n\n\n\nNotice how the default rates now increase monotonically across the bins, making the relationship clearer and more interpretable compared to the raw data we visualized earlier.\n\n\n\nTo make this process more efficient for multiple variables, let’s create a reusable function that handles the entire binning workflow:\n\ncreate_bins &lt;- function(var, outcome, max_depth = 10, plot = TRUE){\n  # Determine relationship direction automatically\n  corr &lt;- cor(var, outcome, method = \"spearman\")\n  direction &lt;- ifelse(corr &gt; 0, 1, -1)  # 1 for positive, -1 for negative correlation\n  \n  # Build XGBoost model with appropriate monotonicity constraint\n  mdl &lt;- xgboost(\n    verbose = 0,\n    data = as.matrix(var),\n    label = outcome,\n    nrounds = 100,  # Single round is sufficient for binning\n    params = list(objective = \"binary:logistic\",\n                  monotone_constraints = direction,  # Apply constraint based on correlation\n                  max_depth = max_depth))  # Control tree complexity\n  \n  # Extract and return split points\n  splits &lt;- xgb.model.dt.tree(model = mdl)\n  cuts &lt;- c(-Inf, sort(unique(splits$Split)), Inf)  # Include boundaries for complete coverage\n  \n  # Optionally visualize the bins\n  if(plot) {\n    data.frame(target = outcome,\n               buckets = cut(var, \n                            breaks = cuts, \n                            include.lowest = TRUE, \n                            right = TRUE)) %&gt;% \n      group_by(buckets) %&gt;%\n      summarise(total = n(),\n                events = sum(target == 1)) %&gt;%\n      mutate(pct = events/total) %&gt;%\n      ggplot(aes(x = buckets, y = pct)) + \n      geom_col() + \n      theme_minimal() + \n      labs(x = \"Bins\", \n           y = \"Default rate\",\n           title = \"Monotonic Bins\")\n  }\n  \n  return(cuts)  # Return the bin boundaries\n}\n\n\n\n\nYou can use this function to create monotonic bins for any numeric variable by passing the variable and outcome columns:\n\n# Example: Create monotonic bins for annual income\nincome_bins &lt;- create_bins(\n  var = train$annual_inc,\n  outcome = train$bad_flag,\n  max_depth = 5\n)"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#required-libraries",
    "href": "posts/monotonic-binning-using-xgboost.html#required-libraries",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "We’ll use the following R packages for this demonstration:\n\nlibrary(recipes)  # For data preprocessing\nlibrary(dplyr)    # For data manipulation\nlibrary(xgboost)  # For creating monotonic bins\nlibrary(ggplot2)  # For visualization"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#sample-dataset",
    "href": "posts/monotonic-binning-using-xgboost.html#sample-dataset",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "For this demonstration, we’ll use a sample from the Lending Club dataset, which contains loan information including whether loans defaulted:\n\n# Load sample data from Lending Club dataset\nsample &lt;- read.csv(\"https://bit.ly/42ypcnJ\")\n\n# Check dimensions of the dataset\ndim(sample)\n\n[1] 10000   153"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#creating-a-target-variable",
    "href": "posts/monotonic-binning-using-xgboost.html#creating-a-target-variable",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "First, we need to create a binary target variable that indicates whether a loan defaulted (1) or not (0):\n\n# Define loan statuses that represent defaults\ncodes &lt;- c(\"Charged Off\", \"Does not meet the credit policy. Status:Charged Off\")\n\n# Create binary target variable\nmodel_data &lt;- sample %&gt;%\n  mutate(bad_flag = ifelse(loan_status %in% codes, 1, 0))"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#data-preparation",
    "href": "posts/monotonic-binning-using-xgboost.html#data-preparation",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "Next, we’ll preprocess the data using the recipes package to: 1. Select only numeric variables 2. Impute missing values with median values\n\n# Create a recipe for preprocessing\nrec &lt;- recipe(bad_flag ~ ., data = model_data) %&gt;%\n  step_select(where(is.numeric)) %&gt;%  # Keep only numeric variables\n  step_impute_median(all_predictors())  # Fill missing values with medians\n\n# Apply the preprocessing steps\nrec &lt;- prep(rec, training = model_data)\ntrain &lt;- bake(rec, new_data = model_data)"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#analyzing-directional-trends",
    "href": "posts/monotonic-binning-using-xgboost.html#analyzing-directional-trends",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "Before creating monotonic bins, it’s helpful to visualize the raw relationship between a predictor variable and the target. Let’s examine how the number of credit inquiries in the past 6 months relates to default rates:\n\n# Create dataframe with inquiries and default flag\ndata.frame(x = model_data$inq_last_6mths,\n           y = model_data$bad_flag) %&gt;%\n  filter(x &lt;= 5) %&gt;%  # Focus on 0-5 inquiries for clarity\n  group_by(x) %&gt;% \n  summarise(count = n(),  # Count observations in each group\n            events = sum(y)) %&gt;%  # Count defaults in each group\n  mutate(pct = events/count) %&gt;%  # Calculate default rate\n  ggplot(aes(x = factor(x), y = pct)) + \n  geom_col() + \n  theme_minimal() + \n  labs(x = \"# of inquiries in past 6 months\", \n       y = \"Default rate\",\n       title = \"Default rate vs number of inquiries\")\n\n\n\n\n\n\n\n\nNotice that while there’s a general upward trend (more inquiries correlate with higher default rates), the relationship isn’t perfectly monotonic. This is where our binning approach will help."
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#creating-monotonic-bins-with-xgboost",
    "href": "posts/monotonic-binning-using-xgboost.html#creating-monotonic-bins-with-xgboost",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "Now we’ll leverage XGBoost’s monotonicity constraints to create bins that have a strictly increasing relationship with default rates. The key parameter is monotone_constraints = 1, which forces the model to create splits that maintain a positive relationship with the target:\n\n# Train XGBoost model with monotonicity constraint\nmdl &lt;- xgboost(\n  data = train %&gt;%\n    select(inq_last_6mths) %&gt;%  # Use only the inquiries variable\n    as.matrix(),  \n  label = train[[\"bad_flag\"]],  # Target variable\n  nrounds = 5,  # Number of boosting rounds\n  params = list(\n    booster = \"gbtree\",\n    objective = \"binary:logistic\",\n    monotone_constraints = 1,  # Force positive relationship\n    max_depth = 1  # Simple trees with single splits\n  ),\n  verbose = 0  # Suppress output\n)"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#retrieving-split-points-and-creating-bins",
    "href": "posts/monotonic-binning-using-xgboost.html#retrieving-split-points-and-creating-bins",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "After training the model, we can extract the split points that XGBoost identified and use them to create our monotonic bins:\n\n# Extract split points from the model\nsplits &lt;- xgb.model.dt.tree(model = mdl)  \n\n# Create bin boundaries including -Inf and Inf for complete coverage\ncuts &lt;- c(-Inf, unique(sort(splits$Split)), Inf)\n\n# Create and visualize the monotonic bins\ndata.frame(target = train$bad_flag,\n           buckets = cut(train$inq_last_6mths, \n                         breaks = cuts, \n                         include.lowest = TRUE, \n                         right = TRUE)) %&gt;% \n  group_by(buckets) %&gt;%\n  summarise(total = n(),  # Count observations in each bin\n            events = sum(target == 1)) %&gt;%  # Count defaults in each bin\n  mutate(pct = events/total) %&gt;%  # Calculate default rate\n  ggplot(aes(x = buckets, y = pct)) + \n  geom_col() + \n  theme_minimal() + \n  labs(x = \"Bins\", \n       y = \"Default rate\",\n       title = \"Monotonic Bins for Inquiries\")\n\n\n\n\n\n\n\n\nNotice how the default rates now increase monotonically across the bins, making the relationship clearer and more interpretable compared to the raw data we visualized earlier."
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#creating-a-reusable-function",
    "href": "posts/monotonic-binning-using-xgboost.html#creating-a-reusable-function",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "To make this process more efficient for multiple variables, let’s create a reusable function that handles the entire binning workflow:\n\ncreate_bins &lt;- function(var, outcome, max_depth = 10, plot = TRUE){\n  # Determine relationship direction automatically\n  corr &lt;- cor(var, outcome, method = \"spearman\")\n  direction &lt;- ifelse(corr &gt; 0, 1, -1)  # 1 for positive, -1 for negative correlation\n  \n  # Build XGBoost model with appropriate monotonicity constraint\n  mdl &lt;- xgboost(\n    verbose = 0,\n    data = as.matrix(var),\n    label = outcome,\n    nrounds = 100,  # Single round is sufficient for binning\n    params = list(objective = \"binary:logistic\",\n                  monotone_constraints = direction,  # Apply constraint based on correlation\n                  max_depth = max_depth))  # Control tree complexity\n  \n  # Extract and return split points\n  splits &lt;- xgb.model.dt.tree(model = mdl)\n  cuts &lt;- c(-Inf, sort(unique(splits$Split)), Inf)  # Include boundaries for complete coverage\n  \n  # Optionally visualize the bins\n  if(plot) {\n    data.frame(target = outcome,\n               buckets = cut(var, \n                            breaks = cuts, \n                            include.lowest = TRUE, \n                            right = TRUE)) %&gt;% \n      group_by(buckets) %&gt;%\n      summarise(total = n(),\n                events = sum(target == 1)) %&gt;%\n      mutate(pct = events/total) %&gt;%\n      ggplot(aes(x = buckets, y = pct)) + \n      geom_col() + \n      theme_minimal() + \n      labs(x = \"Bins\", \n           y = \"Default rate\",\n           title = \"Monotonic Bins\")\n  }\n  \n  return(cuts)  # Return the bin boundaries\n}"
  },
  {
    "objectID": "posts/monotonic-binning-using-xgboost.html#example-usage",
    "href": "posts/monotonic-binning-using-xgboost.html#example-usage",
    "title": "Monotonic Binning Using XGBoost",
    "section": "",
    "text": "You can use this function to create monotonic bins for any numeric variable by passing the variable and outcome columns:\n\n# Example: Create monotonic bins for annual income\nincome_bins &lt;- create_bins(\n  var = train$annual_inc,\n  outcome = train$bad_flag,\n  max_depth = 5\n)"
  },
  {
    "objectID": "posts/intro-to-r-analytics.html",
    "href": "posts/intro-to-r-analytics.html",
    "title": "Introduction to R for Analytics",
    "section": "",
    "text": "R is a powerful language specifically designed for data analysis and visualization. This guide will walk you through practical examples of using R for real-world analytics tasks."
  },
  {
    "objectID": "posts/intro-to-r-analytics.html#exploring-your-first-dataset",
    "href": "posts/intro-to-r-analytics.html#exploring-your-first-dataset",
    "title": "Introduction to R for Analytics",
    "section": "Exploring Your First Dataset",
    "text": "Exploring Your First Dataset\nR comes with several built-in datasets perfect for practice. Let’s start by examining the mtcars dataset:\n\n# View the first few rows\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n# Quick summary of the dataset structure\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n# Statistical summary of key variables\nsummary(mtcars[, c(\"mpg\", \"wt\", \"hp\")])\n\n      mpg              wt              hp       \n Min.   :10.40   Min.   :1.513   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:2.581   1st Qu.: 96.5  \n Median :19.20   Median :3.325   Median :123.0  \n Mean   :20.09   Mean   :3.217   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:3.610   3rd Qu.:180.0  \n Max.   :33.90   Max.   :5.424   Max.   :335.0  \n\n\nThe mtcars dataset contains information about 32 cars from Motor Trend magazine, including fuel efficiency (mpg), weight (wt), and horsepower (hp)."
  },
  {
    "objectID": "posts/intro-to-r-analytics.html#effective-data-visualization",
    "href": "posts/intro-to-r-analytics.html#effective-data-visualization",
    "title": "Introduction to R for Analytics",
    "section": "Effective Data Visualization",
    "text": "Effective Data Visualization\nVisualization is essential for understanding patterns in your data. Let’s create some informative plots:\n\nlibrary(ggplot2)\n\n# 1. A scatter plot with regression line\np1 &lt;- ggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(aes(size = hp, color = factor(cyl)), alpha = 0.7) +\n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"#2c3e50\") +\n  labs(title = \"Car Weight vs. Fuel Efficiency\",\n       subtitle = \"Size represents horsepower, color represents cylinders\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\", name = \"Cylinders\")\n\n# 2. Distribution of fuel efficiency\np2 &lt;- ggplot(mtcars, aes(x = mpg, fill = factor(cyl))) +\n  geom_histogram(bins = 10, alpha = 0.7, position = \"identity\") +\n  labs(title = \"Distribution of Fuel Efficiency\",\n       x = \"Miles Per Gallon\",\n       y = \"Count\") +\n  scale_fill_brewer(palette = \"Set1\", name = \"Cylinders\") +\n  theme_minimal()\n\n# Display plots (if using patchwork)\nlibrary(patchwork)\np1 / p2\n\n\n\n\n\n\n\n\nThese visualizations reveal:\n\nA clear negative correlation between car weight and fuel efficiency\nHigher cylinder cars tend to be heavier with lower MPG\nThe MPG distribution varies significantly by cylinder count"
  },
  {
    "objectID": "posts/intro-to-r-analytics.html#practical-data-transformation",
    "href": "posts/intro-to-r-analytics.html#practical-data-transformation",
    "title": "Introduction to R for Analytics",
    "section": "Practical Data Transformation",
    "text": "Practical Data Transformation\nData rarely comes in the exact format you need. The dplyr package makes transformations straightforward:\n\n# Load required packages\nlibrary(dplyr)\nlibrary(tibble)  # For rownames_to_column function\n\n# Create an enhanced version of the dataset\nmtcars_enhanced &lt;- mtcars %&gt;%\n  # Add car names as a column (they're currently row names)\n  rownames_to_column(\"car_name\") %&gt;%\n  # Create useful derived metrics\n  mutate(\n    # Efficiency ratio (higher is better)\n    efficiency_ratio = mpg / wt,\n    \n    # Power-to-weight ratio (higher is better)\n    power_to_weight = hp / wt,\n    \n    # Categorize cars by efficiency\n    efficiency_category = case_when(\n      mpg &gt; 25 ~ \"High Efficiency\",\n      mpg &gt; 15 ~ \"Medium Efficiency\",\n      TRUE ~ \"Low Efficiency\"\n    )\n  ) %&gt;%\n  # Arrange from most to least efficient\n  arrange(desc(efficiency_ratio))\n\n# Display the top 5 most efficient cars\nhead(mtcars_enhanced[, c(\"car_name\", \"mpg\", \"wt\", \"hp\", \"efficiency_ratio\", \"efficiency_category\")], 5)\n\n        car_name  mpg    wt  hp efficiency_ratio efficiency_category\n1   Lotus Europa 30.4 1.513 113         20.09253     High Efficiency\n2    Honda Civic 30.4 1.615  52         18.82353     High Efficiency\n3 Toyota Corolla 33.9 1.835  65         18.47411     High Efficiency\n4       Fiat 128 32.4 2.200  66         14.72727     High Efficiency\n5      Fiat X1-9 27.3 1.935  66         14.10853     High Efficiency"
  },
  {
    "objectID": "posts/intro-to-r-analytics.html#answering-business-questions-with-data",
    "href": "posts/intro-to-r-analytics.html#answering-business-questions-with-data",
    "title": "Introduction to R for Analytics",
    "section": "Answering Business Questions with Data",
    "text": "Answering Business Questions with Data\nLet’s use our enhanced dataset to answer some practical questions:\n\n# Question 1: What are the average characteristics by cylinder count?\ncylinder_analysis &lt;- mtcars_enhanced %&gt;%\n  group_by(cyl) %&gt;%\n  summarize(\n    count = n(),\n    avg_mpg = mean(mpg),\n    avg_weight = mean(wt),\n    avg_horsepower = mean(hp),\n    avg_efficiency_ratio = mean(efficiency_ratio),\n    avg_power_to_weight = mean(power_to_weight)\n  ) %&gt;%\n  arrange(cyl)\n\n# Display the results\ncylinder_analysis\n\n# A tibble: 3 × 7\n    cyl count avg_mpg avg_weight avg_horsepower avg_efficiency_ratio\n  &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;                &lt;dbl&gt;\n1     4    11    26.7       2.29           82.6                12.7 \n2     6     7    19.7       3.12          122.                  6.44\n3     8    14    15.1       4.00          209.                  3.95\n# ℹ 1 more variable: avg_power_to_weight &lt;dbl&gt;\n\n# Question 2: Which transmission type is more fuel efficient?\ntransmission_efficiency &lt;- mtcars_enhanced %&gt;%\n  # am: 0 = automatic, 1 = manual\n  mutate(transmission = if_else(am == 1, \"Manual\", \"Automatic\")) %&gt;%\n  group_by(transmission) %&gt;%\n  summarize(\n    count = n(),\n    avg_mpg = mean(mpg),\n    median_mpg = median(mpg),\n    mpg_std_dev = sd(mpg)\n  )\n\n# Display the results\ntransmission_efficiency\n\n# A tibble: 2 × 5\n  transmission count avg_mpg median_mpg mpg_std_dev\n  &lt;chr&gt;        &lt;int&gt;   &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1 Automatic       19    17.1       17.3        3.83\n2 Manual          13    24.4       22.8        6.17\n\n# Visualize the difference\nggplot(mtcars, aes(x = factor(am, labels = c(\"Automatic\", \"Manual\")), y = mpg, fill = factor(am))) +\n  geom_boxplot(alpha = 0.7) +\n  geom_jitter(width = 0.1, alpha = 0.5) +\n  labs(title = \"Fuel Efficiency by Transmission Type\",\n       x = \"Transmission Type\",\n       y = \"Miles Per Gallon\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/intro-to-r-analytics.html#correlation-analysis-for-decision-making",
    "href": "posts/intro-to-r-analytics.html#correlation-analysis-for-decision-making",
    "title": "Introduction to R for Analytics",
    "section": "Correlation Analysis for Decision Making",
    "text": "Correlation Analysis for Decision Making\nUnderstanding relationships between variables is crucial for business decisions:\n\n# Calculate correlations\ncor_matrix &lt;- cor(mtcars[, c(\"mpg\", \"wt\", \"hp\", \"disp\", \"qsec\")])\ncor_df &lt;- round(cor_matrix, 2)\n\n# Display correlation matrix\ncor_df\n\n       mpg    wt    hp  disp  qsec\nmpg   1.00 -0.87 -0.78 -0.85  0.42\nwt   -0.87  1.00  0.66  0.89 -0.17\nhp   -0.78  0.66  1.00  0.79 -0.71\ndisp -0.85  0.89  0.79  1.00 -0.43\nqsec  0.42 -0.17 -0.71 -0.43  1.00\n\n# Visualize correlations (requires the corrplot package)\nlibrary(corrplot)\ncorrplot(cor_matrix, method = \"circle\", type = \"upper\", \n         tl.col = \"black\", tl.srt = 45, addCoef.col = \"black\")\n\n\n\n\n\n\n\n# Scatter plot matrix of key variables\npairs(mtcars[, c(\"mpg\", \"wt\", \"hp\", \"disp\")], \n      main = \"Scatter Plot Matrix of Key Variables\",\n      pch = 21, bg = \"lightblue\", cex = 1.2)"
  },
  {
    "objectID": "posts/intro-to-r-analytics.html#working-with-real-world-datasets",
    "href": "posts/intro-to-r-analytics.html#working-with-real-world-datasets",
    "title": "Introduction to R for Analytics",
    "section": "Working with Real-World Datasets",
    "text": "Working with Real-World Datasets\nLet’s analyze the famous Iris dataset to demonstrate a complete workflow:\n\n# Load packages\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Examine the dataset\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n# Calculate summary statistics by species\niris_stats &lt;- iris %&gt;%\n  group_by(Species) %&gt;%\n  summarize(across(where(is.numeric), \n                   list(mean = mean, \n                        median = median,\n                        sd = sd,\n                        min = min,\n                        max = max)))\n\n# View summary for Sepal.Length\niris_stats %&gt;% select(Species, starts_with(\"Sepal.Length\"))\n\n# A tibble: 3 × 6\n  Species Sepal.Length_mean Sepal.Length_median Sepal.Length_sd Sepal.Length_min\n  &lt;fct&gt;               &lt;dbl&gt;               &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n1 setosa               5.01                 5             0.352              4.3\n2 versic…              5.94                 5.9           0.516              4.9\n3 virgin…              6.59                 6.5           0.636              4.9\n# ℹ 1 more variable: Sepal.Length_max &lt;dbl&gt;\n\n# Create a visualization comparing all measurements across species\niris_long &lt;- iris %&gt;%\n  pivot_longer(\n    cols = -Species,\n    names_to = \"Measurement\",\n    values_to = \"Value\"\n  )\n\n# Box plots with data points\nggplot(iris_long, aes(x = Species, y = Value, fill = Species)) +\n  geom_boxplot(alpha = 0.6) +\n  geom_jitter(width = 0.15, alpha = 0.5, color = \"darkgrey\") +\n  facet_wrap(~Measurement, scales = \"free_y\") +\n  labs(title = \"Iris Measurements Across Species\",\n       subtitle = \"Box plots with individual observations\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n# Find the most distinguishing features between species\niris_wide &lt;- iris %&gt;%\n  pivot_longer(cols = -Species, names_to = \"Measurement\", values_to = \"Value\") %&gt;%\n  group_by(Measurement, Species) %&gt;%\n  summarise(mean_value = mean(Value), .groups = \"drop\") %&gt;%\n  pivot_wider(names_from = Species, values_from = mean_value) %&gt;%\n  mutate(versicolor_vs_setosa = abs(versicolor - setosa),\n         virginica_vs_setosa = abs(virginica - setosa),\n         virginica_vs_versicolor = abs(virginica - versicolor),\n         max_difference = pmax(versicolor_vs_setosa, virginica_vs_setosa, virginica_vs_versicolor))\n\n# Display the results ordered by maximum difference\niris_wide %&gt;% arrange(desc(max_difference))\n\n# A tibble: 4 × 8\n  Measurement  setosa versicolor virginica versicolor_vs_setosa\n  &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;                &lt;dbl&gt;\n1 Petal.Length  1.46        4.26      5.55                2.80 \n2 Petal.Width   0.246       1.33      2.03                1.08 \n3 Sepal.Length  5.01        5.94      6.59                0.93 \n4 Sepal.Width   3.43        2.77      2.97                0.658\n# ℹ 3 more variables: virginica_vs_setosa &lt;dbl&gt;, virginica_vs_versicolor &lt;dbl&gt;,\n#   max_difference &lt;dbl&gt;"
  },
  {
    "objectID": "posts/intro-to-r-analytics.html#handling-missing-data-in-practice",
    "href": "posts/intro-to-r-analytics.html#handling-missing-data-in-practice",
    "title": "Introduction to R for Analytics",
    "section": "Handling Missing Data in Practice",
    "text": "Handling Missing Data in Practice\nLet’s tackle the common challenge of missing data using a practical example:\n\n# Create a simulated customer dataset with missing values\nset.seed(123) # For reproducibility\n\ncustomers &lt;- data.frame(\n  customer_id = 1:100,\n  age = sample(18:70, 100, replace = TRUE),\n  income = round(rnorm(100, 50000, 15000)),\n  years_as_customer = sample(0:20, 100, replace = TRUE),\n  purchase_frequency = sample(1:10, 100, replace = TRUE)\n)\n\n# Introduce missing values randomly\nset.seed(456)\ncustomers$age[sample(1:100, 10)] &lt;- NA\ncustomers$income[sample(1:100, 15)] &lt;- NA\ncustomers$purchase_frequency[sample(1:100, 5)] &lt;- NA\n\n# 1. Identify missing data\nmissing_summary &lt;- sapply(customers, function(x) sum(is.na(x)))\nmissing_summary\n\n       customer_id                age             income  years_as_customer \n                 0                 10                 15                  0 \npurchase_frequency \n                 5 \n\n# 2. Visualize the pattern of missing data\nlibrary(naniar) # May need to install this package\nvis_miss(customers)\n\n\n\n\n\n\n\n# 3. Handle missing data with multiple approaches\n\n# Option A: Remove rows with any missing values\nclean_customers &lt;- na.omit(customers)\nnrow(customers) - nrow(clean_customers) # Number of rows removed\n\n[1] 26\n\n# Option B: Impute with mean/median (numeric variables only)\nimputed_customers &lt;- customers %&gt;%\n  mutate(\n    age = ifelse(is.na(age), median(age, na.rm = TRUE), age),\n    income = ifelse(is.na(income), mean(income, na.rm = TRUE), income),\n    purchase_frequency = ifelse(is.na(purchase_frequency), \n                               median(purchase_frequency, na.rm = TRUE), \n                               purchase_frequency)\n  )\n\n# Option C: Predictive imputation (using age to predict income)\nlibrary(mice) # For more sophisticated imputation\n# Quick imputation model - in practice you'd use more parameters\nimputed_data &lt;- mice(customers, m = 5, method = \"pmm\", printFlag = FALSE)\ncustomers_complete &lt;- complete(imputed_data)\n\n# Compare results by calculating customer value score\ncalculate_value &lt;- function(df) {\n  df %&gt;%\n    mutate(customer_value = (income/10000) * (purchase_frequency/10) * log(years_as_customer + 1)) %&gt;%\n    arrange(desc(customer_value)) %&gt;%\n    select(customer_id, customer_value, everything())\n}\n\n# Top 5 customers by value (original with NAs removed)\nhead(calculate_value(clean_customers), 5)\n\n  customer_id customer_value age income years_as_customer purchase_frequency\n1           7       24.63960  67  82249                19                 10\n2          54       15.73965  22  70961                15                  8\n3          59       15.67045  50  70649                15                  8\n4          84       15.09251  21  55732                14                 10\n5          72       14.27848  23  61853                12                  9\n\n# Top 5 customers by value (with imputed values)\nhead(calculate_value(customers_complete), 5)\n\n  customer_id customer_value age income years_as_customer purchase_frequency\n1           7       24.63960  67  82249                19                 10\n2          54       15.73965  22  70961                15                  8\n3          59       15.67045  50  70649                15                  8\n4          84       15.09251  21  55732                14                 10\n5          72       14.27848  23  61853                12                  9"
  },
  {
    "objectID": "posts/intro-to-r-analytics.html#time-series-analysis-for-business-trends",
    "href": "posts/intro-to-r-analytics.html#time-series-analysis-for-business-trends",
    "title": "Introduction to R for Analytics",
    "section": "Time Series Analysis for Business Trends",
    "text": "Time Series Analysis for Business Trends\nTime series analysis is essential for understanding business trends and forecasting:\n\n# Load packages\nlibrary(forecast)\nlibrary(tseries)\n\n# Examine the built-in AirPassengers dataset (monthly air passengers from 1949 to 1960)\ndata(AirPassengers)\nclass(AirPassengers)\n\n[1] \"ts\"\n\n# Plot the time series\nautoplots &lt;- autoplot(AirPassengers) +\n  labs(title = \"Monthly Air Passengers (1949-1960)\",\n       y = \"Passenger Count\",\n       x = \"Year\") +\n  theme_minimal()\n\n# Decompose the time series into seasonal components\ndecomposed &lt;- decompose(AirPassengers, \"multiplicative\")\nautoplot(decomposed) +\n  labs(title = \"Decomposition of Air Passengers Time Series\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Forecasting future values using auto.arima\nfit &lt;- auto.arima(AirPassengers)\nforecasts &lt;- forecast(fit, h = 24) # Forecast 2 years ahead\n\n# Plot the forecasts\nplot(forecasts, \n     main = \"Air Passengers Forecast (24 months)\",\n     xlab = \"Year\", \n     ylab = \"Passenger Count\")\n\n\n\n\n\n\n\n# Summary of the forecast model\nsummary(fit)\n\nSeries: AirPassengers \nARIMA(2,1,1)(0,1,0)[12] \n\nCoefficients:\n         ar1     ar2      ma1\n      0.5960  0.2143  -0.9819\ns.e.  0.0888  0.0880   0.0292\n\nsigma^2 = 132.3:  log likelihood = -504.92\nAIC=1017.85   AICc=1018.17   BIC=1029.35\n\nTraining set error measures:\n                 ME     RMSE     MAE      MPE     MAPE     MASE        ACF1\nTraining set 1.3423 10.84619 7.86754 0.420698 2.800458 0.245628 -0.00124847"
  },
  {
    "objectID": "posts/generating-correlated-random-numbers.html",
    "href": "posts/generating-correlated-random-numbers.html",
    "title": "Generating Correlated Random Numbers in R from Scratch",
    "section": "",
    "text": "Need random data with specific correlation patterns for your simulations? This post shows you how to generate correlated random numbers in R using a simple matrix approach – perfect for testing algorithms or creating realistic synthetic datasets."
  },
  {
    "objectID": "posts/generating-correlated-random-numbers.html#the-cholesky-method-in-four-steps",
    "href": "posts/generating-correlated-random-numbers.html#the-cholesky-method-in-four-steps",
    "title": "Generating Correlated Random Numbers in R from Scratch",
    "section": "The Cholesky Method in Four Steps",
    "text": "The Cholesky Method in Four Steps\n\n# 1. Define your target correlation matrix\ncor_mat &lt;- matrix(c(1, 0.3, \n                   0.3, 1), nrow = 2, byrow = TRUE)\n\n# 2. Apply Cholesky decomposition\nchol_mat &lt;- chol(cor_mat)\n\n# 3. Generate uncorrelated random numbers\nold_random &lt;- matrix(rnorm(2000), ncol = 2)\n\n# 4. Transform to create correlation\nnew_random &lt;- old_random %*% chol_mat\n\n# Verify the correlation\ncor(new_random)\n\n          [,1]      [,2]\n[1,] 1.0000000 0.3206961\n[2,] 0.3206961 1.0000000\n\n\nThat’s it! The new_random matrix now contains values with approximately your target correlation structure. This technique uses Cholesky decomposition to create a transformation matrix that induces the desired correlation when applied to uncorrelated data."
  },
  {
    "objectID": "posts/generating-correlated-random-numbers.html#watch-out-for-these-pitfalls",
    "href": "posts/generating-correlated-random-numbers.html#watch-out-for-these-pitfalls",
    "title": "Generating Correlated Random Numbers in R from Scratch",
    "section": "Watch Out for These Pitfalls",
    "text": "Watch Out for These Pitfalls\n\n1. Start with Truly Random Data\nYour input data must be uncorrelated for this method to work correctly:\n\n# What happens with already correlated input?\nsimulate_correlation &lt;- function(input_correlation, target = 0.3) {\n  results &lt;- replicate(1000, {\n    # Create input with specified correlation\n    x &lt;- rnorm(1000)\n    y &lt;- input_correlation * x + rnorm(1000, sd = sqrt(1 - input_correlation^2))\n    \n    # Apply our method\n    old_random &lt;- cbind(x, y)\n    chol_mat &lt;- chol(matrix(c(1, target, target, 1), ncol = 2))\n    new_random &lt;- old_random %*% chol_mat\n    \n    # Return resulting correlation\n    cor(new_random)[1,2]\n  })\n  return(results)\n}\n\n# Compare results with different input correlations\npar(mfrow = c(1, 2))\nhist(simulate_correlation(0.8), main = \"Starting with Correlated Data\",\n     xlim = c(0, 1), col = \"salmon\")\nhist(simulate_correlation(0.001), main = \"Starting with Random Data\",\n     xlim = c(0, 1), col = \"lightblue\")\n\n\n\n\n\n\n\n\nWhen your input data already has correlation patterns, the Cholesky method can’t properly override them to create your target correlation.\n\n\n2. Use the Same Distribution for All Variables\n\n# Different distributions cause problems\nset.seed(123)\nx1 &lt;- rchisq(1000, df = 3)  # Chi-squared (skewed)\ny1 &lt;- rnorm(1000)           # Normal (symmetric)\nold_mixed &lt;- cbind(x1, y1)\n\n# Same distribution works better\nx2 &lt;- rchisq(1000, df = 3)\ny2 &lt;- rchisq(1000, df = 3)\nold_same &lt;- cbind(x2, y2)\n\n# Apply the same transformation to both\nchol_mat &lt;- chol(matrix(c(1, 0.7, 0.7, 1), ncol = 2))\nnew_mixed &lt;- old_mixed %*% chol_mat\nnew_same &lt;- old_same %*% chol_mat\n\n# Compare results\ncat(\"Target correlation: 0.7\\n\")\n\nTarget correlation: 0.7\n\ncat(\"Mixed distributions result:\", round(cor(new_mixed)[1,2], 3), \"\\n\")\n\nMixed distributions result: 0.915 \n\ncat(\"Same distribution result:\", round(cor(new_same)[1,2], 3))\n\nSame distribution result: 0.699\n\n\nMixing different distributions (like normal and chi-squared) can lead to unexpected correlation patterns after transformation.\n\n\n3. Distribution Properties Can Change\n\n# Original positive-only distribution\nx &lt;- rchisq(1000, df = 3)  # Always positive\ny &lt;- rchisq(1000, df = 3)  # Always positive\nold_random &lt;- cbind(x, y)\n\n# Apply negative correlation\nchol_mat &lt;- chol(matrix(c(1, -0.7, -0.7, 1), ncol = 2))\nnew_random &lt;- old_random %*% chol_mat\n\n# Check what happened\ncat(\"Original data range:\", round(range(old_random), 2), \"\\n\")\n\nOriginal data range: 0.02 19.93 \n\ncat(\"Transformed data range:\", round(range(new_random), 2), \"\\n\")\n\nTransformed data range: -12.81 19.93 \n\ncat(\"Negative values in result:\", sum(new_random &lt; 0), \"out of\", length(new_random))\n\nNegative values in result: 488 out of 2000\n\n\nThe Cholesky transformation can fundamentally change your data’s properties - like introducing negative values into a previously positive-only distribution."
  },
  {
    "objectID": "posts/generating-correlated-random-numbers.html#the-easy-way-using-mvtnorm",
    "href": "posts/generating-correlated-random-numbers.html#the-easy-way-using-mvtnorm",
    "title": "Generating Correlated Random Numbers in R from Scratch",
    "section": "The Easy Way: Using mvtnorm",
    "text": "The Easy Way: Using mvtnorm\nFor most real applications, the mvtnorm package offers a simpler solution:\n\n# Load the package\nlibrary(mvtnorm)\n\n# Define means and covariance matrix\nmeans &lt;- c(10, 20)  # Mean for each variable\nsigma &lt;- matrix(c(4, 2,   # Covariance matrix\n                  2, 3), ncol = 2)\n\n# See the implied correlation\ncov2cor(sigma)\n\n          [,1]      [,2]\n[1,] 1.0000000 0.5773503\n[2,] 0.5773503 1.0000000\n\n# Generate correlated normal data in one step\nx &lt;- rmvnorm(n = 1000, mean = means, sigma = sigma)\n\n# Verify the result\nround(cor(x), 3)\n\n      [,1]  [,2]\n[1,] 1.000 0.613\n[2,] 0.613 1.000"
  },
  {
    "objectID": "posts/generating-correlated-random-numbers.html#when-to-use-each-method",
    "href": "posts/generating-correlated-random-numbers.html#when-to-use-each-method",
    "title": "Generating Correlated Random Numbers in R from Scratch",
    "section": "When to Use Each Method",
    "text": "When to Use Each Method\nUse the Cholesky method when: - You need to understand the mathematical principles - You’re working with non-normal distributions - You need to create custom correlation structures\nUse mvtnorm when: - You need multivariate normal data quickly - You want precise control over means and variances - You’re working with many variables"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html",
    "href": "posts/building-particle-swarm-optimizer.html",
    "title": "Building a Particle Swarm Optimizer from Scratch in R",
    "section": "",
    "text": "Nature-inspired algorithms can solve complex optimization problems with surprising efficiency. This post shows you how to build a Particle Swarm Optimizer (PSO) from scratch in R – mimicking how birds flock or fish school to efficiently search for food."
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#required-libraries",
    "href": "posts/building-particle-swarm-optimizer.html#required-libraries",
    "title": "Building a Particle Swarm Optimizer from Scratch in R",
    "section": "Required Libraries",
    "text": "Required Libraries\n\n# Load required packages\nlibrary(dplyr)     # For data manipulation\nlibrary(ggplot2)   # For visualization\nlibrary(gganimate) # For animations\nlibrary(metR)      # For geom_arrow"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#the-challenge-a-complex-optimization-surface",
    "href": "posts/building-particle-swarm-optimizer.html#the-challenge-a-complex-optimization-surface",
    "title": "Building a Particle Swarm Optimizer from Scratch in R",
    "section": "The Challenge: A Complex Optimization Surface",
    "text": "The Challenge: A Complex Optimization Surface\nWe’ll test our optimizer on Ackley’s function – a challenging benchmark with many local minima that can trap optimization algorithms:\n\nobj_func &lt;- function(x, y){\n  # Modified Ackley function with global minimum at (1,1)\n  -20 * exp(-0.2 * sqrt(0.5 *((x-1)^2 + (y-1)^2))) - \n    exp(0.5*(cos(2*pi*x) + cos(2*pi*y))) + exp(1) + 20\n}\n\n# Create a visualization grid\nx &lt;- seq(-5, 5, length.out = 50)\ny &lt;- seq(-5, 5, length.out = 50)\ngrid &lt;- expand.grid(x, y, stringsAsFactors = FALSE)\ngrid$z &lt;- obj_func(grid[,1], grid[,2])\n\n# Create a contour plot\ncontour_plot &lt;- ggplot(grid, aes(x = Var1, y = Var2)) +\n  geom_contour_filled(aes(z = z), color = \"black\", alpha = 0.5) +\n  scale_fill_brewer(palette = \"Spectral\") + \n  theme_minimal() + \n  labs(x = \"x\", y = \"y\", title = \"Ackley's Function\")\n\ncontour_plot"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#how-pso-works",
    "href": "posts/building-particle-swarm-optimizer.html#how-pso-works",
    "title": "Building a Particle Swarm Optimizer from Scratch in R",
    "section": "How PSO Works",
    "text": "How PSO Works\nPSO mimics how birds find food by combining individual memory with social information:\n\nScatter random “particles” across the search space\nEach particle remembers its personal best position\nThe swarm shares information about the global best position\nParticles adjust their movement based on both personal and swarm knowledge\n\nThe movement equation balances three forces:\n\\[v_{new} = w \\cdot v_{current} + c_1 \\cdot r_1 \\cdot (p_{best} - p_{current}) + c_2 \\cdot r_2 \\cdot (g_{best} - p_{current})\\]\nWhere: - w: Inertia weight (momentum) - c1: Personal influence (memory) - c2: Social influence (cooperation) - r1,r2: Random values adding exploration"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#building-pso-step-by-step",
    "href": "posts/building-particle-swarm-optimizer.html#building-pso-step-by-step",
    "title": "Building a Particle Swarm Optimizer from Scratch in R",
    "section": "Building PSO Step by Step",
    "text": "Building PSO Step by Step\n\nStep 1: Initialize the Swarm\nFirst, we create a random swarm of particles and place them across our search space:\n\n# Set parameters\nn_particles &lt;- 20\nw &lt;- 0.5     # Inertia weight\nc1 &lt;- 0.05   # Personal learning rate\nc2 &lt;- 0.1    # Social learning rate\n\n# Create random particle positions\nx_range &lt;- seq(-5, 5, length.out = 20)\ny_range &lt;- seq(-5, 5, length.out = 20)\nX &lt;- data.frame(\n  x = sample(x_range, n_particles, replace = FALSE),\n  y = sample(y_range, n_particles, replace = FALSE)\n)\n\n# Visualize initial positions\ncontour_plot + \n  geom_point(data = X, aes(x, y), color = \"red\", size = 2.5) + \n  labs(title = \"Initial Particle Positions\")\n\n\n\n\n\n\n\n\n\n\nStep 2: Track Best Positions and Initialize Velocities\nNext, we track each particle’s personal best position and the swarm’s global best position:\n\n# Initialize random velocities\ndX &lt;- matrix(runif(n_particles * 2), ncol = 2) * w\n\n# Set initial personal best positions\npbest &lt;- X\npbest_obj &lt;- obj_func(X[,1], X[,2])\n\n# Find global best position\ngbest &lt;- pbest[which.min(pbest_obj),]\ngbest_obj &lt;- min(pbest_obj)\n\n# Visualize with arrows showing pull toward global best\nX_dir &lt;- X %&gt;% \n  mutate(g_x = gbest[1,1], \n         g_y = gbest[1,2], \n         angle = atan((g_y - y)/(g_x - x))*180/pi,\n         angle = ifelse(g_x &lt; x, 180 + angle, angle))\n\ncontour_plot + \n  geom_point(data = X, aes(x, y), color = \"red\", size = 2.5) + \n  geom_segment(data = X_dir, \n               aes(x = x, y = y, \n                   xend = x + 0.5*cos(angle*pi/180), \n                   yend = y + 0.5*sin(angle*pi/180)), \n               arrow = arrow(length = unit(0.1, \"cm\")), \n               color = \"blue\") + \n  labs(title = \"Forces Acting on Particles\")\n\n\n\n\n\n\n\n\n\n\nStep 3: Update Particle Positions\nNow we update each particle’s position based on its velocity and the forces acting on it:\n\n# Calculate new velocities using PSO equation\ndX &lt;- w * dX + \n      c1*runif(1)*(pbest - X) + \n      c2*runif(1)*(as.matrix(gbest) - X)\n\n# Update positions\nX &lt;- X + dX\n\n# Evaluate function at new positions\nobj &lt;- obj_func(X[,1], X[,2])\n\n# Update personal best positions if improved\nidx &lt;- which(obj &lt;= pbest_obj)\npbest[idx,] &lt;- X[idx,]\npbest_obj[idx] &lt;- obj[idx]\n\n# Update global best position\nidx &lt;- which.min(pbest_obj)\ngbest &lt;- pbest[idx,]\ngbest_obj &lt;- min(pbest_obj)\n\n# Visualize updated positions\nX_dir &lt;- X %&gt;% \n  mutate(g_x = gbest[1,1], \n         g_y = gbest[1,2], \n         angle = atan((g_y - y)/(g_x - x))*180/pi,\n         angle = ifelse(g_x &lt; x, 180 + angle, angle))\n\ncontour_plot + \n  geom_point(data = X, aes(x, y), color = \"red\", size = 2.5) + \n  geom_segment(data = X_dir, \n               aes(x = x, y = y, \n                   xend = x + 0.5*cos(angle*pi/180), \n                   yend = y + 0.5*sin(angle*pi/180)), \n               arrow = arrow(length = unit(0.1, \"cm\")), \n               color = \"blue\") + \n  labs(title = \"Particles After First Update\")"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#complete-pso-implementation",
    "href": "posts/building-particle-swarm-optimizer.html#complete-pso-implementation",
    "title": "Building a Particle Swarm Optimizer from Scratch in R",
    "section": "Complete PSO Implementation",
    "text": "Complete PSO Implementation\nNow let’s package everything into a reusable function:\n\npso_optim &lt;- function(obj_func,      # Function to minimize\n                      c1 = 0.05,      # Personal learning rate\n                      c2 = 0.05,      # Social learning rate\n                      w = 0.8,        # Inertia weight\n                      n_particles = 20,  # Swarm size\n                      init_fact = 0.1,   # Initial velocity factor\n                      n_iter = 50        # Maximum iterations\n){\n  # Define search domain\n  x &lt;- seq(-5, 5, length.out = 100)\n  y &lt;- seq(-5, 5, length.out = 100)\n  \n  # Initialize particles\n  X &lt;- cbind(sample(x, n_particles, replace = FALSE),\n             sample(y, n_particles, replace = FALSE))\n  dX &lt;- matrix(runif(n_particles * 2) * init_fact, ncol = 2)\n  \n  # Initialize best positions\n  pbest &lt;- X\n  pbest_obj &lt;- obj_func(x = X[,1], y = X[,2])\n  gbest &lt;- pbest[which.min(pbest_obj),]\n  gbest_obj &lt;- min(pbest_obj)\n  \n  # Store positions for visualization\n  loc_df &lt;- data.frame(X, iter = 0)\n  iter &lt;- 1\n  \n  # Main optimization loop\n  while(iter &lt; n_iter){\n    # Update velocities\n    dX &lt;- w * dX + \n          c1*runif(1)*(pbest - X) + \n          c2*runif(1)*t(gbest - t(X))\n    \n    # Update positions\n    X &lt;- X + dX\n    \n    # Evaluate and update best positions\n    obj &lt;- obj_func(x = X[,1], y = X[,2])\n    idx &lt;- which(obj &lt;= pbest_obj)\n    pbest[idx,] &lt;- X[idx,]\n    pbest_obj[idx] &lt;- obj[idx]\n    \n    # Update global best\n    idx &lt;- which.min(pbest_obj)\n    gbest &lt;- pbest[idx,]\n    gbest_obj &lt;- min(pbest_obj)\n    \n    # Store for visualization\n    iter &lt;- iter + 1\n    loc_df &lt;- rbind(loc_df, data.frame(X, iter = iter))\n  }\n  \n  return(list(X = loc_df, \n              obj = gbest_obj, \n              obj_loc = paste0(gbest, collapse = \",\")))\n}\n\nLet’s test our optimizer on the Ackley function:\n\n# Run the PSO algorithm\nout &lt;- pso_optim(obj_func,\n                 c1 = 0.01,    # Low personal influence\n                 c2 = 0.05,    # Moderate social influence\n                 w = 0.5,      # Medium inertia\n                 n_particles = 50,\n                 init_fact = 0.1,\n                 n_iter = 200)\n\n# Check the result (global minimum should be at (1,1))\nout$obj_loc\n\n[1] \"1.0000238436846,0.999984789266684\""
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#visualizing-the-swarm-in-action",
    "href": "posts/building-particle-swarm-optimizer.html#visualizing-the-swarm-in-action",
    "title": "Building a Particle Swarm Optimizer from Scratch in R",
    "section": "Visualizing the Swarm in Action",
    "text": "Visualizing the Swarm in Action\nThe real beauty of PSO is watching the particles converge on the solution:\n\n# Create animation of the optimization process\nggplot(out$X) +\n  geom_contour(data = grid, aes(x = Var1, y = Var2, z = z), color = \"black\") +\n  geom_point(aes(X1, X2)) +\n  labs(x = \"X\", y = \"Y\") +\n  transition_time(iter) +\n  ease_aes(\"linear\")"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#fine-tuning-your-swarm",
    "href": "posts/building-particle-swarm-optimizer.html#fine-tuning-your-swarm",
    "title": "Building a Particle Swarm Optimizer from Scratch in R",
    "section": "Fine-Tuning Your Swarm",
    "text": "Fine-Tuning Your Swarm\nThe PSO algorithm’s behavior can be dramatically altered by adjusting three key parameters:\n\nInertia Weight (w)\n\nHigh values (&gt;0.8): Particles maintain momentum and explore widely\nLow values (&lt;0.4): Particles slow down and focus on refining solutions\n\nPersonal Learning Rate (c1)\n\nHigh values: Particles favor their own discoveries\nLow values: Particles ignore their history\n\nSocial Learning Rate (c2)\n\nHigh values: Particles rush toward the global best\nLow values: Particles explore independently\n\n\nCommon parameter combinations: - Exploration focus: High w (0.9), balanced c1/c2 (0.5/0.5) - Exploitation focus: Low w (0.4), higher c2 than c1 (0.1/0.7)"
  },
  {
    "objectID": "posts/building-particle-swarm-optimizer.html#enhancing-your-pso-implementation",
    "href": "posts/building-particle-swarm-optimizer.html#enhancing-your-pso-implementation",
    "title": "Building a Particle Swarm Optimizer from Scratch in R",
    "section": "Enhancing Your PSO Implementation",
    "text": "Enhancing Your PSO Implementation\nFor real-world applications, consider these improvements:\n\nAdd boundary constraints to keep particles within valid regions\nImplement adaptive parameters that change during optimization\nAdd convergence-based stopping criteria\nExtend to higher dimensions for more complex problems\n\nThe R package pso offers a production-ready implementation!"
  },
  {
    "objectID": "posts/assessing-score-reliability.html",
    "href": "posts/assessing-score-reliability.html",
    "title": "Understanding Variability in Credit Score Predictions",
    "section": "",
    "text": "Credit scoring models work well in the middle of the score distribution but often become less reliable at the extremes where data is sparse. This post shows how to use bootstrapping to measure prediction variability across different score ranges – helping you identify where your model is most dependable."
  },
  {
    "objectID": "posts/assessing-score-reliability.html#why-estimation-variance-matters",
    "href": "posts/assessing-score-reliability.html#why-estimation-variance-matters",
    "title": "Understanding Variability in Credit Score Predictions",
    "section": "Why Estimation Variance Matters",
    "text": "Why Estimation Variance Matters\nSmaller sample sizes lead to higher variance in estimates, especially for extreme values. While statistics like means remain stable with limited data, tail percentiles (95th, 99th) show much more volatility. This matters for credit scoring, where very high and very low scores represent these unstable tail regions.\n\n# Number of samples to be drawn from a probability distribution\nn_samples &lt;- 1000\n\n# Number of times, sampling should be repeated\nrepeats &lt;- 100\n\n# Mean and std-dev for a standard normal distribution\nmu &lt;- 5\nstd_dev &lt;- 2\n\n# Sample\nsamples &lt;- rnorm(n_samples * repeats, mean = 10)\n\n# Fit into a matrix like object with `n_samples' number of rows \n# and `repeats` number of columns\nsamples &lt;- matrix(samples, nrow = n_samples, ncol = repeats)\n\n# Compute mean across each column\nsample_means &lt;- apply(samples, 1, mean)\n\n# Similarly, compute 75% and 95% quantile across each column\nsample_75_quantile &lt;- apply(samples, 1, quantile, p = 0.75)\nsample_95_quantile &lt;- apply(samples, 1, quantile, p = 0.95)\nsample_99_quantile &lt;- apply(samples, 1, quantile, p = 0.99)\n\n# Compare coefficient of variation\nsd(sample_means)/mean(sample_means)\n\n[1] 0.01017306\n\nsd(sample_75_quantile)/mean(sample_75_quantile)\n\n[1] 0.0127586\n\nsd(sample_95_quantile)/mean(sample_75_quantile)\n\n[1] 0.01873297\n\n# Plot the distributions\ncombined_vec &lt;- c(sample_means, sample_75_quantile, sample_95_quantile, sample_99_quantile)\n\nplot(density(sample_means), \n     col = \"#6F69AC\", \n     lwd = 3, \n     main = \"Estimating the mean vs tail quantiles\", \n     xlab = \"\", \n     xlim = c(min(combined_vec), max(combined_vec)))\n\nlines(density(sample_75_quantile), col = \"#95DAC1\", lwd = 3)\nlines(density(sample_95_quantile), col = \"#FFEBA1\", lwd = 3)\nlines(density(sample_99_quantile), col = \"#FD6F96\", lwd = 3)\ngrid()\n\nlegend(\"topright\", \n       fill = c(\"#6F69AC\", \"#95DAC1\", \"#FFEBA1\", \"#FD6F96\"), \n       legend = c(\"Mean\", \"75% Quantile\", \"95% Quantile\", \"99% Quantile\"), \n       cex = 0.7)\n\n\n\n\n\n\n\n\nThe plot shows how uncertainty increases dramatically when estimating extreme values. The distribution for the mean (purple) is much narrower than for the 99th percentile (pink). This directly translates to credit scoring – where very high or low scores have greater uncertainty.\n\n# Load required packages\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(rsample)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/assessing-score-reliability.html#data-acquisition-and-preprocessing",
    "href": "posts/assessing-score-reliability.html#data-acquisition-and-preprocessing",
    "title": "Understanding Variability in Credit Score Predictions",
    "section": "Data Acquisition and Preprocessing",
    "text": "Data Acquisition and Preprocessing\n\n# Load sample data (sample of the lending club data)\nsample &lt;- read.csv(\"http://bit.ly/42ypcnJ\")\n\n# Mark which loan status will be tagged as default\ncodes &lt;- c(\"Charged Off\", \"Does not meet the credit policy. Status:Charged Off\")\n\n# Apply above codes and create target\nsample %&lt;&gt;% mutate(bad_flag = ifelse(loan_status %in% codes, 1, 0))\n\n# Replace missing values with a default value\nsample[is.na(sample)] &lt;- -1\n\n# Get summary tally\ntable(sample$bad_flag)\n\n\n   0    1 \n8838 1162 \n\n\nWe’re using Lending Club data with charged-off loans marked as defaults. The class imbalance shown is typical in credit portfolios and contributes to prediction challenges at distribution extremes."
  },
  {
    "objectID": "posts/assessing-score-reliability.html#implementing-bootstrap-resampling-strategy",
    "href": "posts/assessing-score-reliability.html#implementing-bootstrap-resampling-strategy",
    "title": "Understanding Variability in Credit Score Predictions",
    "section": "Implementing Bootstrap Resampling Strategy",
    "text": "Implementing Bootstrap Resampling Strategy\nWe’ll create 100 bootstrap samples to measure how model predictions vary across the score range. This technique creates multiple simulated datasets to measure prediction uncertainty without collecting additional data.\n\n# Create 100 samples\nboot_sample &lt;- bootstraps(data = sample, times = 100)\n\nhead(boot_sample, 3)\n\n# A tibble: 3 × 2\n  splits               id          \n  &lt;list&gt;               &lt;chr&gt;       \n1 &lt;split [10000/3707]&gt; Bootstrap001\n2 &lt;split [10000/3688]&gt; Bootstrap002\n3 &lt;split [10000/3694]&gt; Bootstrap003\n\n# Each row represents a separate bootstrapped sample with an analysis set and assessment set\nboot_sample$splits[[1]]\n\n&lt;Analysis/Assess/Total&gt;\n&lt;10000/3707/10000&gt;\n\n# Show the first 5 rows and 5 columns of the first sample\nanalysis(boot_sample$splits[[1]]) %&gt;% .[1:5, 1:5]\n\n     V1       id member_id loan_amnt funded_amnt\n1 99086 11295404        -1     24000       24000\n2 88986   747883        -1     30000       30000\n3 69923 21741383        -1     14000       14000\n4 30931 10588167        -1      3500        3500\n5 64867 24044848        -1      6000        6000\n\n\nEach bootstrap sample contains random draws (with replacement) from our original data, creating slight variations that reveal model sensitivity to different data compositions."
  },
  {
    "objectID": "posts/assessing-score-reliability.html#developing-the-predictive-model-framework",
    "href": "posts/assessing-score-reliability.html#developing-the-predictive-model-framework",
    "title": "Understanding Variability in Credit Score Predictions",
    "section": "Developing the Predictive Model Framework",
    "text": "Developing the Predictive Model Framework\nWe’ll use logistic regression – the standard for credit risk models due to its interpretability and regulatory acceptance. Our model includes typical credit variables like loan amount, income, and credit history metrics.\n\nglm_model &lt;- function(df){\n  \n  # Fit a simple model with a set specification\n  mdl &lt;- glm(bad_flag ~\n               loan_amnt + funded_amnt + annual_inc + delinq_2yrs +\n               inq_last_6mths + mths_since_last_delinq + fico_range_low +\n               mths_since_last_record + revol_util + total_pymnt,\n             family = \"binomial\",\n             data = df)\n  \n  # Return fitted values\n  return(predict(mdl))\n}\n\n# Test the function\n# Retrieve a data frame\ntrain &lt;- analysis(boot_sample$splits[[1]])\n\n# Predict\npred &lt;- glm_model(train)\n\n# Check output\nrange(pred)  # Output is on log odds scale\n\n[1] -20.022728   1.124628\n\n\nThe function returns predictions in log-odds, which we’ll later convert to a more intuitive credit score scale."
  },
  {
    "objectID": "posts/assessing-score-reliability.html#iterative-model-training-and-prediction-collection",
    "href": "posts/assessing-score-reliability.html#iterative-model-training-and-prediction-collection",
    "title": "Understanding Variability in Credit Score Predictions",
    "section": "Iterative Model Training and Prediction Collection",
    "text": "Iterative Model Training and Prediction Collection\n\n# First apply the glm fitting function to each of the sample\n# Note the use of lapply\noutput &lt;- lapply(boot_sample$splits, function(x){\n  train &lt;- analysis(x)\n  pred &lt;- glm_model(train)\n\n  return(pred)\n})\n\n# Collate all predictions into a vector \nboot_preds &lt;- do.call(c, output)\nrange(boot_preds)\n\n[1] -141.189195    8.873295\n\n# Get outliers\nq_high &lt;- quantile(boot_preds, 0.99)\nq_low &lt;- quantile(boot_preds, 0.01)\n\n# Truncate the overall distribution to within the lower 1% and upper 1% quantiles\n# Doing this since it creates issues later on when scaling the output\nboot_preds[boot_preds &gt; q_high] &lt;- q_high\nboot_preds[boot_preds &lt; q_low] &lt;- q_low\n\nrange(boot_preds)\n\n[1] -5.0581226 -0.2312987\n\n# Convert to a data frame\nboot_preds &lt;- data.frame(pred = boot_preds, \n                         id = rep(1:length(boot_sample$splits), each = nrow(sample)))\nhead(boot_preds)\n\n        pred id\n1 -3.5243889  1\n2 -5.0220877  1\n3 -1.7385793  1\n4 -1.9862655  1\n5 -1.5898113  1\n6 -0.5679697  1\n\n\nWe apply our model to each bootstrap sample and collect the predictions, then truncate extreme values (beyond 1st and 99th percentiles) to remove outliers – similar to capping techniques used in production credit models."
  },
  {
    "objectID": "posts/assessing-score-reliability.html#transforming-predictions-to-credit-score-scale",
    "href": "posts/assessing-score-reliability.html#transforming-predictions-to-credit-score-scale",
    "title": "Understanding Variability in Credit Score Predictions",
    "section": "Transforming Predictions to Credit Score Scale",
    "text": "Transforming Predictions to Credit Score Scale\nNow we’ll convert log-odds to a recognizable credit score using the industry-standard Points to Double Odds (PDO) method. Using parameters similar to real credit systems (PDO=30, Anchor=700), we transform our predictions into intuitive scores where higher numbers indicate lower risk.\n\nscaling_func &lt;- function(vec, PDO = 30, OddsAtAnchor = 5, Anchor = 700){\n  beta &lt;- PDO / log(2)\n  alpha &lt;- Anchor - PDO * OddsAtAnchor\n  \n  # Simple linear scaling of the log odds\n  scr &lt;- alpha - beta * vec  \n  \n  # Round off\n  return(round(scr, 0))\n}\n\nboot_preds$scores &lt;- scaling_func(boot_preds$pred, 30, 2, 700)\n\n# Chart the distribution of predictions across all the samples\nggplot(boot_preds, aes(x = scores, color = factor(id))) + \n  geom_density() + \n  theme_minimal() + \n  theme(legend.position = \"none\") + \n  scale_color_grey() + \n  labs(title = \"Predictions from bootstrapped samples\", \n       subtitle = \"Density function\", \n       x = \"Predictions (Log odds)\", \n       y = \"Density\")\n\n\n\n\n\n\n\n\nEach gray line shows the score distribution from a different bootstrap sample. Where lines cluster tightly, our predictions are stable; where they diverge, we have higher uncertainty."
  },
  {
    "objectID": "posts/assessing-score-reliability.html#quantifying-prediction-uncertainty-across-score-ranges",
    "href": "posts/assessing-score-reliability.html#quantifying-prediction-uncertainty-across-score-ranges",
    "title": "Understanding Variability in Credit Score Predictions",
    "section": "Quantifying Prediction Uncertainty Across Score Ranges",
    "text": "Quantifying Prediction Uncertainty Across Score Ranges\nNow we can directly measure how prediction reliability varies across score ranges by calculating standard deviation within each score bin. This approach quantifies uncertainty at different score levels.\n\n# Create bins using quantiles\nbreaks &lt;- quantile(boot_preds$scores, probs = seq(0, 1, length.out = 20))\nboot_preds$bins &lt;- cut(boot_preds$scores, breaks = unique(breaks), include.lowest = T, right = T)\n\n# Chart standard deviation of model predictions across each score bin\nboot_preds %&gt;%\n  group_by(bins) %&gt;%\n  summarise(std_dev = sd(scores)) %&gt;%\n  ggplot(aes(x = bins, y = std_dev)) +\n  geom_col(color = \"black\", fill = \"#90AACB\") +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 90)) + \n  theme(legend.position = \"none\") + \n  labs(title = \"Variability in model predictions across samples\", \n       subtitle = \"(measured using standard deviation)\", \n       x = \"Score Range\", \n       y = \"Standard Deviation\")\n\n\n\n\n\n\n\n\nAs expected, the model’s predictions are more reliable within a certain range of values (700-800) whereas there is significant variability in the model’s predictions in the lowest and highest score buckets.\nThe chart reveals a clear “U-shape” pattern of prediction variability—a common phenomenon in credit risk modeling. The highest uncertainty appears in the extreme score ranges (very high and very low scores), while predictions in the middle range show greater stability. The chart confirms our hypothesis: variability is highest at score extremes and lowest in the middle range (600-800). This directly informs credit policy – scores in the middle range are most reliable, while decisions at the extremes should incorporate additional caution due to higher uncertainty.\nThese findings have direct business applications:\n\nFor extremely high scores, add verification steps before auto-approval\nFor very low scores, consider manual review rather than automatic rejection"
  },
  {
    "objectID": "posts/assessing-score-reliability.html#advanced-approach-isolating-training-data-effects",
    "href": "posts/assessing-score-reliability.html#advanced-approach-isolating-training-data-effects",
    "title": "Understanding Variability in Credit Score Predictions",
    "section": "Advanced Approach: Isolating Training Data Effects",
    "text": "Advanced Approach: Isolating Training Data Effects\nCredit: Richard Warnung\nFor a more controlled analysis, we can train models on bootstrap samples but evaluate them on the same validation set. This isolates the impact of training data variation:\n\nVs &lt;- function(boot_split){\n  # Train model on the bootstrapped data\n  train &lt;- analysis(boot_split)\n  \n  # Fit model\n  mdl &lt;- glm(bad_flag ~\n               loan_amnt + funded_amnt + annual_inc + delinq_2yrs +\n               inq_last_6mths + mths_since_last_delinq + fico_range_low +\n               mths_since_last_record + revol_util + total_pymnt,\n             family = \"binomial\",\n             data = train)\n  \n  # Apply to a common validation set\n  validate_preds &lt;- predict(mdl, newdata = validate_set)\n  \n  # Return predictions\n  return(validate_preds)\n}\n\nThis method provides a clearer picture of how variations in training data affect model predictions, which is valuable when evaluating model updates in production."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R'tichoke",
    "section": "",
    "text": "R’tichoke is a repository of R programming content, tutorials, and resources. Whether you’re just starting your journey with R or you’re an experienced data scientist looking to expand your toolkit, you’ll find valuable content here to help you grow your skills."
  },
  {
    "objectID": "index.html#welcome-to-rtichoke",
    "href": "index.html#welcome-to-rtichoke",
    "title": "R'tichoke",
    "section": "",
    "text": "R’tichoke is a repository of R programming content, tutorials, and resources. Whether you’re just starting your journey with R or you’re an experienced data scientist looking to expand your toolkit, you’ll find valuable content here to help you grow your skills."
  },
  {
    "objectID": "index.html#latest-articles",
    "href": "index.html#latest-articles",
    "title": "R'tichoke",
    "section": "Latest Articles",
    "text": "Latest Articles\nCheck out our blog for the latest tutorials, case studies, and tips on R programming."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "R'tichoke",
    "section": "Getting Started",
    "text": "Getting Started\nNew to R? Start here:\n\nInstalling R and RStudio - Set up your development environment\nIntroduction to Analytics with R - Learn the basics\nData Wrangling with dplyr - Level up your data manipulation skills"
  },
  {
    "objectID": "get-started/reading-data.html",
    "href": "get-started/reading-data.html",
    "title": "Reading Data into R",
    "section": "",
    "text": "R provides several built-in functions for importing data from various file formats. Here’s how to use the most common ones:\n\n\nComma-separated values (CSV) files are one of the most common data formats:\n\n# Create a sample CSV file\nwrite.csv(mtcars[1:5, ], \"sample_cars.csv\", row.names = TRUE)\n\n# Read the CSV file\ncars_data &lt;- read.csv(\"sample_cars.csv\")\nhead(cars_data)\n\n                  X  mpg cyl disp  hp drat    wt  qsec vs am gear carb\n1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n# Read with specific options\ncars_data2 &lt;- read.csv(\"sample_cars.csv\", \n                      header = TRUE,       # First row contains column names\n                      sep = \",\",           # Separator is a comma\n                      stringsAsFactors = FALSE, # Don't convert strings to factors\n                      na.strings = c(\"NA\", \"N/A\", \"\")) # Values to treat as NA\nhead(cars_data2)\n\n                  X  mpg cyl disp  hp drat    wt  qsec vs am gear carb\n1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n\n\n\n\nTab-delimited files are another common format:\n\n# Create a sample tab-delimited file\nwrite.table(mtcars[1:5, ], \"sample_cars.txt\", sep = \"\\t\", row.names = TRUE)\n\n# Read the tab-delimited file\ncars_data_tab &lt;- read.delim(\"sample_cars.txt\")\nhead(cars_data_tab)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n# Or use read.table with tab separator\ncars_data_tab2 &lt;- read.table(\"sample_cars.txt\", \n                            header = TRUE, \n                            sep = \"\\t\")\nhead(cars_data_tab2)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n\n\n\n\nFixed-width files have fields of consistent width:\n\n# Create a sample fixed-width file\ncat(\"John  Smith 35\\nMary  Jones 28\\nDavid Brown 42\\n\", file = \"sample_people.txt\")\n\n# Read the fixed-width file\npeople_data &lt;- read.fwf(\"sample_people.txt\", \n                       widths = c(5, 6, 3),  # Width of each column\n                       col.names = c(\"First\", \"Last\", \"Age\"))\npeople_data\n\n  First   Last Age\n1 John   Smith  35\n2 Mary   Jones  28\n3 David  Brown  42\n\n\n\n\n\nR has its own binary file format for saving and loading R objects:\n\n# Save R objects to a file\nsample_data &lt;- list(x = 1:10, y = letters[1:10])\nsave(sample_data, file = \"sample_data.RData\")\n\n# Load the saved objects\nload(\"sample_data.RData\")\nsample_data\n\n$x\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$y\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\n\n# Save a single object\nsaveRDS(mtcars[1:5, ], \"sample_cars.rds\")\n\n# Read the saved object\ncars_subset &lt;- readRDS(\"sample_cars.rds\")\nhead(cars_subset)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n\n\n\n\nYou can read data directly from the web:\n\n# Read CSV from a URL (example with a small dataset)\nurl &lt;- \"https://raw.githubusercontent.com/datasets/iris/master/data/iris.csv\"\niris_data &lt;- try(read.csv(url), silent = TRUE)\n\n# Check if the data was loaded successfully\nif (!inherits(iris_data, \"try-error\")) {\n  head(iris_data)\n} else {\n  print(\"Could not access the URL. Check your internet connection.\")\n}\n\n[1] \"Could not access the URL. Check your internet connection.\"\n\n\n\n\n\nWhile not part of base R, the readxl package is commonly used:\n\n# Check if readxl is installed\nif (!requireNamespace(\"readxl\", quietly = TRUE)) {\n  message(\"The readxl package is not installed. You can install it with: install.packages('readxl')\")\n} else {\n  library(readxl)\n  # This would read an Excel file if it existed\n  # excel_data &lt;- read_excel(\"sample.xlsx\", sheet = 1)\n}\n\n\n\n\nBase R provides the DBI package for database connections:\n\n# Example of connecting to SQLite (not run)\n# if (!requireNamespace(\"RSQLite\", quietly = TRUE)) {\n#   message(\"The RSQLite package is not installed\")\n# } else {\n#   library(DBI)\n#   con &lt;- dbConnect(RSQLite::SQLite(), \":memory:\")\n#   dbWriteTable(con, \"mtcars\", mtcars)\n#   data &lt;- dbGetQuery(con, \"SELECT * FROM mtcars WHERE cyl = 4\")\n#   dbDisconnect(con)\n# }\n\n\n\n\nR provides functions to work with file paths:\n\n# Get current working directory\ngetwd()\n\n[1] \"C:/Users/riddh/OneDrive/Desktop/rtichoke/get-started\"\n\n# List files in the current directory\nlist.files(pattern = \".csv\")\n\n[1] \"sample_cars.csv\"\n\n# Check if a file exists\nfile.exists(\"sample_cars.csv\")\n\n[1] TRUE\n\n# Get full path to a file\nnormalizePath(\"sample_cars.csv\", mustWork = FALSE)\n\n[1] \"C:\\\\Users\\\\riddh\\\\OneDrive\\\\Desktop\\\\rtichoke\\\\get-started\\\\sample_cars.csv\"\n\n\n\n\n\nLet’s remove the sample files we created:\n\n# List of files to remove\nfiles_to_remove &lt;- c(\"sample_cars.csv\", \"sample_cars.txt\", \n                    \"sample_people.txt\", \"sample_data.RData\", \n                    \"sample_cars.rds\")\n\n# Remove files\nfor (file in files_to_remove) {\n  if (file.exists(file)) {\n    file.remove(file)\n  }\n}\n\nRemember to check the documentation with ?read.csv or similar commands to explore all available options for these functions."
  },
  {
    "objectID": "get-started/reading-data.html#reading-data-into-r-using-base-functions",
    "href": "get-started/reading-data.html#reading-data-into-r-using-base-functions",
    "title": "Reading Data into R",
    "section": "",
    "text": "R provides several built-in functions for importing data from various file formats. Here’s how to use the most common ones:\n\n\nComma-separated values (CSV) files are one of the most common data formats:\n\n# Create a sample CSV file\nwrite.csv(mtcars[1:5, ], \"sample_cars.csv\", row.names = TRUE)\n\n# Read the CSV file\ncars_data &lt;- read.csv(\"sample_cars.csv\")\nhead(cars_data)\n\n                  X  mpg cyl disp  hp drat    wt  qsec vs am gear carb\n1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n# Read with specific options\ncars_data2 &lt;- read.csv(\"sample_cars.csv\", \n                      header = TRUE,       # First row contains column names\n                      sep = \",\",           # Separator is a comma\n                      stringsAsFactors = FALSE, # Don't convert strings to factors\n                      na.strings = c(\"NA\", \"N/A\", \"\")) # Values to treat as NA\nhead(cars_data2)\n\n                  X  mpg cyl disp  hp drat    wt  qsec vs am gear carb\n1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n\n\n\n\nTab-delimited files are another common format:\n\n# Create a sample tab-delimited file\nwrite.table(mtcars[1:5, ], \"sample_cars.txt\", sep = \"\\t\", row.names = TRUE)\n\n# Read the tab-delimited file\ncars_data_tab &lt;- read.delim(\"sample_cars.txt\")\nhead(cars_data_tab)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n# Or use read.table with tab separator\ncars_data_tab2 &lt;- read.table(\"sample_cars.txt\", \n                            header = TRUE, \n                            sep = \"\\t\")\nhead(cars_data_tab2)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n\n\n\n\nFixed-width files have fields of consistent width:\n\n# Create a sample fixed-width file\ncat(\"John  Smith 35\\nMary  Jones 28\\nDavid Brown 42\\n\", file = \"sample_people.txt\")\n\n# Read the fixed-width file\npeople_data &lt;- read.fwf(\"sample_people.txt\", \n                       widths = c(5, 6, 3),  # Width of each column\n                       col.names = c(\"First\", \"Last\", \"Age\"))\npeople_data\n\n  First   Last Age\n1 John   Smith  35\n2 Mary   Jones  28\n3 David  Brown  42\n\n\n\n\n\nR has its own binary file format for saving and loading R objects:\n\n# Save R objects to a file\nsample_data &lt;- list(x = 1:10, y = letters[1:10])\nsave(sample_data, file = \"sample_data.RData\")\n\n# Load the saved objects\nload(\"sample_data.RData\")\nsample_data\n\n$x\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$y\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\n\n# Save a single object\nsaveRDS(mtcars[1:5, ], \"sample_cars.rds\")\n\n# Read the saved object\ncars_subset &lt;- readRDS(\"sample_cars.rds\")\nhead(cars_subset)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n\n\n\n\nYou can read data directly from the web:\n\n# Read CSV from a URL (example with a small dataset)\nurl &lt;- \"https://raw.githubusercontent.com/datasets/iris/master/data/iris.csv\"\niris_data &lt;- try(read.csv(url), silent = TRUE)\n\n# Check if the data was loaded successfully\nif (!inherits(iris_data, \"try-error\")) {\n  head(iris_data)\n} else {\n  print(\"Could not access the URL. Check your internet connection.\")\n}\n\n[1] \"Could not access the URL. Check your internet connection.\"\n\n\n\n\n\nWhile not part of base R, the readxl package is commonly used:\n\n# Check if readxl is installed\nif (!requireNamespace(\"readxl\", quietly = TRUE)) {\n  message(\"The readxl package is not installed. You can install it with: install.packages('readxl')\")\n} else {\n  library(readxl)\n  # This would read an Excel file if it existed\n  # excel_data &lt;- read_excel(\"sample.xlsx\", sheet = 1)\n}\n\n\n\n\nBase R provides the DBI package for database connections:\n\n# Example of connecting to SQLite (not run)\n# if (!requireNamespace(\"RSQLite\", quietly = TRUE)) {\n#   message(\"The RSQLite package is not installed\")\n# } else {\n#   library(DBI)\n#   con &lt;- dbConnect(RSQLite::SQLite(), \":memory:\")\n#   dbWriteTable(con, \"mtcars\", mtcars)\n#   data &lt;- dbGetQuery(con, \"SELECT * FROM mtcars WHERE cyl = 4\")\n#   dbDisconnect(con)\n# }\n\n\n\n\nR provides functions to work with file paths:\n\n# Get current working directory\ngetwd()\n\n[1] \"C:/Users/riddh/OneDrive/Desktop/rtichoke/get-started\"\n\n# List files in the current directory\nlist.files(pattern = \".csv\")\n\n[1] \"sample_cars.csv\"\n\n# Check if a file exists\nfile.exists(\"sample_cars.csv\")\n\n[1] TRUE\n\n# Get full path to a file\nnormalizePath(\"sample_cars.csv\", mustWork = FALSE)\n\n[1] \"C:\\\\Users\\\\riddh\\\\OneDrive\\\\Desktop\\\\rtichoke\\\\get-started\\\\sample_cars.csv\"\n\n\n\n\n\nLet’s remove the sample files we created:\n\n# List of files to remove\nfiles_to_remove &lt;- c(\"sample_cars.csv\", \"sample_cars.txt\", \n                    \"sample_people.txt\", \"sample_data.RData\", \n                    \"sample_cars.rds\")\n\n# Remove files\nfor (file in files_to_remove) {\n  if (file.exists(file)) {\n    file.remove(file)\n  }\n}\n\nRemember to check the documentation with ?read.csv or similar commands to explore all available options for these functions."
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html",
    "href": "get-started/parallel-computing-in-r.html",
    "title": "Getting Started with Parallel Computing in R",
    "section": "",
    "text": "Have you ever run an R script that took hours to complete? Parallel computing can help! Instead of running calculations one after another (sequentially), parallel computing allows you to run multiple calculations at the same time by using all available CPU cores on your computer.\nBenefits: - Speed: Run code significantly faster - Efficiency: Make better use of your computer’s resources - Scalability: Handle larger datasets and more complex models"
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#what-is-parallel-computing-and-why-use-it",
    "href": "get-started/parallel-computing-in-r.html#what-is-parallel-computing-and-why-use-it",
    "title": "Getting Started with Parallel Computing in R",
    "section": "",
    "text": "Have you ever run an R script that took hours to complete? Parallel computing can help! Instead of running calculations one after another (sequentially), parallel computing allows you to run multiple calculations at the same time by using all available CPU cores on your computer.\nBenefits: - Speed: Run code significantly faster - Efficiency: Make better use of your computer’s resources - Scalability: Handle larger datasets and more complex models"
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#quick-setup-required-packages",
    "href": "get-started/parallel-computing-in-r.html#quick-setup-required-packages",
    "title": "Getting Started with Parallel Computing in R",
    "section": "Quick Setup: Required Packages",
    "text": "Quick Setup: Required Packages\nLet’s start by installing and loading the packages we’ll need:\n\n# Install packages if needed (uncomment to run)\n# install.packages(c(\"parallel\", \"foreach\", \"doParallel\", \"tictoc\"))\n\n# Load the essential packages\nlibrary(parallel)    # Base R parallel functions\nlibrary(foreach)     # For parallel loops\nlibrary(doParallel)  # Backend for foreach\nlibrary(tictoc)      # For timing comparisons"
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#how-many-cores-do-you-have",
    "href": "get-started/parallel-computing-in-r.html#how-many-cores-do-you-have",
    "title": "Getting Started with Parallel Computing in R",
    "section": "How Many Cores Do You Have?",
    "text": "How Many Cores Do You Have?\nThe first step is to check how many CPU cores are available on your computer:\n\n# Detect the number of CPU cores\ndetectCores()\n\n[1] 16\n\n\nIt’s usually good practice to leave one core free for your operating system, so we’ll typically use detectCores() - 1 for our parallel operations."
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#your-first-parallel-code-the-basics",
    "href": "get-started/parallel-computing-in-r.html#your-first-parallel-code-the-basics",
    "title": "Getting Started with Parallel Computing in R",
    "section": "Your First Parallel Code: The Basics",
    "text": "Your First Parallel Code: The Basics\nLet’s create a simple function that takes some time to run, then compare how long it takes to run sequentially versus in parallel:\n\n# A function that takes time to execute\nslow_function &lt;- function(x) {\n  Sys.sleep(0.5)  # Simulate computation time (half a second)\n  return(x^2)     # Return the square of x\n}\n\n# Create a list of numbers to process\nnumbers &lt;- 1:10\n\n\nMethod 1: Using parLapply (Works on All Systems)\nThis method works on all operating systems including Windows:\n\n# Step 1: Create a cluster of workers\ncl &lt;- makeCluster(detectCores() - 1)\n\n# Step 2: Export any functions our workers need\nclusterExport(cl, \"slow_function\")\n\n# Run the sequential version and time it\ntic(\"Sequential version\")\nresult_sequential &lt;- lapply(numbers, slow_function)\ntoc()\n\nSequential version: 5.08 sec elapsed\n\n# Run the parallel version and time it\ntic(\"Parallel version\")\nresult_parallel &lt;- parLapply(cl, numbers, slow_function)\ntoc()\n\nParallel version: 0.52 sec elapsed\n\n# Step 3: Always stop the cluster when done!\nstopCluster(cl)\n\n# Verify both methods give the same results\nall.equal(result_sequential, result_parallel)\n\n[1] TRUE\n\n\n\n\nMethod 2: Using mclapply (Unix/Mac Only)\nIf you’re on Mac or Linux, you can use this simpler approach:\n\n# For Mac/Linux users only\ntic(\"Parallel mclapply (Mac/Linux only)\")\nresult_parallel &lt;- mclapply(numbers, slow_function, mc.cores = detectCores() - 1)\ntoc()"
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#the-foreach-package-a-more-intuitive-approach",
    "href": "get-started/parallel-computing-in-r.html#the-foreach-package-a-more-intuitive-approach",
    "title": "Getting Started with Parallel Computing in R",
    "section": "The foreach Package: A More Intuitive Approach",
    "text": "The foreach Package: A More Intuitive Approach\nMany R users find the foreach package easier to understand and use. It works like a loop but can run in parallel:\n\n# Step 1: Create and register a parallel backend\ncl &lt;- makeCluster(detectCores() - 1)\nregisterDoParallel(cl)\n\n# Run sequential foreach with %do%\ntic(\"Sequential foreach\")\nresult_sequential &lt;- foreach(i = 1:10) %do% {\n  slow_function(i)\n}\ntoc()\n\n# Run parallel foreach with %dopar%\ntic(\"Parallel foreach\")\nresult_parallel &lt;- foreach(i = 1:10) %dopar% {\n  slow_function(i)\n}\ntoc()\n\n# Always stop the cluster when done\nstopCluster(cl)\n\n# Verify results\nall.equal(result_sequential, result_parallel)\n\n\nCombining Results with foreach\nOne of the great features of foreach is how easily you can combine results:\n\n# Create and register a parallel backend\ncl &lt;- makeCluster(detectCores() - 1)\nregisterDoParallel(cl)\n\n# Sum all results automatically with .combine='+'\ntic(\"Parallel sum of squares\")\ntotal &lt;- foreach(i = 1:100, .combine = '+') %dopar% {\n  i^2\n}\ntoc()\n\nParallel sum of squares: 0.13 sec elapsed\n\n# Stop the cluster\nstopCluster(cl)\n\n# Verify the result\nprint(paste(\"Our result:\", total))\n\n[1] \"Our result: 338350\"\n\nprint(paste(\"Correct answer:\", sum((1:100)^2)))\n\n[1] \"Correct answer: 338350\""
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#a-real-example-matrix-operations",
    "href": "get-started/parallel-computing-in-r.html#a-real-example-matrix-operations",
    "title": "Getting Started with Parallel Computing in R",
    "section": "A Real Example: Matrix Operations",
    "text": "A Real Example: Matrix Operations\nLet’s try something more realistic. Matrix operations are perfect for parallelization:\n\n# A more computationally intensive function\nmatrix_function &lt;- function(n) {\n  # Create a random n×n matrix\n  m &lt;- matrix(rnorm(n*n), ncol = n)\n  # Calculate eigenvalues (computationally expensive)\n  eigen(m)\n  return(sum(diag(m)))\n}\n\n# Let's process 8 matrices of size 300×300\nmatrix_sizes &lt;- rep(300, 8)\n\n\nComparing Methods\nLet’s compare how different methods perform:\n\n# Sequential execution\ntic(\"Sequential\")\nsequential_result &lt;- lapply(matrix_sizes, matrix_function)\nsequential_time &lt;- toc(quiet = TRUE)\nsequential_time &lt;- sequential_time$toc - sequential_time$tic\n\n# Parallel with parLapply\ncl &lt;- makeCluster(detectCores() - 1)\nclusterExport(cl, \"matrix_function\")\ntic(\"parLapply\")\nparlapply_result &lt;- parLapply(cl, matrix_sizes, matrix_function)\nparlapply_time &lt;- toc(quiet = TRUE)\nparlapply_time &lt;- parlapply_time$toc - parlapply_time$tic\nstopCluster(cl)\n\n# Parallel with foreach\ncl &lt;- makeCluster(detectCores() - 1)\nregisterDoParallel(cl)\ntic(\"foreach\")\nforeach_result &lt;- foreach(s = matrix_sizes) %dopar% {\n  matrix_function(s)\n}\nforeach_time &lt;- toc(quiet = TRUE)\nforeach_time &lt;- foreach_time$toc - foreach_time$tic\nstopCluster(cl)\n\n# Create a results table\nresults &lt;- data.frame(\n  Method = c(\"Sequential\", \"parLapply\", \"foreach\"),\n  Time = c(sequential_time, parlapply_time, foreach_time),\n  Speedup = c(1, sequential_time/parlapply_time, sequential_time/foreach_time)\n)\n\n# Display the results\nresults\n\n      Method Time  Speedup\n1 Sequential 2.49 1.000000\n2  parLapply 0.31 8.032258\n3    foreach 0.42 5.928571\n\n\n\n\nVisualizing the Results\n\n# Load ggplot2 for visualization\nlibrary(ggplot2)\n\n# Plot execution times\nggplot(results, aes(x = reorder(Method, -Time), y = Time, fill = Method)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Execution Time Comparison\",\n       x = \"Method\", y = \"Time (seconds)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n# Plot speedup\nggplot(results, aes(x = reorder(Method, Speedup), y = Speedup, fill = Method)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Speedup Comparison\",\n       x = \"Method\", y = \"Times faster than sequential\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#when-to-use-parallel-computing",
    "href": "get-started/parallel-computing-in-r.html#when-to-use-parallel-computing",
    "title": "Getting Started with Parallel Computing in R",
    "section": "When to Use Parallel Computing",
    "text": "When to Use Parallel Computing\nParallel computing isn’t always the right choice. Here’s when to use it:\n✅ Good for parallelization: - Independent calculations (like applying the same function to different data chunks) - Computationally intensive tasks (simulations, bootstrap resampling) - Tasks that take more than a few seconds to run sequentially\n❌ Not good for parallelization: - Very quick operations (parallelization overhead may exceed the time saved) - Tasks with heavy dependencies between steps - I/O-bound operations (reading/writing files)"
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#quick-tips-for-success",
    "href": "get-started/parallel-computing-in-r.html#quick-tips-for-success",
    "title": "Getting Started with Parallel Computing in R",
    "section": "Quick Tips for Success",
    "text": "Quick Tips for Success\n\nAlways stop your clusters with stopCluster(cl) when you’re done\nLeave one core free for your operating system\nStart small and test with a subset of your data\nWatch your memory usage - each worker needs its own copy of the data"
  },
  {
    "objectID": "get-started/parallel-computing-in-r.html#next-steps",
    "href": "get-started/parallel-computing-in-r.html#next-steps",
    "title": "Getting Started with Parallel Computing in R",
    "section": "Next Steps",
    "text": "Next Steps\nOnce you’re comfortable with these basics, you can explore:\n\nThe future package for more advanced parallel computing\nThe furrr package for parallel versions of purrr functions\nParallel computing with large datasets using data.table or dplyr\nDistributed computing across multiple machines"
  },
  {
    "objectID": "get-started/r-package-management.html",
    "href": "get-started/r-package-management.html",
    "title": "Installing and Managing R Packages",
    "section": "",
    "text": "R’s true power comes from its vast ecosystem of packages. This guide will show you how to effectively install, update, and manage packages for your data analysis projects."
  },
  {
    "objectID": "get-started/r-package-management.html#installing-packages",
    "href": "get-started/r-package-management.html#installing-packages",
    "title": "Installing and Managing R Packages",
    "section": "Installing Packages",
    "text": "Installing Packages\nR packages can be installed from CRAN (the Comprehensive R Archive Network) using the install.packages() function:\n\n# Install a single package\ninstall.packages(\"dplyr\")\n\n# Install multiple packages at once\ninstall.packages(c(\"ggplot2\", \"tidyr\", \"readr\"))\n\nSome packages may require you to select a CRAN mirror for downloading. Simply choose a location near you from the list that appears."
  },
  {
    "objectID": "get-started/r-package-management.html#loading-packages",
    "href": "get-started/r-package-management.html#loading-packages",
    "title": "Installing and Managing R Packages",
    "section": "Loading Packages",
    "text": "Loading Packages\nOnce installed, you need to load packages in each R session before using them:\n\n# Load a package\nlibrary(ggplot2)\n\n# You can now use functions from the package\nggplot(mtcars, aes(x = wt, y = mpg)) + \n  geom_point() + \n  theme_minimal()"
  },
  {
    "objectID": "get-started/r-package-management.html#checking-installed-packages",
    "href": "get-started/r-package-management.html#checking-installed-packages",
    "title": "Installing and Managing R Packages",
    "section": "Checking Installed Packages",
    "text": "Checking Installed Packages\nTo see what packages are installed on your system:\n\n# List all installed packages\ninstalled.packages()[, c(\"Package\", \"Version\")]\n\n# Check if a specific package is installed\n\"dplyr\" %in% rownames(installed.packages())"
  },
  {
    "objectID": "get-started/r-package-management.html#updating-packages",
    "href": "get-started/r-package-management.html#updating-packages",
    "title": "Installing and Managing R Packages",
    "section": "Updating Packages",
    "text": "Updating Packages\nKeeping packages up-to-date ensures you have the latest features and bug fixes:\n\n# Update all packages\nupdate.packages()\n\n# Update without asking for confirmation\nupdate.packages(ask = FALSE)"
  },
  {
    "objectID": "get-started/r-package-management.html#installing-from-github",
    "href": "get-started/r-package-management.html#installing-from-github",
    "title": "Installing and Managing R Packages",
    "section": "Installing from GitHub",
    "text": "Installing from GitHub\nMany cutting-edge packages are available on GitHub before they reach CRAN:\n\n# First, install the devtools package if you haven't already\ninstall.packages(\"devtools\")\n\n# Then use it to install packages from GitHub\nlibrary(devtools)\ninstall_github(\"tidyverse/ggplot2\")"
  },
  {
    "objectID": "get-started/r-package-management.html#package-dependencies",
    "href": "get-started/r-package-management.html#package-dependencies",
    "title": "Installing and Managing R Packages",
    "section": "Package Dependencies",
    "text": "Package Dependencies\nR automatically handles dependencies (other packages required by your target package). However, sometimes you may encounter issues with dependencies that require manual intervention:\n\n# Force reinstallation of a package and its dependencies\ninstall.packages(\"problematic_package\", dependencies = TRUE)"
  },
  {
    "objectID": "get-started/r-package-management.html#creating-a-reproducible-environment",
    "href": "get-started/r-package-management.html#creating-a-reproducible-environment",
    "title": "Installing and Managing R Packages",
    "section": "Creating a Reproducible Environment",
    "text": "Creating a Reproducible Environment\nFor collaborative or production work, it’s important to track package versions:\n\n# Record packages and versions with renv\ninstall.packages(\"renv\")\nlibrary(renv)\nrenv::init()      # Initialize a project environment\nrenv::snapshot()  # Save the current state of packages\n\nThe renv package creates isolated, reproducible environments similar to Python’s virtual environments."
  },
  {
    "objectID": "get-started/r-package-management.html#managing-package-conflicts",
    "href": "get-started/r-package-management.html#managing-package-conflicts",
    "title": "Installing and Managing R Packages",
    "section": "Managing Package Conflicts",
    "text": "Managing Package Conflicts\nSometimes packages have functions with the same name, causing conflicts:\n\n# Specify the package explicitly\ndplyr::filter(df, x &gt; 10)  # Use filter from dplyr\nstats::filter(x, rep(1/3, 3))  # Use filter from stats"
  },
  {
    "objectID": "get-started/r-package-management.html#pro-tip-package-installation-script",
    "href": "get-started/r-package-management.html#pro-tip-package-installation-script",
    "title": "Installing and Managing R Packages",
    "section": "Pro Tip: Package Installation Script",
    "text": "Pro Tip: Package Installation Script\nFor projects requiring multiple packages, create an installation script:\n\n# Create a function to check and install packages\ninstall_if_missing &lt;- function(pkg) {\n  if (!require(pkg, character.only = TRUE)) {\n    install.packages(pkg)\n    library(pkg, character.only = TRUE)\n  }\n}\n\n# List all required packages\npackages &lt;- c(\"tidyverse\", \"data.table\", \"caret\", \"lubridate\", \"janitor\")\n\n# Install all packages\ninvisible(sapply(packages, install_if_missing))\n\nThis script installs packages only if they’re not already available, saving time when setting up on a new machine or sharing code with collaborators."
  },
  {
    "objectID": "get-started/variables.html",
    "href": "get-started/variables.html",
    "title": "Variables in R",
    "section": "",
    "text": "Variables in R are used to store data that can be referenced and manipulated throughout your code. Here’s how to create and work with variables:\n\n\n\n# Using the assignment operator (&lt;-)\nx &lt;- 10\ny &lt;- \"Hello, R!\"\nz &lt;- TRUE\n\n# Print the variables\nx\n\n[1] 10\n\ny\n\n[1] \"Hello, R!\"\n\nz\n\n[1] TRUE\n\n\n\n\n\n\n# Using the equals sign (=)\nage = 25\n\n# Using the assignment operator in reverse (-&gt;)\n\"Data Scientist\" -&gt; job_title\n\n# Print the variables\nage\n\n[1] 25\n\njob_title\n\n[1] \"Data Scientist\"\n\n\n\n\n\n\nNames can contain letters, numbers, dots (.) and underscores (_)\nNames must start with a letter or a dot\nIf a name starts with a dot, it cannot be followed by a number\nNames are case-sensitive (Value and value are different variables)\n\n\n# Valid variable names\nvalid_name &lt;- 1\nvalidName &lt;- 2\nvalid.name &lt;- 3\n.hidden &lt;- 4\n\n# Print variables\nvalid_name\n\n[1] 1\n\nvalidName\n\n[1] 2\n\nvalid.name\n\n[1] 3\n\n.hidden\n\n[1] 4\n\n\n\n\n\nR has several basic data types:\n\n# Numeric\nnum &lt;- 42.5\ntypeof(num)\n\n[1] \"double\"\n\n# Integer (note the L suffix)\nint &lt;- 42L\ntypeof(int)\n\n[1] \"integer\"\n\n# Character\ntext &lt;- \"R programming\"\ntypeof(text)\n\n[1] \"character\"\n\n# Logical\nflag &lt;- TRUE\ntypeof(flag)\n\n[1] \"logical\"\n\n\n\n\n\n\n# Check if a variable is of a specific type\nis.numeric(num)\n\n[1] TRUE\n\nis.character(text)\n\n[1] TRUE\n\n# Convert between types\nas.character(num)\n\n[1] \"42.5\"\n\nas.numeric(\"100\")\n\n[1] 100\n\nas.logical(1)\n\n[1] TRUE\n\n\n\n\n\n\n# Get information about a variable\nx &lt;- c(1, 2, 3, 4, 5)\nclass(x)\n\n[1] \"numeric\"\n\nlength(x)\n\n[1] 5\n\nstr(x)\n\n num [1:5] 1 2 3 4 5\n\n\nRemember that R is dynamically typed, so variables can change types during execution. This flexibility is one of R’s strengths for data analysis."
  },
  {
    "objectID": "get-started/variables.html#creating-variables-in-r",
    "href": "get-started/variables.html#creating-variables-in-r",
    "title": "Variables in R",
    "section": "",
    "text": "Variables in R are used to store data that can be referenced and manipulated throughout your code. Here’s how to create and work with variables:\n\n\n\n# Using the assignment operator (&lt;-)\nx &lt;- 10\ny &lt;- \"Hello, R!\"\nz &lt;- TRUE\n\n# Print the variables\nx\n\n[1] 10\n\ny\n\n[1] \"Hello, R!\"\n\nz\n\n[1] TRUE\n\n\n\n\n\n\n# Using the equals sign (=)\nage = 25\n\n# Using the assignment operator in reverse (-&gt;)\n\"Data Scientist\" -&gt; job_title\n\n# Print the variables\nage\n\n[1] 25\n\njob_title\n\n[1] \"Data Scientist\"\n\n\n\n\n\n\nNames can contain letters, numbers, dots (.) and underscores (_)\nNames must start with a letter or a dot\nIf a name starts with a dot, it cannot be followed by a number\nNames are case-sensitive (Value and value are different variables)\n\n\n# Valid variable names\nvalid_name &lt;- 1\nvalidName &lt;- 2\nvalid.name &lt;- 3\n.hidden &lt;- 4\n\n# Print variables\nvalid_name\n\n[1] 1\n\nvalidName\n\n[1] 2\n\nvalid.name\n\n[1] 3\n\n.hidden\n\n[1] 4\n\n\n\n\n\nR has several basic data types:\n\n# Numeric\nnum &lt;- 42.5\ntypeof(num)\n\n[1] \"double\"\n\n# Integer (note the L suffix)\nint &lt;- 42L\ntypeof(int)\n\n[1] \"integer\"\n\n# Character\ntext &lt;- \"R programming\"\ntypeof(text)\n\n[1] \"character\"\n\n# Logical\nflag &lt;- TRUE\ntypeof(flag)\n\n[1] \"logical\"\n\n\n\n\n\n\n# Check if a variable is of a specific type\nis.numeric(num)\n\n[1] TRUE\n\nis.character(text)\n\n[1] TRUE\n\n# Convert between types\nas.character(num)\n\n[1] \"42.5\"\n\nas.numeric(\"100\")\n\n[1] 100\n\nas.logical(1)\n\n[1] TRUE\n\n\n\n\n\n\n# Get information about a variable\nx &lt;- c(1, 2, 3, 4, 5)\nclass(x)\n\n[1] \"numeric\"\n\nlength(x)\n\n[1] 5\n\nstr(x)\n\n num [1:5] 1 2 3 4 5\n\n\nRemember that R is dynamically typed, so variables can change types during execution. This flexibility is one of R’s strengths for data analysis."
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installing R and RStudio",
    "section": "",
    "text": "This guide will walk you through the process of installing R and RStudio on your computer. These are the essential tools you’ll need to start your journey with R programming.\n\n\nR is a free, open-source programming language and software environment designed for statistical computing and graphics. It’s widely used among statisticians, data scientists, and researchers for data analysis and visualization.\n\n\n\nRStudio is an integrated development environment (IDE) for R. It makes working with R much easier by providing a user-friendly interface with features like syntax highlighting, code completion, and visualization tools."
  },
  {
    "objectID": "installation.html#getting-started-with-r",
    "href": "installation.html#getting-started-with-r",
    "title": "Installing R and RStudio",
    "section": "",
    "text": "This guide will walk you through the process of installing R and RStudio on your computer. These are the essential tools you’ll need to start your journey with R programming.\n\n\nR is a free, open-source programming language and software environment designed for statistical computing and graphics. It’s widely used among statisticians, data scientists, and researchers for data analysis and visualization.\n\n\n\nRStudio is an integrated development environment (IDE) for R. It makes working with R much easier by providing a user-friendly interface with features like syntax highlighting, code completion, and visualization tools."
  },
  {
    "objectID": "installation.html#installation-guide",
    "href": "installation.html#installation-guide",
    "title": "Installing R and RStudio",
    "section": "Installation Guide",
    "text": "Installation Guide\n\nInstalling R\n\nWindows\n\nGo to the CRAN (Comprehensive R Archive Network) website\nClick on “Download R for Windows”\nClick on “base”\nClick on the download link for the latest version (e.g., “Download R-4.x.x for Windows”)\nRun the downloaded installer and follow the installation prompts\n\nAccept the default settings unless you have specific preferences\nNote the installation location in case you need it later\n\n\n\n\nmacOS\n\nGo to the CRAN website\nClick on “Download R for macOS”\nDownload the latest .pkg file for your macOS version\nOpen the downloaded file and follow the installation instructions\n\n\n\nLinux (Ubuntu/Debian)\n\nOpen a terminal window\nUpdate your system’s package index:\nsudo apt update\nInstall R:\nsudo apt install r-base\n\n\n\n\nInstalling RStudio\nAfter installing R, you should install RStudio:\n\nGo to the RStudio download page\nScroll down to find the installer for your operating system\nDownload the appropriate installer\nRun the installer and follow the installation prompts"
  },
  {
    "objectID": "installation.html#verifying-your-installation",
    "href": "installation.html#verifying-your-installation",
    "title": "Installing R and RStudio",
    "section": "Verifying Your Installation",
    "text": "Verifying Your Installation\nTo verify that R and RStudio are installed correctly:\n\nOpen RStudio\nIn the Console pane (usually at the bottom left), type:\nR.version\nPress Enter. You should see information about your R installation."
  },
  {
    "objectID": "installation.html#installing-r-packages",
    "href": "installation.html#installing-r-packages",
    "title": "Installing R and RStudio",
    "section": "Installing R Packages",
    "text": "Installing R Packages\nR’s functionality can be extended with packages. Here’s how to install a package:\n\nIn RStudio, go to the Console\nType the following command to install a package (replace “packagename” with the actual package name):\ninstall.packages(\"packagename\")\nFor example, to install the tidyverse collection of packages:\ninstall.packages(\"tidyverse\")\n\n\nEssential Packages for Beginners\nConsider installing these useful packages to get started:\n# Run these commands in the RStudio console\ninstall.packages(\"tidyverse\")  # Data manipulation and visualization\ninstall.packages(\"rmarkdown\")  # For creating dynamic documents\ninstall.packages(\"knitr\")      # For report generation\ninstall.packages(\"shiny\")      # For interactive web applications"
  },
  {
    "objectID": "installation.html#troubleshooting",
    "href": "installation.html#troubleshooting",
    "title": "Installing R and RStudio",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nCommon Issues on Windows\n\nPermission errors: Run RStudio as administrator\nPath too long errors: Install R in a directory with a shorter path\n\n\n\nCommon Issues on macOS\n\nPackage installation failures: Make sure you have the necessary development tools installed:\nxcode-select --install\n\n\n\nCommon Issues on Linux\n\nMissing dependencies: Install common R dependencies:\nsudo apt install libcurl4-openssl-dev libssl-dev libxml2-dev"
  },
  {
    "objectID": "installation.html#next-steps",
    "href": "installation.html#next-steps",
    "title": "Installing R and RStudio",
    "section": "Next Steps",
    "text": "Next Steps\nNow that you have R and RStudio installed, you’re ready to start your R programming journey! Check out our Introduction to Analytics with R post to begin learning."
  },
  {
    "objectID": "posts/bayesian-optimization-xgboost.html",
    "href": "posts/bayesian-optimization-xgboost.html",
    "title": "Using Bayesian Optimization to Tune XGBoost Models in R",
    "section": "",
    "text": "Tuning machine learning models can be time-consuming and computationally expensive. This post shows how to use Bayesian optimization to efficiently find optimal XGBoost hyperparameters – saving time and improving model performance."
  },
  {
    "objectID": "posts/bayesian-optimization-xgboost.html#required-packages",
    "href": "posts/bayesian-optimization-xgboost.html#required-packages",
    "title": "Using Bayesian Optimization to Tune XGBoost Models in R",
    "section": "Required Packages",
    "text": "Required Packages\n\n# Load required packages\nlibrary(xgboost)\nlibrary(ParBayesianOptimization)\nlibrary(mlbench)\nlibrary(dplyr)\nlibrary(recipes)\nlibrary(rsample)"
  },
  {
    "objectID": "posts/bayesian-optimization-xgboost.html#data-preparation",
    "href": "posts/bayesian-optimization-xgboost.html#data-preparation",
    "title": "Using Bayesian Optimization to Tune XGBoost Models in R",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe’ll use the Boston Housing dataset – a classic regression problem with both numeric and categorical variables.\n\n# Load the Boston Housing dataset\ndata(\"BostonHousing2\")\n\n# Quick look at the data structure\nstr(BostonHousing2)\n\n'data.frame':   506 obs. of  19 variables:\n $ town   : Factor w/ 92 levels \"Arlington\",\"Ashland\",..: 54 77 77 46 46 46 69 69 69 69 ...\n $ tract  : int  2011 2021 2022 2031 2032 2033 2041 2042 2043 2044 ...\n $ lon    : num  -71 -71 -70.9 -70.9 -70.9 ...\n $ lat    : num  42.3 42.3 42.3 42.3 42.3 ...\n $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n $ cmedv  : num  24 21.6 34.7 33.4 36.2 28.7 22.9 22.1 16.5 18.9 ...\n $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n $ chas   : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n $ rm     : num  6.58 6.42 7.18 7 7.15 ...\n $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...\n $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...\n $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...\n $ tax    : int  296 242 242 222 222 222 311 311 311 311 ...\n $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n $ b      : num  397 397 393 395 397 ...\n $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...\n\n\nXGBoost requires numeric inputs, so we’ll use the recipes package to transform our categorical variables:\n\n# Create a recipe for preprocessing\nrec &lt;- recipe(cmedv ~ ., data = BostonHousing2) %&gt;%\n  # Collapse categories where population is &lt; 3%\n  step_other(town, chas, threshold = .03, other = \"Other\") %&gt;% \n  # Create dummy variables for all factor variables \n  step_dummy(all_nominal_predictors())\n\n# Train the recipe on the dataset\nprep &lt;- prep(rec, training = BostonHousing2)\n\n# Create the final model matrix\nmodel_df &lt;- bake(prep, new_data = BostonHousing2)\n\n# Check the column names after one-hot encoding\ncolnames(model_df)\n\n [1] \"tract\"                  \"lon\"                    \"lat\"                   \n [4] \"medv\"                   \"crim\"                   \"zn\"                    \n [7] \"indus\"                  \"nox\"                    \"rm\"                    \n[10] \"age\"                    \"dis\"                    \"rad\"                   \n[13] \"tax\"                    \"ptratio\"                \"b\"                     \n[16] \"lstat\"                  \"cmedv\"                  \"town_Boston.Savin.Hill\"\n[19] \"town_Cambridge\"         \"town_Lynn\"              \"town_Newton\"           \n[22] \"town_Other\"             \"chas_X1\"               \n\n\nNext, we’ll split our data into training and testing sets:\n\n# Create a 70/30 train-test split\nsplits &lt;- rsample::initial_split(model_df, prop = 0.7)\ntrain_df &lt;- rsample::training(splits)\ntest_df &lt;- rsample::testing(splits)\n\n# Prepare the training data for XGBoost\nX &lt;- train_df %&gt;%\n  select(!medv, !cmedv) %&gt;%\n  as.matrix()\n\n# Get the target variable\ny &lt;- train_df %&gt;% pull(cmedv)\n\n# Create cross-validation folds\nfolds &lt;- list(\n  fold1 = as.integer(seq(1, nrow(X), by = 5)),\n  fold2 = as.integer(seq(2, nrow(X), by = 5))\n)"
  },
  {
    "objectID": "posts/bayesian-optimization-xgboost.html#setting-up-bayesian-optimization",
    "href": "posts/bayesian-optimization-xgboost.html#setting-up-bayesian-optimization",
    "title": "Using Bayesian Optimization to Tune XGBoost Models in R",
    "section": "Setting Up Bayesian Optimization",
    "text": "Setting Up Bayesian Optimization\nBayesian optimization requires two key components:\n\nAn objective function that evaluates model performance\nThe parameter bounds we want to explore\n\n\n# Our objective function takes hyperparameters as inputs\nobj_func &lt;- function(eta, max_depth, min_child_weight, subsample, lambda, alpha) {\n  \n  param &lt;- list(\n    # Learning parameters\n    eta = eta,                       # Learning rate\n    max_depth = max_depth,           # Tree depth\n    min_child_weight = min_child_weight, # Min observations per node\n    subsample = subsample,           # Data subsampling\n    lambda = lambda,                 # L2 regularization\n    alpha = alpha,                   # L1 regularization\n    \n    booster = \"gbtree\",             # Use tree model\n    objective = \"reg:squarederror\",  # Regression task\n    eval_metric = \"mape\"            # Mean Absolute Percentage Error\n  )\n  \n  xgbcv &lt;- xgb.cv(params = param,\n                  data = X,\n                  label = y,\n                  nround = 50,\n                  folds = folds,\n                  prediction = TRUE,\n                  early_stopping_rounds = 5,\n                  verbose = 0,\n                  maximize = FALSE)\n  \n  lst &lt;- list(\n    # First argument must be named as \"Score\"\n    # Function finds maxima so inverting the output\n    Score = -min(xgbcv$evaluation_log$test_mape_mean),\n    \n    # Get number of trees for the best performing model\n    nrounds = xgbcv$best_iteration\n  )\n  \n  return(lst)\n}\n\n# Define the search space for each parameter\nbounds &lt;- list(\n  eta = c(0.001, 0.2),             # Learning rate range\n  max_depth = c(1L, 10L),           # Tree depth range\n  min_child_weight = c(1, 50),      # Min observations range\n  subsample = c(0.1, 1),            # Subsampling range\n  lambda = c(1, 10),                # L2 regularization range\n  alpha = c(1, 10)                  # L1 regularization range\n)"
  },
  {
    "objectID": "posts/bayesian-optimization-xgboost.html#running-bayesian-optimization",
    "href": "posts/bayesian-optimization-xgboost.html#running-bayesian-optimization",
    "title": "Using Bayesian Optimization to Tune XGBoost Models in R",
    "section": "Running Bayesian Optimization",
    "text": "Running Bayesian Optimization\nNow we’ll run the optimization process to intelligently search for the best parameters:\n\nset.seed(1234)\nbayes_out &lt;- bayesOpt(\n  FUN = obj_func,                    # Our objective function\n  bounds = bounds,                   # Parameter bounds\n  initPoints = length(bounds) + 2,   # Initial random points\n  iters.n = 10,                      # Number of iterations\n  verbose = 0                        # Suppress output\n)\n\n# View top results\nbayes_out$scoreSummary[1:5, c(3:8, 13)]\n\n          eta max_depth min_child_weight subsample   lambda    alpha      Score\n        &lt;num&gt;     &lt;num&gt;            &lt;num&gt;     &lt;num&gt;    &lt;num&gt;    &lt;num&gt;      &lt;num&gt;\n1: 0.13392137         8         4.913332 0.2105925 4.721124 3.887629 -0.1292920\n2: 0.19400811         2        25.454160 0.9594105 9.329695 3.173695 -0.1790158\n3: 0.16079775         2        14.035652 0.5118349 1.229953 5.093530 -0.1662595\n4: 0.08957707         4        12.534842 0.3844404 4.358837 1.788342 -0.1672395\n5: 0.02876388         4        36.586761 0.8107181 6.137100 6.039125 -0.3320015\n\n# Get the best parameters\nbest_params &lt;- getBestPars(bayes_out)\ndata.frame(best_params)\n\n        eta max_depth min_child_weight subsample lambda    alpha\n1 0.1251447        10                1         1      1 5.905011"
  },
  {
    "objectID": "posts/bayesian-optimization-xgboost.html#training-the-final-model",
    "href": "posts/bayesian-optimization-xgboost.html#training-the-final-model",
    "title": "Using Bayesian Optimization to Tune XGBoost Models in R",
    "section": "Training the Final Model",
    "text": "Training the Final Model\nWith the optimal hyperparameters identified, we can now train our final XGBoost model.\n\n# Combine best params with base params\nopt_params &lt;- append(\n  list(booster = \"gbtree\", \n       objective = \"reg:squarederror\", \n       eval_metric = \"mae\"), \n  best_params\n)\n\n# Run cross-validation to determine optimal number of rounds\nxgbcv &lt;- xgb.cv(\n  params = opt_params,\n  data = X,\n  label = y,\n  nround = 100,\n  folds = folds,\n  prediction = TRUE,\n  early_stopping_rounds = 5,\n  verbose = 0,\n  maximize = FALSE\n)\n\n# Get optimal number of rounds\nnrounds = xgbcv$best_iteration\n\n# Fit the final XGBoost model\nmdl &lt;- xgboost(\n  data = X, \n  label = y, \n  params = opt_params, \n  maximize = FALSE, \n  early_stopping_rounds = 5, \n  nrounds = nrounds, \n  verbose = 0\n)\n\n# Make predictions on the test set\nactuals &lt;- test_df$cmedv\npredicted &lt;- test_df %&gt;%\n  select_at(mdl$feature_names) %&gt;%\n  as.matrix() %&gt;%\n  predict(mdl, newdata = .)\n\n# Evaluate performance using Mean Absolute Percentage Error (MAPE)\nmape &lt;- mean(abs(actuals - predicted)/actuals)\ncat(\"MAPE on test set:\", mape)\n\nMAPE on test set: 0.006424492"
  },
  {
    "objectID": "posts/bayesian-optimization-xgboost.html#why-bayesian-optimization",
    "href": "posts/bayesian-optimization-xgboost.html#why-bayesian-optimization",
    "title": "Using Bayesian Optimization to Tune XGBoost Models in R",
    "section": "Why Bayesian Optimization",
    "text": "Why Bayesian Optimization\nBayesian optimization offers several key advantages over traditional grid search:\n\nEfficiency: Finds optimal parameters in fewer iterations\nIntelligence: Learns from previous evaluations to focus on promising areas\nScalability: Remains efficient even with many hyperparameters\nSpeed: Completes in a fraction of the time while achieving comparable or better results\n\nThis approach becomes increasingly valuable as model complexity grows. For production models, consider increasing the iterations (iters.n) to ensure thorough exploration of the parameter space.\nThe ParBayesianOptimization package makes this powerful technique accessible to R users, allowing you to build better models with less computational overhead."
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html",
    "href": "posts/custom-charting-functions-ggplot2.html",
    "title": "Custom Charting Functions Using ggplot2",
    "section": "",
    "text": "While R has a variety of options for 2D graphics and data visualization, it’s hard to beat ggplot2 in terms of features, functionality, and overall visual quality. This post demonstrates how to create customized charting functions for specific chart types using ggplot2 as the underlying visualization engine."
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#required-libraries",
    "href": "posts/custom-charting-functions-ggplot2.html#required-libraries",
    "title": "Custom Charting Functions Using ggplot2",
    "section": "Required Libraries",
    "text": "Required Libraries\n\n# Load required packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(stringr)"
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#sample-dataset",
    "href": "posts/custom-charting-functions-ggplot2.html#sample-dataset",
    "title": "Custom Charting Functions Using ggplot2",
    "section": "Sample Dataset",
    "text": "Sample Dataset\nFor this demonstration, we’ll use a summarized version of the COVID-19 Data Repository hosted by Johns Hopkins University.\n\n# Load COVID-19 data\ndf &lt;- read.csv(\"https://bit.ly/3G8G63u\")\n\n# Get top 5 countries by death count\ntop_countries &lt;- df %&gt;% \n  group_by(country) %&gt;% \n  summarise(count = sum(deaths_daily)) %&gt;% \n  top_n(5) %&gt;% \n  .$country\n\nprint(top_countries)\n\n[1] \"Brazil\" \"India\"  \"Mexico\" \"Russia\" \"US\"    \n\n\nLet’s prepare our data for visualization by creating a 7-day moving average of daily confirmed cases for the top five countries:\n\n# Create a data frame with the required information\n# Note that a centered 7-day moving average is used\nplotdf &lt;- df %&gt;% \n  mutate(date = as.Date(date, format = \"%m/%d/%Y\")) %&gt;% \n  filter(country %in% top_countries) %&gt;% \n  group_by(country, date) %&gt;% \n  summarise(count = sum(confirmed_daily)) %&gt;%\n  arrange(country, date) %&gt;% \n  group_by(country) %&gt;% \n  mutate(MA = zoo::rollapply(count, FUN = mean, width = 7, by = 1, fill = NA, align = \"center\"))"
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#building-a-simple-line-chart-function",
    "href": "posts/custom-charting-functions-ggplot2.html#building-a-simple-line-chart-function",
    "title": "Custom Charting Functions Using ggplot2",
    "section": "Building a Simple Line Chart Function",
    "text": "Building a Simple Line Chart Function\nLet’s start by creating a basic line chart function. Note the use of aes_string() instead of just aes(). This allows us to supply arguments to ggplot2 as strings, making our function more flexible.\n\n# Function definition\nline_chart &lt;- function(df, \n                       x, \n                       y, \n                       group_color = NULL, \n                       line_width = 1, \n                       line_type = 1){\n  \n  ggplot(df, aes(x = !! sym(x), \n                 y = !! sym(y), \n                 color = !! sym(group_color))) + \n    geom_line(linewidth = line_width, \n              linetype = line_type)\n}\n\n# Test run\nline_chart(plotdf,\n           x = \"date\",\n           y = \"MA\",\n           group_color = \"country\", \n           line_type = 1, \n           line_width = 1.2)"
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#creating-a-custom-theme",
    "href": "posts/custom-charting-functions-ggplot2.html#creating-a-custom-theme",
    "title": "Custom Charting Functions Using ggplot2",
    "section": "Creating a Custom Theme",
    "text": "Creating a Custom Theme\nNow that we know how to encapsulate the call to ggplot2 in a more intuitive manner, we can create a customized theme for our charts. This is useful since this theme can be applied to any chart.\n\ncustom_theme &lt;- function(plt, \n                         base_size = 11, \n                         base_line_size = 1, \n                         palette = \"Set1\"){\n  \n  # Note the use of \"+\" and not \"%&gt;%\"\n  plt + \n    # Adjust overall font size\n    theme_minimal(base_size = base_size, \n                  base_line_size = base_line_size) + \n    \n    # Put legend at the bottom\n    theme(legend.position = \"bottom\") + \n    \n    # Different colour scale\n    scale_color_brewer(palette = palette)\n}\n\n# Test run\nline_chart(plotdf, \"date\", \"MA\", \"country\") %&gt;% custom_theme()"
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#enhancing-our-functions",
    "href": "posts/custom-charting-functions-ggplot2.html#enhancing-our-functions",
    "title": "Custom Charting Functions Using ggplot2",
    "section": "Enhancing Our Functions",
    "text": "Enhancing Our Functions\nLet’s add more features to our line_chart() function to make it more versatile:\n\nline_chart &lt;- function(df, \n                       x, y, \n                       group_color = NULL, \n                       line_width = 1, \n                       line_type = 1, \n                       xlab = NULL, \n                       ylab = NULL, \n                       title = NULL, \n                       subtitle = NULL, \n                       caption = NULL){\n  # Base plot\n  ggplot(df, aes(x = !! sym(x), \n                 y = !! sym(y), \n                 color = !! sym(group_color))) + \n    \n    # Line chart \n    geom_line(size = line_width, \n              linetype = line_type) + \n    \n    # Titles and subtitles\n    labs(x = xlab, \n         y = ylab, \n         title = title, \n         subtitle = subtitle, \n         caption = caption)\n}\n\nWe’ll also enhance our custom_theme() function to handle different axis formatting options:\n\ncustom_theme &lt;- function(plt, \n                         palette = \"Set1\", \n                         format_x_axis_as = NULL, \n                         format_y_axis_as = NULL, \n                         x_axis_scale = 1, \n                         y_axis_scale = 1, \n                         x_axis_text_size = 10, \n                         y_axis_text_size = 10, \n                         base_size = 11, \n                         base_line_size = 1, \n                         x_angle = 45){\n  \n  mappings &lt;- names(unlist(plt$mapping))\n  \n  p &lt;- plt + \n    \n    # Adjust overall font size\n    theme_minimal(base_size = base_size, \n                  base_line_size = base_line_size) + \n    \n    # Put legend at the bottom\n    theme(legend.position = \"bottom\", \n          axis.text.x = element_text(angle = x_angle)) + \n    \n    # Different colour palette\n    {if(\"colour\" %in% mappings) scale_color_brewer(palette = palette)}+\n    \n    {if(\"fill\" %in% mappings) scale_fill_brewer(palette = palette)}+\n    \n    # Change some theme options\n    theme(plot.background = element_rect(fill = \"#f7f7f7\"), \n          plot.subtitle = element_text(face = \"italic\"), \n          axis.title.x = element_text(face = \"bold\", \n                                      size = x_axis_text_size), \n          axis.title.y = element_text(face = \"bold\", \n                                      size = y_axis_text_size)) + \n    \n    # Change x-axis formatting\n    {if(!is.null(format_x_axis_as))\n      switch(format_x_axis_as, \n             \"date\" = scale_x_date(breaks = pretty_breaks(n = 12)), \n             \"number\" = scale_x_continuous(labels = number_format(accuracy = 0.1, \n                                                                  decimal.mark = \",\", \n                                                                  scale = x_axis_scale)), \n             \"percent\" = scale_x_continuous(labels = percent))} + \n    \n    # Change y-axis formatting\n    {if(!is.null(format_y_axis_as))\n      \n      switch(format_y_axis_as, \n             \"date\" = scale_y_date(breaks = pretty_breaks(n = 12)), \n             \"number\" = scale_y_continuous(labels = number_format(accuracy = 0.1, \n                                                                  decimal.mark = \",\", \n                                                                  scale = y_axis_scale)), \n             \"percent\" = scale_y_continuous(labels = percent))}\n  \n  # Capitalise all names\n  vec &lt;- lapply(p$labels, str_to_title)\n  names(vec) &lt;- names(p$labels)\n  p$labels &lt;- vec\n  \n  return(p)\n}"
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#putting-it-all-together",
    "href": "posts/custom-charting-functions-ggplot2.html#putting-it-all-together",
    "title": "Custom Charting Functions Using ggplot2",
    "section": "Putting It All Together",
    "text": "Putting It All Together\nNow let’s see how our enhanced functions work together to create a polished visualization:\n\nline_chart(plotdf,\n           x = \"date\", \n           y = \"MA\", \n           group_color = \"country\", \n           xlab = \"Date\", \n           ylab = \"Moving Avg. (in '000)\", \n           title = \"Daily COVID19 Case Load\", \n           subtitle = \"Top 5 countries by volume\") %&gt;% \n  \n  custom_theme(format_x_axis_as = \"date\", \n               format_y_axis_as = \"number\", \n               y_axis_scale = 0.001)"
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#applying-the-custom-theme-to-other-chart-types",
    "href": "posts/custom-charting-functions-ggplot2.html#applying-the-custom-theme-to-other-chart-types",
    "title": "Custom Charting Functions Using ggplot2",
    "section": "Applying the Custom Theme to Other Chart Types",
    "text": "Applying the Custom Theme to Other Chart Types\nThe beauty of our custom_theme() function is that it can be applied to any ggplot2 object. Let’s create a bar chart to demonstrate this flexibility:\n\np &lt;- plotdf %&gt;%  \n  mutate(month = format(date, \"%m-%b\")) %&gt;% \n  ggplot(aes(x = month, y = MA, fill = country)) + \n  geom_col(position = \"dodge\") + \n  labs(title = \"Monthly COVID19 Case load trend\", \n       subtitle = \"Top 5 countries\", \n       x = \"Month\", \n       y = \"Moving Average ('000)\")\n\ncustom_theme(p, \n             palette = \"Set2\", \n             format_y_axis_as = \"number\", \n             y_axis_scale = 0.001)"
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#benefits-of-custom-charting-functions",
    "href": "posts/custom-charting-functions-ggplot2.html#benefits-of-custom-charting-functions",
    "title": "Custom Charting Functions Using ggplot2",
    "section": "Benefits of Custom Charting Functions",
    "text": "Benefits of Custom Charting Functions\nCreating custom charting functions with ggplot2 offers several advantages:\n\nConsistency: Ensures all charts in your reports or dashboards have a consistent look and feel.\nEfficiency: Reduces the amount of code you need to write for commonly used chart types.\nMaintainability: Makes it easier to update the style of all charts by modifying a single function.\nSimplicity: Abstracts away the complexity of ggplot2 for team members who may not be as familiar with the package."
  },
  {
    "objectID": "posts/custom-charting-functions-ggplot2.html#when-to-use-custom-functions-vs.-direct-ggplot2",
    "href": "posts/custom-charting-functions-ggplot2.html#when-to-use-custom-functions-vs.-direct-ggplot2",
    "title": "Custom Charting Functions Using ggplot2",
    "section": "When to Use Custom Functions vs. Direct ggplot2",
    "text": "When to Use Custom Functions vs. Direct ggplot2\nIt’s worth noting that building customized charting functions using ggplot2 is most useful when you need to create the same type of chart(s) repeatedly. When doing exploratory work, using ggplot2 directly is often easier and more flexible since you can build all kinds of charts (or layer different chart types) within the same pipeline."
  },
  {
    "objectID": "posts/getting-started-with-reticulate.html",
    "href": "posts/getting-started-with-reticulate.html",
    "title": "Getting Started with Python using R and reticulate",
    "section": "",
    "text": "Want to use Python’s powerful libraries without leaving R? The reticulate package gives you the best of both worlds - R’s elegant data handling and visualization with Python’s machine learning and scientific computing tools. This post shows you how to set up and use this powerful bridge between languages."
  },
  {
    "objectID": "posts/getting-started-with-reticulate.html#quick-setup-in-4-steps",
    "href": "posts/getting-started-with-reticulate.html#quick-setup-in-4-steps",
    "title": "Getting Started with Python using R and reticulate",
    "section": "Quick Setup in 4 Steps",
    "text": "Quick Setup in 4 Steps\n\n1. Install reticulate\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\n\n\n\n2. Install Python via Miniconda\nThe easiest approach is to let reticulate handle Python installation for you:\n\ninstall_miniconda(path = \"c:/miniconda\")\n\n\n\n3. Connect to Python\nReticulate creates a default environment called r-reticulate. Let’s connect to it:\n\n# Check available environments\nconda_list()\n\n# Connect to the default environment\nuse_condaenv(\"r-reticulate\")\n\n\n\n4. Install Python Packages\nNow you can install any Python packages you need:\n\npy_install(c(\"pandas\", \"scikit-learn\", \"matplotlib\"))"
  },
  {
    "objectID": "posts/getting-started-with-reticulate.html#three-ways-to-use-python-in-r",
    "href": "posts/getting-started-with-reticulate.html#three-ways-to-use-python-in-r",
    "title": "Getting Started with Python using R and reticulate",
    "section": "Three Ways to Use Python in R",
    "text": "Three Ways to Use Python in R\n\n1. Import Python Modules Directly\n\n# Import pandas and use it like any R package\npd &lt;- import(\"pandas\")\n\n# Create a pandas Series\npd$Series(c(1, 2, 3, 4, 5))\n\n# Import numpy for numerical operations\nnp &lt;- import(\"numpy\")\nnp$mean(c(1:100))  # Calculate mean using numpy\n\n\n\n2. Write Python Code in R Markdown\nYou can mix R and Python code in the same document by using Python code chunks:\n\n# This is Python code!\nimport pandas as pd\nimport numpy as np\n\n# Create a simple DataFrame\ndf = pd.DataFrame({\n    'A': np.random.randn(5),\n    'B': np.random.randn(5)\n})\n\nprint(df.describe())\n\n\n\n3. Use Python Libraries in R Workflows\nThe most powerful approach is using Python’s machine learning libraries within R:\n\n# Import scikit-learn\nsk &lt;- import(\"sklearn.linear_model\")\n\n# Create and fit a linear regression model\nmodel &lt;- sk$LinearRegression()\nmodel$fit(X = as.matrix(mtcars[, c(\"disp\", \"hp\", \"wt\")]), \n         y = mtcars$mpg)\n\n# Get predictions and coefficients\npredictions &lt;- model$predict(as.matrix(mtcars[, c(\"disp\", \"hp\", \"wt\")]))\ncoefficients &lt;- data.frame(\n  Feature = c(\"Intercept\", \"disp\", \"hp\", \"wt\"),\n  Coefficient = c(model$intercept_, model$coef_)\n)\n\ncoefficients"
  },
  {
    "objectID": "posts/getting-started-with-reticulate.html#real-world-applications",
    "href": "posts/getting-started-with-reticulate.html#real-world-applications",
    "title": "Getting Started with Python using R and reticulate",
    "section": "Real-World Applications",
    "text": "Real-World Applications\nHere are some powerful ways to combine R and Python in your data science workflow:\n\nData Science Pipeline\n\n# 1. Data cleaning with R's tidyverse\nlibrary(readr)\nclean_data &lt;- read_csv(\"data.csv\") %&gt;%\n  filter(!is.na(important_column)) %&gt;%\n  mutate(new_feature = feature1 / feature2)\n\n# 2. Machine learning with Python's scikit-learn\nsk &lt;- import(\"sklearn.ensemble\")\nmodel &lt;- sk$RandomForestClassifier(n_estimators=100)\nmodel$fit(X = as.matrix(clean_data[, features]), \n         y = clean_data$target)\n\n# 3. Visualization with R's ggplot2\npredictions &lt;- model$predict_proba(as.matrix(clean_data[, features]))[,2]\nclean_data %&gt;%\n  mutate(prediction = predictions) %&gt;%\n  ggplot(aes(x=feature1, y=feature2, color=prediction)) +\n  geom_point() +\n  scale_color_viridis_c()\n\n\n\nWhen to Use Each Language\nUse R for:\n\nData manipulation with dplyr/data.table\nStatistical modeling and hypothesis testing\nPublication-quality visualization\nInteractive reports and dashboards\n\nUse Python for:\n\nDeep learning with TensorFlow/PyTorch\nNatural language processing\nComputer vision\nAdvanced machine learning algorithms\n\nWith reticulate, you don’t have to choose - use the best tool for each part of your analysis!"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html",
    "href": "posts/measuring-model-performance-gains-table.html",
    "title": "Measuring Model Performance Using a Gains Table",
    "section": "",
    "text": "In credit risk modeling, analysts often use a tool called a gains table (or KS table) to measure and quantify the performance of classification models. This post explores how to build and interpret such a table using R.\n\n\nA gains table discretizes the population (typically a test or validation set) into groups based on the model’s output (probability, log odds, or scores). Usually, each group represents 10% of the total population (deciles). The table then presents summary statistics for each group and analyzes the cumulative distributions of events (defaults) and non-events to quantify the model’s performance.\n\n\n\n\n# Load required packages\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(knitr)\nlibrary(scales)\n\n\n\n\nWe’ll use a sample from the Lending Club dataset, which contains information about loans and their outcomes.\n\n# Load the sample data\nsample &lt;- read.csv(\"https://bit.ly/42ypcnJ\")\n\n# Check dimensions\ndim(sample)\n\n[1] 10000   153\n\n\n\n\n\nFirst, we need to create a target (outcome) variable to model. Since this is a credit risk use case, we’ll identify borrowers who defaulted on their payments.\n\n# Check unique loan statuses\nunique(sample$loan_status)\n\n[1] \"Fully Paid\"                                         \n[2] \"Current\"                                            \n[3] \"Charged Off\"                                        \n[4] \"Late (31-120 days)\"                                 \n[5] \"Late (16-30 days)\"                                  \n[6] \"In Grace Period\"                                    \n[7] \"Does not meet the credit policy. Status:Fully Paid\" \n[8] \"Does not meet the credit policy. Status:Charged Off\"\n\n# Define \"bad\" loans as those that are charged off\ncodes &lt;- c(\"Charged Off\", \"Does not meet the credit policy. Status:Charged Off\")\n\n# Create a binary flag for defaults\nsample %&lt;&gt;% mutate(bad_flag = ifelse(loan_status %in% codes, 1, 0))\n\n# Check overall event rates\nsample %&gt;% \n  summarise(events = sum(bad_flag == 1), \n            non_events = sum(bad_flag == 0)) %&gt;% \n  mutate(event_rate = events/(events + non_events))\n\n  events non_events event_rate\n1   1162       8838     0.1162\n\n\n\n\n\nNext, let’s build a quick model, the output of which we’ll use to create the gains table.\n\n# Replace NA values with a default value\nsample[is.na(sample)] &lt;- -1\n\n# Clean the data\nsample %&lt;&gt;% \n  # Remove cases where home ownership and payment plan are not reported\n  filter(!home_ownership %in% c(\"\", \"NONE\"),\n         pymnt_plan != \"\") %&gt;% \n  # Convert categorical variables to factors\n  mutate(home_ownership = factor(home_ownership), \n         pymnt_plan = factor(pymnt_plan))\n\n# Train-test split (70-30)\nidx &lt;- sample(1:nrow(sample), size = 0.7 * nrow(sample), replace = FALSE)\ntrain &lt;- sample[idx,]\ntest &lt;- sample[-idx,]\n\n\n# Build a logistic regression model\nmdl &lt;- glm(\n  formula = bad_flag ~ \n    loan_amnt + term + mths_since_last_delinq + total_pymnt + \n    home_ownership + acc_now_delinq + \n    inq_last_6mths + delinq_amnt + \n    mths_since_last_record + mths_since_recent_revol_delinq + \n    mths_since_last_major_derog + mths_since_recent_inq + \n    mths_since_recent_bc + num_accts_ever_120_pd,\n  family = \"binomial\", \n  data = train\n)\n\n# Generate predictions on the test set\ntest$pred &lt;- predict(mdl, newdata = test)\n\n\n\n\nNow let’s build the gains table step by step:\n\n\n\n# Create deciles based on model predictions\nq &lt;- quantile(test$pred, probs = seq(0, 1, length.out = 11))\n\n# Add bins to test dataset\ntest$bins &lt;- cut(test$pred, breaks = q, include.lowest = TRUE, \n                right = TRUE, ordered_result = TRUE)\n\n# Check the bin levels (note they're in increasing order)\nlevels(test$bins)\n\n [1] \"[-5.37,-3.3]\"  \"(-3.3,-2.9]\"   \"(-2.9,-2.66]\"  \"(-2.66,-2.45]\"\n [5] \"(-2.45,-2.25]\" \"(-2.25,-2.07]\" \"(-2.07,-1.86]\" \"(-1.86,-1.61]\"\n [9] \"(-1.61,-1.23]\" \"(-1.23,1.6]\"  \n\n\n\n\n\n\n# Create initial gains table with counts\ngains_table &lt;- test %&gt;% \n  group_by(bins) %&gt;% \n  summarise(total = n(), \n            events = sum(bad_flag == 1), \n            non_events = sum(bad_flag == 0))\n\n# Add event rate column\ngains_table %&lt;&gt;%\n  mutate(event_rate = percent(events / total, 0.1, 100))\n\n# Display the table\nkable(gains_table)\n\n\n\n\nbins\ntotal\nevents\nnon_events\nevent_rate\n\n\n\n\n[-5.37,-3.3]\n300\n5\n295\n1.7%\n\n\n(-3.3,-2.9]\n300\n8\n292\n2.7%\n\n\n(-2.9,-2.66]\n300\n8\n292\n2.7%\n\n\n(-2.66,-2.45]\n300\n11\n289\n3.7%\n\n\n(-2.45,-2.25]\n300\n22\n278\n7.3%\n\n\n(-2.25,-2.07]\n300\n34\n266\n11.3%\n\n\n(-2.07,-1.86]\n300\n56\n244\n18.7%\n\n\n(-1.86,-1.61]\n300\n65\n235\n21.7%\n\n\n(-1.61,-1.23]\n300\n59\n241\n19.7%\n\n\n(-1.23,1.6]\n300\n75\n225\n25.0%\n\n\n\n\n\n\n\n\n\n# Add population percentage and cumulative distributions\ngains_table %&lt;&gt;%\n  mutate(pop_pct = percent(total/sum(total), 0.1, 100), \n         \n         # Calculate cumulative percentages\n         c.events_pct = cumsum(events) / sum(events),\n         c.non_events_pct = cumsum(non_events) / sum(non_events))\n\n# Display the updated table\nkable(gains_table)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbins\ntotal\nevents\nnon_events\nevent_rate\npop_pct\nc.events_pct\nc.non_events_pct\n\n\n\n\n[-5.37,-3.3]\n300\n5\n295\n1.7%\n10.0%\n0.0145773\n0.1110275\n\n\n(-3.3,-2.9]\n300\n8\n292\n2.7%\n10.0%\n0.0379009\n0.2209259\n\n\n(-2.9,-2.66]\n300\n8\n292\n2.7%\n10.0%\n0.0612245\n0.3308242\n\n\n(-2.66,-2.45]\n300\n11\n289\n3.7%\n10.0%\n0.0932945\n0.4395935\n\n\n(-2.45,-2.25]\n300\n22\n278\n7.3%\n10.0%\n0.1574344\n0.5442228\n\n\n(-2.25,-2.07]\n300\n34\n266\n11.3%\n10.0%\n0.2565598\n0.6443357\n\n\n(-2.07,-1.86]\n300\n56\n244\n18.7%\n10.0%\n0.4198251\n0.7361686\n\n\n(-1.86,-1.61]\n300\n65\n235\n21.7%\n10.0%\n0.6093294\n0.8246142\n\n\n(-1.61,-1.23]\n300\n59\n241\n19.7%\n10.0%\n0.7813411\n0.9153180\n\n\n(-1.23,1.6]\n300\n75\n225\n25.0%\n10.0%\n1.0000000\n1.0000000\n\n\n\n\n\n\n\n\n\n# Add KS statistic, capture rate, and cumulative event rate\ngains_table %&lt;&gt;%\n  mutate(\n    # KS statistic (difference between cumulative distributions)\n    ks = round(abs(c.events_pct - c.non_events_pct), 2), \n    \n    # Capture rate (percentage of total events captured)\n    cap_rate = percent(cumsum(events)/sum(events), 1, 100), \n    \n    # Cumulative event rate\n    c_event_rate = percent(cumsum(events)/cumsum(total), 0.1, 100), \n    \n    # Format percentage columns\n    c.events_pct = percent(c.events_pct, 0.1, 100),\n    c.non_events_pct = percent(c.non_events_pct, 0.1, 100))\n\n# Display the final table\nkable(gains_table)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbins\ntotal\nevents\nnon_events\nevent_rate\npop_pct\nc.events_pct\nc.non_events_pct\nks\ncap_rate\nc_event_rate\n\n\n\n\n[-5.37,-3.3]\n300\n5\n295\n1.7%\n10.0%\n1.5%\n11.1%\n0.10\n1%\n1.7%\n\n\n(-3.3,-2.9]\n300\n8\n292\n2.7%\n10.0%\n3.8%\n22.1%\n0.18\n4%\n2.2%\n\n\n(-2.9,-2.66]\n300\n8\n292\n2.7%\n10.0%\n6.1%\n33.1%\n0.27\n6%\n2.3%\n\n\n(-2.66,-2.45]\n300\n11\n289\n3.7%\n10.0%\n9.3%\n44.0%\n0.35\n9%\n2.7%\n\n\n(-2.45,-2.25]\n300\n22\n278\n7.3%\n10.0%\n15.7%\n54.4%\n0.39\n16%\n3.6%\n\n\n(-2.25,-2.07]\n300\n34\n266\n11.3%\n10.0%\n25.7%\n64.4%\n0.39\n26%\n4.9%\n\n\n(-2.07,-1.86]\n300\n56\n244\n18.7%\n10.0%\n42.0%\n73.6%\n0.32\n42%\n6.9%\n\n\n(-1.86,-1.61]\n300\n65\n235\n21.7%\n10.0%\n60.9%\n82.5%\n0.22\n61%\n8.7%\n\n\n(-1.61,-1.23]\n300\n59\n241\n19.7%\n10.0%\n78.1%\n91.5%\n0.13\n78%\n9.9%\n\n\n(-1.23,1.6]\n300\n75\n225\n25.0%\n10.0%\n100.0%\n100.0%\n0.00\n100%\n11.4%\n\n\n\n\n\n\n\n\n\nLet’s encapsulate all the above steps into a single function that can be reused for any binary classification model:\n\ngains_table &lt;- function(act, pred, increasing = TRUE, nBins = 10) {\n  \n  # Create bins based on predictions\n  q &lt;- quantile(pred, probs = seq(0, 1, length.out = nBins + 1))\n  bins &lt;- cut(pred, breaks = q, include.lowest = TRUE, right = TRUE, ordered_result = TRUE)\n  \n  df &lt;- data.frame(act, pred, bins)\n  \n  df %&gt;% \n    # Group by bins and calculate statistics\n    group_by(bins) %&gt;% \n    summarise(total = n(), \n              events = sum(act == 1), \n              non_events = sum(act == 0)) %&gt;% \n    mutate(event_rate = percent(events / total, 0.1, 100)) %&gt;% \n    \n    # Sort the table based on the 'increasing' parameter\n    {if(increasing == TRUE) {\n      arrange(., bins)\n    } else {\n      arrange(., desc(bins))\n    }} %&gt;% \n    \n    # Add all performance metrics\n    mutate(pop_pct = percent(total/sum(total), 0.1, 100), \n           c.events_pct = cumsum(events) / sum(events),\n           c.non_events_pct = cumsum(non_events) / sum(non_events), \n           ks = round(abs(c.events_pct - c.non_events_pct), 2), \n           cap_rate = percent(cumsum(events)/sum(events), 1, 100), \n           c_event_rate = percent(cumsum(events)/cumsum(total), 0.1, 100), \n           c.events_pct = percent(c.events_pct, 0.1, 100),\n           c.non_events_pct = percent(c.non_events_pct, 0.1, 100))\n}\n\n\n\n\n# Generate a gains table with bins in descending order\ntab &lt;- gains_table(test$bad_flag, test$pred, FALSE, 10)\nkable(tab)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbins\ntotal\nevents\nnon_events\nevent_rate\npop_pct\nc.events_pct\nc.non_events_pct\nks\ncap_rate\nc_event_rate\n\n\n\n\n(-1.23,1.6]\n300\n75\n225\n25.0%\n10.0%\n21.9%\n8.5%\n0.13\n22%\n25.0%\n\n\n(-1.61,-1.23]\n300\n59\n241\n19.7%\n10.0%\n39.1%\n17.5%\n0.22\n39%\n22.3%\n\n\n(-1.86,-1.61]\n300\n65\n235\n21.7%\n10.0%\n58.0%\n26.4%\n0.32\n58%\n22.1%\n\n\n(-2.07,-1.86]\n300\n56\n244\n18.7%\n10.0%\n74.3%\n35.6%\n0.39\n74%\n21.2%\n\n\n(-2.25,-2.07]\n300\n34\n266\n11.3%\n10.0%\n84.3%\n45.6%\n0.39\n84%\n19.3%\n\n\n(-2.45,-2.25]\n300\n22\n278\n7.3%\n10.0%\n90.7%\n56.0%\n0.35\n91%\n17.3%\n\n\n(-2.66,-2.45]\n300\n11\n289\n3.7%\n10.0%\n93.9%\n66.9%\n0.27\n94%\n15.3%\n\n\n(-2.9,-2.66]\n300\n8\n292\n2.7%\n10.0%\n96.2%\n77.9%\n0.18\n96%\n13.8%\n\n\n(-3.3,-2.9]\n300\n8\n292\n2.7%\n10.0%\n98.5%\n88.9%\n0.10\n99%\n12.5%\n\n\n[-5.37,-3.3]\n300\n5\n295\n1.7%\n10.0%\n100.0%\n100.0%\n0.00\n100%\n11.4%\n\n\n\n\n\n\n\n\n\nA gains table provides several key insights into model performance:\n\nMonotonicity: The event rates should consistently increase (or decrease) across bins. This confirms that the model effectively rank-orders risk.\nBin Consistency: If bin sizes are not consistent (ideally ~10% each), it suggests the model is assigning the same output/score to many borrowers (clumping), which could pose issues when deciding cutoffs.\nKS Statistic: The maximum value of the KS column indicates the model’s discriminatory power. A higher value (closer to 1) indicates better separation between good and bad borrowers.\nCapture Rate: Shows what percentage of all bad accounts are captured at each cutoff point.\nCumulative Event Rate: Indicates the bad rate among all accounts up to that bin, useful for setting approval thresholds.\n\n\n\n\nIn credit risk management, the gains table helps with:\n\nSetting Cutoffs: Identifying appropriate score thresholds for approving or rejecting applications.\nStrategy Development: Creating tiered strategies (e.g., approve, review, decline) based on risk levels.\nPerformance Monitoring: Tracking model performance over time by comparing actual vs. expected distributions.\nModel Comparison: Evaluating different models by comparing their KS statistics and capture rates.\n\nThe gains table is a powerful tool for evaluating binary classification models, especially in credit risk applications. By providing a structured view of how well a model separates good and bad cases across the score distribution, it helps analysts make informed decisions about model quality and operational implementation."
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#what-is-a-gains-table",
    "href": "posts/measuring-model-performance-gains-table.html#what-is-a-gains-table",
    "title": "Measuring Model Performance Using a Gains Table",
    "section": "",
    "text": "A gains table discretizes the population (typically a test or validation set) into groups based on the model’s output (probability, log odds, or scores). Usually, each group represents 10% of the total population (deciles). The table then presents summary statistics for each group and analyzes the cumulative distributions of events (defaults) and non-events to quantify the model’s performance."
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#required-libraries",
    "href": "posts/measuring-model-performance-gains-table.html#required-libraries",
    "title": "Measuring Model Performance Using a Gains Table",
    "section": "",
    "text": "# Load required packages\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(knitr)\nlibrary(scales)"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#sample-dataset",
    "href": "posts/measuring-model-performance-gains-table.html#sample-dataset",
    "title": "Measuring Model Performance Using a Gains Table",
    "section": "",
    "text": "We’ll use a sample from the Lending Club dataset, which contains information about loans and their outcomes.\n\n# Load the sample data\nsample &lt;- read.csv(\"https://bit.ly/42ypcnJ\")\n\n# Check dimensions\ndim(sample)\n\n[1] 10000   153"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#defining-the-target-variable",
    "href": "posts/measuring-model-performance-gains-table.html#defining-the-target-variable",
    "title": "Measuring Model Performance Using a Gains Table",
    "section": "",
    "text": "First, we need to create a target (outcome) variable to model. Since this is a credit risk use case, we’ll identify borrowers who defaulted on their payments.\n\n# Check unique loan statuses\nunique(sample$loan_status)\n\n[1] \"Fully Paid\"                                         \n[2] \"Current\"                                            \n[3] \"Charged Off\"                                        \n[4] \"Late (31-120 days)\"                                 \n[5] \"Late (16-30 days)\"                                  \n[6] \"In Grace Period\"                                    \n[7] \"Does not meet the credit policy. Status:Fully Paid\" \n[8] \"Does not meet the credit policy. Status:Charged Off\"\n\n# Define \"bad\" loans as those that are charged off\ncodes &lt;- c(\"Charged Off\", \"Does not meet the credit policy. Status:Charged Off\")\n\n# Create a binary flag for defaults\nsample %&lt;&gt;% mutate(bad_flag = ifelse(loan_status %in% codes, 1, 0))\n\n# Check overall event rates\nsample %&gt;% \n  summarise(events = sum(bad_flag == 1), \n            non_events = sum(bad_flag == 0)) %&gt;% \n  mutate(event_rate = events/(events + non_events))\n\n  events non_events event_rate\n1   1162       8838     0.1162"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#building-a-simple-model",
    "href": "posts/measuring-model-performance-gains-table.html#building-a-simple-model",
    "title": "Measuring Model Performance Using a Gains Table",
    "section": "",
    "text": "Next, let’s build a quick model, the output of which we’ll use to create the gains table.\n\n# Replace NA values with a default value\nsample[is.na(sample)] &lt;- -1\n\n# Clean the data\nsample %&lt;&gt;% \n  # Remove cases where home ownership and payment plan are not reported\n  filter(!home_ownership %in% c(\"\", \"NONE\"),\n         pymnt_plan != \"\") %&gt;% \n  # Convert categorical variables to factors\n  mutate(home_ownership = factor(home_ownership), \n         pymnt_plan = factor(pymnt_plan))\n\n# Train-test split (70-30)\nidx &lt;- sample(1:nrow(sample), size = 0.7 * nrow(sample), replace = FALSE)\ntrain &lt;- sample[idx,]\ntest &lt;- sample[-idx,]\n\n\n# Build a logistic regression model\nmdl &lt;- glm(\n  formula = bad_flag ~ \n    loan_amnt + term + mths_since_last_delinq + total_pymnt + \n    home_ownership + acc_now_delinq + \n    inq_last_6mths + delinq_amnt + \n    mths_since_last_record + mths_since_recent_revol_delinq + \n    mths_since_last_major_derog + mths_since_recent_inq + \n    mths_since_recent_bc + num_accts_ever_120_pd,\n  family = \"binomial\", \n  data = train\n)\n\n# Generate predictions on the test set\ntest$pred &lt;- predict(mdl, newdata = test)"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#creating-the-gains-table",
    "href": "posts/measuring-model-performance-gains-table.html#creating-the-gains-table",
    "title": "Measuring Model Performance Using a Gains Table",
    "section": "",
    "text": "Now let’s build the gains table step by step:\n\n\n\n# Create deciles based on model predictions\nq &lt;- quantile(test$pred, probs = seq(0, 1, length.out = 11))\n\n# Add bins to test dataset\ntest$bins &lt;- cut(test$pred, breaks = q, include.lowest = TRUE, \n                right = TRUE, ordered_result = TRUE)\n\n# Check the bin levels (note they're in increasing order)\nlevels(test$bins)\n\n [1] \"[-5.37,-3.3]\"  \"(-3.3,-2.9]\"   \"(-2.9,-2.66]\"  \"(-2.66,-2.45]\"\n [5] \"(-2.45,-2.25]\" \"(-2.25,-2.07]\" \"(-2.07,-1.86]\" \"(-1.86,-1.61]\"\n [9] \"(-1.61,-1.23]\" \"(-1.23,1.6]\"  \n\n\n\n\n\n\n# Create initial gains table with counts\ngains_table &lt;- test %&gt;% \n  group_by(bins) %&gt;% \n  summarise(total = n(), \n            events = sum(bad_flag == 1), \n            non_events = sum(bad_flag == 0))\n\n# Add event rate column\ngains_table %&lt;&gt;%\n  mutate(event_rate = percent(events / total, 0.1, 100))\n\n# Display the table\nkable(gains_table)\n\n\n\n\nbins\ntotal\nevents\nnon_events\nevent_rate\n\n\n\n\n[-5.37,-3.3]\n300\n5\n295\n1.7%\n\n\n(-3.3,-2.9]\n300\n8\n292\n2.7%\n\n\n(-2.9,-2.66]\n300\n8\n292\n2.7%\n\n\n(-2.66,-2.45]\n300\n11\n289\n3.7%\n\n\n(-2.45,-2.25]\n300\n22\n278\n7.3%\n\n\n(-2.25,-2.07]\n300\n34\n266\n11.3%\n\n\n(-2.07,-1.86]\n300\n56\n244\n18.7%\n\n\n(-1.86,-1.61]\n300\n65\n235\n21.7%\n\n\n(-1.61,-1.23]\n300\n59\n241\n19.7%\n\n\n(-1.23,1.6]\n300\n75\n225\n25.0%\n\n\n\n\n\n\n\n\n\n# Add population percentage and cumulative distributions\ngains_table %&lt;&gt;%\n  mutate(pop_pct = percent(total/sum(total), 0.1, 100), \n         \n         # Calculate cumulative percentages\n         c.events_pct = cumsum(events) / sum(events),\n         c.non_events_pct = cumsum(non_events) / sum(non_events))\n\n# Display the updated table\nkable(gains_table)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbins\ntotal\nevents\nnon_events\nevent_rate\npop_pct\nc.events_pct\nc.non_events_pct\n\n\n\n\n[-5.37,-3.3]\n300\n5\n295\n1.7%\n10.0%\n0.0145773\n0.1110275\n\n\n(-3.3,-2.9]\n300\n8\n292\n2.7%\n10.0%\n0.0379009\n0.2209259\n\n\n(-2.9,-2.66]\n300\n8\n292\n2.7%\n10.0%\n0.0612245\n0.3308242\n\n\n(-2.66,-2.45]\n300\n11\n289\n3.7%\n10.0%\n0.0932945\n0.4395935\n\n\n(-2.45,-2.25]\n300\n22\n278\n7.3%\n10.0%\n0.1574344\n0.5442228\n\n\n(-2.25,-2.07]\n300\n34\n266\n11.3%\n10.0%\n0.2565598\n0.6443357\n\n\n(-2.07,-1.86]\n300\n56\n244\n18.7%\n10.0%\n0.4198251\n0.7361686\n\n\n(-1.86,-1.61]\n300\n65\n235\n21.7%\n10.0%\n0.6093294\n0.8246142\n\n\n(-1.61,-1.23]\n300\n59\n241\n19.7%\n10.0%\n0.7813411\n0.9153180\n\n\n(-1.23,1.6]\n300\n75\n225\n25.0%\n10.0%\n1.0000000\n1.0000000\n\n\n\n\n\n\n\n\n\n# Add KS statistic, capture rate, and cumulative event rate\ngains_table %&lt;&gt;%\n  mutate(\n    # KS statistic (difference between cumulative distributions)\n    ks = round(abs(c.events_pct - c.non_events_pct), 2), \n    \n    # Capture rate (percentage of total events captured)\n    cap_rate = percent(cumsum(events)/sum(events), 1, 100), \n    \n    # Cumulative event rate\n    c_event_rate = percent(cumsum(events)/cumsum(total), 0.1, 100), \n    \n    # Format percentage columns\n    c.events_pct = percent(c.events_pct, 0.1, 100),\n    c.non_events_pct = percent(c.non_events_pct, 0.1, 100))\n\n# Display the final table\nkable(gains_table)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbins\ntotal\nevents\nnon_events\nevent_rate\npop_pct\nc.events_pct\nc.non_events_pct\nks\ncap_rate\nc_event_rate\n\n\n\n\n[-5.37,-3.3]\n300\n5\n295\n1.7%\n10.0%\n1.5%\n11.1%\n0.10\n1%\n1.7%\n\n\n(-3.3,-2.9]\n300\n8\n292\n2.7%\n10.0%\n3.8%\n22.1%\n0.18\n4%\n2.2%\n\n\n(-2.9,-2.66]\n300\n8\n292\n2.7%\n10.0%\n6.1%\n33.1%\n0.27\n6%\n2.3%\n\n\n(-2.66,-2.45]\n300\n11\n289\n3.7%\n10.0%\n9.3%\n44.0%\n0.35\n9%\n2.7%\n\n\n(-2.45,-2.25]\n300\n22\n278\n7.3%\n10.0%\n15.7%\n54.4%\n0.39\n16%\n3.6%\n\n\n(-2.25,-2.07]\n300\n34\n266\n11.3%\n10.0%\n25.7%\n64.4%\n0.39\n26%\n4.9%\n\n\n(-2.07,-1.86]\n300\n56\n244\n18.7%\n10.0%\n42.0%\n73.6%\n0.32\n42%\n6.9%\n\n\n(-1.86,-1.61]\n300\n65\n235\n21.7%\n10.0%\n60.9%\n82.5%\n0.22\n61%\n8.7%\n\n\n(-1.61,-1.23]\n300\n59\n241\n19.7%\n10.0%\n78.1%\n91.5%\n0.13\n78%\n9.9%\n\n\n(-1.23,1.6]\n300\n75\n225\n25.0%\n10.0%\n100.0%\n100.0%\n0.00\n100%\n11.4%"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#creating-a-reusable-function",
    "href": "posts/measuring-model-performance-gains-table.html#creating-a-reusable-function",
    "title": "Measuring Model Performance Using a Gains Table",
    "section": "",
    "text": "Let’s encapsulate all the above steps into a single function that can be reused for any binary classification model:\n\ngains_table &lt;- function(act, pred, increasing = TRUE, nBins = 10) {\n  \n  # Create bins based on predictions\n  q &lt;- quantile(pred, probs = seq(0, 1, length.out = nBins + 1))\n  bins &lt;- cut(pred, breaks = q, include.lowest = TRUE, right = TRUE, ordered_result = TRUE)\n  \n  df &lt;- data.frame(act, pred, bins)\n  \n  df %&gt;% \n    # Group by bins and calculate statistics\n    group_by(bins) %&gt;% \n    summarise(total = n(), \n              events = sum(act == 1), \n              non_events = sum(act == 0)) %&gt;% \n    mutate(event_rate = percent(events / total, 0.1, 100)) %&gt;% \n    \n    # Sort the table based on the 'increasing' parameter\n    {if(increasing == TRUE) {\n      arrange(., bins)\n    } else {\n      arrange(., desc(bins))\n    }} %&gt;% \n    \n    # Add all performance metrics\n    mutate(pop_pct = percent(total/sum(total), 0.1, 100), \n           c.events_pct = cumsum(events) / sum(events),\n           c.non_events_pct = cumsum(non_events) / sum(non_events), \n           ks = round(abs(c.events_pct - c.non_events_pct), 2), \n           cap_rate = percent(cumsum(events)/sum(events), 1, 100), \n           c_event_rate = percent(cumsum(events)/cumsum(total), 0.1, 100), \n           c.events_pct = percent(c.events_pct, 0.1, 100),\n           c.non_events_pct = percent(c.non_events_pct, 0.1, 100))\n}\n\n\n\n\n# Generate a gains table with bins in descending order\ntab &lt;- gains_table(test$bad_flag, test$pred, FALSE, 10)\nkable(tab)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbins\ntotal\nevents\nnon_events\nevent_rate\npop_pct\nc.events_pct\nc.non_events_pct\nks\ncap_rate\nc_event_rate\n\n\n\n\n(-1.23,1.6]\n300\n75\n225\n25.0%\n10.0%\n21.9%\n8.5%\n0.13\n22%\n25.0%\n\n\n(-1.61,-1.23]\n300\n59\n241\n19.7%\n10.0%\n39.1%\n17.5%\n0.22\n39%\n22.3%\n\n\n(-1.86,-1.61]\n300\n65\n235\n21.7%\n10.0%\n58.0%\n26.4%\n0.32\n58%\n22.1%\n\n\n(-2.07,-1.86]\n300\n56\n244\n18.7%\n10.0%\n74.3%\n35.6%\n0.39\n74%\n21.2%\n\n\n(-2.25,-2.07]\n300\n34\n266\n11.3%\n10.0%\n84.3%\n45.6%\n0.39\n84%\n19.3%\n\n\n(-2.45,-2.25]\n300\n22\n278\n7.3%\n10.0%\n90.7%\n56.0%\n0.35\n91%\n17.3%\n\n\n(-2.66,-2.45]\n300\n11\n289\n3.7%\n10.0%\n93.9%\n66.9%\n0.27\n94%\n15.3%\n\n\n(-2.9,-2.66]\n300\n8\n292\n2.7%\n10.0%\n96.2%\n77.9%\n0.18\n96%\n13.8%\n\n\n(-3.3,-2.9]\n300\n8\n292\n2.7%\n10.0%\n98.5%\n88.9%\n0.10\n99%\n12.5%\n\n\n[-5.37,-3.3]\n300\n5\n295\n1.7%\n10.0%\n100.0%\n100.0%\n0.00\n100%\n11.4%"
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#interpreting-the-gains-table",
    "href": "posts/measuring-model-performance-gains-table.html#interpreting-the-gains-table",
    "title": "Measuring Model Performance Using a Gains Table",
    "section": "",
    "text": "A gains table provides several key insights into model performance:\n\nMonotonicity: The event rates should consistently increase (or decrease) across bins. This confirms that the model effectively rank-orders risk.\nBin Consistency: If bin sizes are not consistent (ideally ~10% each), it suggests the model is assigning the same output/score to many borrowers (clumping), which could pose issues when deciding cutoffs.\nKS Statistic: The maximum value of the KS column indicates the model’s discriminatory power. A higher value (closer to 1) indicates better separation between good and bad borrowers.\nCapture Rate: Shows what percentage of all bad accounts are captured at each cutoff point.\nCumulative Event Rate: Indicates the bad rate among all accounts up to that bin, useful for setting approval thresholds."
  },
  {
    "objectID": "posts/measuring-model-performance-gains-table.html#practical-applications",
    "href": "posts/measuring-model-performance-gains-table.html#practical-applications",
    "title": "Measuring Model Performance Using a Gains Table",
    "section": "",
    "text": "In credit risk management, the gains table helps with:\n\nSetting Cutoffs: Identifying appropriate score thresholds for approving or rejecting applications.\nStrategy Development: Creating tiered strategies (e.g., approve, review, decline) based on risk levels.\nPerformance Monitoring: Tracking model performance over time by comparing actual vs. expected distributions.\nModel Comparison: Evaluating different models by comparing their KS statistics and capture rates.\n\nThe gains table is a powerful tool for evaluating binary classification models, especially in credit risk applications. By providing a structured view of how well a model separates good and bad cases across the score distribution, it helps analysts make informed decisions about model quality and operational implementation."
  },
  {
    "objectID": "posts/particle-swarm-optimization.html",
    "href": "posts/particle-swarm-optimization.html",
    "title": "Portfolio Optimization Using Particle Swarm Optimization in R",
    "section": "",
    "text": "Portfolio optimization is a critical task in investment management, where the goal is to allocate capital across different assets to maximize returns while controlling risk. In this post, we’ll explore how to use Particle Swarm Optimization (PSO) to perform mean-variance portfolio optimization with various constraints.\nPSO is a computational method inspired by the social behavior of bird flocking or fish schooling. It optimizes a problem by having a population of candidate solutions (particles) that move around the search space according to mathematical formulas. Each particle’s movement is influenced by its local best-known position and also guided toward the best-known positions found by other particles in the swarm.\nThis approach is particularly valuable for portfolio optimization because: 1. It can handle non-convex and complex constraint functions 2. It doesn’t require derivatives of the objective function 3. It can escape local optima through its stochastic nature\nFor additional information on mean-variance optimization and the CAPM model, refer to this paper.\n\n\nBefore we begin, let’s load the R packages we’ll need for this analysis:\n\n# Load required packages\nlibrary(pso)       # For PSO implementation (provides psoptim function)\nlibrary(ggplot2)   # For data visualization\nlibrary(dplyr)     # For data manipulation and transformation\nlibrary(quantmod)  # For downloading financial data\nlibrary(tidyr)     # For reshaping data (pivot_wider, gather functions)\nlibrary(plotly)    # For creating interactive 3D visualizations\n\n\n\n\nThe first step in portfolio optimization is gathering the necessary data. We’ll need historical price data to calculate returns and risk metrics.\n\n\nFor this demonstration, we’ll use stocks from the NIFTY50 index, which includes the 50 largest Indian companies by market capitalization:\n\n# Read ticker list from NSE (National Stock Exchange of India) website\nticker_list &lt;- read.csv(\"https://raw.githubusercontent.com/royr2/datasets/refs/heads/main/ind_nifty50list.csv\")\n\n# View the first few rows to understand the data structure\nhead(ticker_list[,1:3], 5)\n\n                                Company.Name           Industry     Symbol\n1                     Adani Enterprises Ltd.    Metals & Mining   ADANIENT\n2 Adani Ports and Special Economic Zone Ltd.           Services ADANIPORTS\n3           Apollo Hospitals Enterprise Ltd.         Healthcare APOLLOHOSP\n4                          Asian Paints Ltd.  Consumer Durables ASIANPAINT\n5                             Axis Bank Ltd. Financial Services   AXISBANK\n\n\n\n\n\nNow we’ll download historical price data for these stocks using the quantmod package, which provides an interface to Yahoo Finance:\n\n# Append \".NS\" to tickers for Yahoo Finance format (NS = National Stock Exchange)\ntickers &lt;- paste0(ticker_list$Symbol, \".NS\")\ntickers &lt;- tickers[!tickers %in% c(\"ETERNAL.NS\", \"JIOFIN.NS\")]\n\n# Initialize empty dataframe to store all ticker data\nticker_df &lt;- data.frame()\n\n# Create a progress bar to monitor the download process\n# pb &lt;- txtProgressBar(min = 1, max = length(tickers), style = 3)\n\n# Loop through each ticker and download its historical data\nfor(nms in tickers){\n  # Download data from Yahoo Finance\n  df &lt;- getSymbols(Symbols = nms, verbose = FALSE, auto.assign = FALSE)\n  \n  # Rename columns for clarity\n  colnames(df) &lt;- c(\"open\", \"high\", \"low\", \"close\", \"volume\", \"adjusted\")\n  df$date = rownames(df)\n  \n  # Convert to dataframe and add ticker and date information\n  df &lt;- data.frame(df)\n  df$ticker &lt;- nms\n  df$date &lt;- rownames(df)\n  \n  # Append to the main dataframe\n  ticker_df &lt;- rbind(ticker_df, df)\n  \n  Sys.sleep(0.2)\n  \n  # Update progress bar\n  # setTxtProgressBar(pb, which(tickers == nms))\n}\n\n# Reshape data to wide format with dates as rows and tickers as columns\n# This makes it easier to calculate returns across all stocks\nprices_df &lt;- pivot_wider(data = ticker_df, id_cols = \"date\", names_from = \"ticker\", values_from = \"close\")\n\n# Remove rows with missing values to ensure complete data\nprices_df &lt;- na.omit(prices_df)\n\n# Check the date range of our data\nrange(prices_df$date)\n\n[1] \"2017-11-17\" \"2025-04-17\"\n\n# Check dimensions (number of trading days × number of stocks + date column)\ndim(prices_df)\n\n[1] 1830   49\n\n\n\n\n\nBefore proceeding with analysis, it’s always good practice to visualize the data to check for anomalies and understand the general trends. Let’s visualize the price data for a subset of stocks (focusing on the metals industry):\n\n# Plot closing prices for metal stocks\nprices_df %&gt;% \n  # Convert from wide to long format for easier plotting with ggplot2\n  pivot_longer(-date, names_to = \"ticker\", values_to = \"price\") %&gt;% \n  \n  # Attach industry information from our original ticker list\n  left_join(ticker_list %&gt;% \n              mutate(ticker = paste0(Symbol, \".NS\")) %&gt;% \n              select(ticker, industry = Industry),\n            by = \"ticker\") %&gt;% \n  \n  # Convert date strings to Date objects\n  mutate(date = as.Date(date)) %&gt;% \n  \n  # Filter to show only metal industry stocks for clarity\n  filter(stringr::str_detect(tolower(industry), \"metal\")) %&gt;% \n  \n  # Create the line plot\n  ggplot(aes(x = date, y = price, color = ticker)) + \n  geom_line(linewidth = 0.8) + \n  theme_minimal() + \n  scale_color_brewer(palette = \"RdBu\") +  # Use a color-blind friendly palette\n  labs(title = \"Closing Prices\", \n       subtitle = \"Nifty 50 metal stocks\",\n       x = \"Date\", \n       y = \"Closing Price\") + \n  theme(legend.position = \"top\", \n        legend.title = element_text(colour = \"transparent\"), \n        axis.title.x = element_text(face = \"bold\"), \n        axis.title.y = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\nClosing prices for metal stocks\n\n\nThe visualization shows the price movements of metal stocks over time. We can observe periods of both correlation and divergence between different stocks, highlighting the importance of diversification in portfolio construction.\n\n\n\nFor portfolio optimization, we need to work with returns rather than prices. Returns better represent the investment performance and have more desirable statistical properties (like stationarity):\n\n# Calculate daily returns for all stocks\n# Formula: (Price_today / Price_yesterday) - 1\nreturns_df &lt;- apply(prices_df[,-1], 2, function(vec){\n  ret &lt;- vec/lag(vec) - 1  # Simple returns calculation\n  return(ret)\n})\n\n# Convert to dataframe for easier manipulation\nreturns_df &lt;- as.data.frame(returns_df)\n\n# Remove first row which contains NA values (no previous day to calculate return)\nreturns_df &lt;- returns_df[-1,]  \n\n# Pre-compute average returns and covariance matrix for optimization\n# These are key inputs to the mean-variance optimization\nmean_returns &lt;- sapply(returns_df, mean)  # Expected returns\ncov_mat &lt;- cov(returns_df)  # Risk (covariance) matrix\n\nThe mean returns represent our expectations for each asset’s performance, while the covariance matrix captures both the individual volatilities and the relationships between assets. These will be the primary inputs to our optimization process.\n\n\n\n\n\n\nThe core of portfolio optimization is the objective function, which defines what we’re trying to maximize or minimize. In mean-variance optimization, we balance three key components:\n\nExpected returns (reward): The weighted average of expected returns for each asset\nPortfolio variance (risk): A measure of the portfolio’s volatility, calculated using the covariance matrix\nRisk aversion parameter: Controls the trade-off between risk and return (higher values prioritize risk reduction)\n\nWe’ll also incorporate constraints through penalty terms:\n\nobj_func &lt;- function(wts, \n                     risk_av = 10,  # Risk aversion parameter\n                     lambda1 = 10,  # Penalty weight for full investment constraint\n                     lambda2 = 1e2,  # Reserved for additional constraints\n                     ret_vec, cov_mat){\n  \n  # Calculate expected portfolio return (weighted average of asset returns)\n  port_returns &lt;- ret_vec %*% wts\n  \n  # Calculate portfolio risk (quadratic form using covariance matrix)\n  port_risk &lt;- t(wts) %*% cov_mat %*% wts\n  \n  # Mean-variance utility function: return - risk_aversion * risk\n  # This is the core Markowitz portfolio optimization formula\n  obj &lt;- port_returns - risk_av * port_risk\n  \n  # Add penalty for violating the full investment constraint (sum of weights = 1)\n  # The squared term ensures the penalty increases quadratically with violation size\n  obj &lt;- obj - lambda1 * (sum(wts) - 1)^2\n  \n  # Return negative value since PSO minimizes by default, but we want to maximize\n  # our objective (higher returns, lower risk)\n  return(-obj)\n}\n\nThis objective function implements the classic mean-variance utility with a quadratic penalty for the full investment constraint. The risk aversion parameter allows us to move along the efficient frontier to find portfolios with different risk-return profiles.\n\n\n\n\nBefore tackling the full portfolio optimization problem, let’s start with a simple two-asset example. This will help us visualize how PSO works and validate our approach:\n\n# Use only the first two assets for this example\n# Calculate their average returns and covariance matrix\nmean_returns_small &lt;- apply(returns_df[,1:2], 2, mean)\ncov_mat_small &lt;- cov(returns_df[,1:2])\n\n# Define a custom PSO optimizer function to track the optimization process\npso_optim &lt;- function(obj_func,\n                      c1 = 0.05,      # Cognitive parameter (personal best influence)\n                      c2 = 0.05,      # Social parameter (global best influence)\n                      w = 0.8,        # Inertia weight (controls momentum)\n                      init_fact = 0.1, # Initial velocity factor\n                      n_particles = 20, # Number of particles in the swarm\n                      n_dim = 2,       # Dimensionality (number of assets)\n                      n_iter = 50,     # Maximum iterations\n                      upper = 1,       # Upper bound for weights\n                      lower = 0,       # Lower bound for weights (no short selling)\n                      n_avg = 10,      # Number of iterations for averaging\n                      ...){\n  \n  # Initialize particle positions randomly within bounds\n  X &lt;- matrix(runif(n_particles * n_dim), nrow = n_particles)\n  X &lt;- X * (upper - lower) + lower  # Scale to fit within bounds\n  \n  # Initialize particle velocities (movement speeds)\n  dX &lt;- matrix(runif(n_particles * n_dim) * init_fact, ncol = n_dim)\n  dX &lt;- dX * (upper - lower) + lower\n  \n  # Initialize personal best positions and objective values\n  pbest &lt;- X  # Each particle's best position so far\n  pbest_obj &lt;- apply(X, 1, obj_func, ...)  # Objective value at personal best\n  \n  # Initialize global best position and objective value\n  gbest &lt;- pbest[which.min(pbest_obj),]  # Best position across all particles\n  gbest_obj &lt;- min(pbest_obj)  # Best objective value found\n  \n  # Store initial positions for visualization\n  loc_df &lt;- data.frame(X, iter = 0, obj = pbest_obj)\n  iter &lt;- 1\n  \n  # Main PSO loop\n  while(iter &lt; n_iter){\n    \n    # Update velocities using PSO formula:\n    # New velocity = inertia + cognitive component + social component\n    dX &lt;- w * dX +                         # Inertia (continue in same direction)\n          c1*runif(1)*(pbest - X) +        # Pull toward personal best\n          c2*runif(1)*t(gbest - t(X))      # Pull toward global best\n    \n    # Update positions based on velocities\n    X &lt;- X + dX\n    \n    # Evaluate objective function at new positions\n    obj &lt;- apply(X, 1, obj_func, ...)\n    \n    # Update personal bests if new positions are better\n    idx &lt;- which(obj &lt;= pbest_obj)\n    pbest[idx,] &lt;- X[idx,]\n    pbest_obj[idx] &lt;- obj[idx]\n    \n    # Update global best if a better solution is found\n    idx &lt;- which.min(pbest_obj)\n    gbest &lt;- pbest[idx,]\n    gbest_obj &lt;- min(pbest_obj)\n    \n    # Store current state for visualization\n    iter &lt;- iter + 1\n    loc_df &lt;- rbind(loc_df, data.frame(X, iter = iter, obj = pbest_obj))\n  }\n  \n  # Return optimization results\n  lst &lt;- list(X = loc_df,          # All particle positions throughout optimization\n              obj = gbest_obj,     # Best objective value found\n              obj_loc = gbest)     # Weights that achieved the best objective\n  return(lst)\n}\n\n# Run the optimization for our two-asset portfolio\nout &lt;- pso_optim(obj_func,\n                 ret_vec = mean_returns_small,  # Expected returns\n                 cov_mat = cov_mat_small,       # Covariance matrix\n                 lambda1 = 10, risk_av = 100,    # Constraint and risk parameters\n                 n_particles = 100,              # Use 100 particles for better coverage\n                 n_dim = 2,                      # Two-asset portfolio\n                 n_iter = 200,                   # Run for 200 iterations\n                 upper = 1, lower = 0,           # Bounds for weights\n                 c1 = 0.02, c2 = 0.02,           # Lower influence parameters for stability\n                 w = 0.05, init_fact = 0.01)     # Low inertia for better convergence\n\n# Verify that the weights sum to approximately 1 (full investment constraint)\nsum(out$obj_loc)\n\n[1] 0.9973249\n\n\nIn this implementation, we’re tracking the movement of all particles throughout the optimization process. This will allow us to visualize how the swarm converges toward the optimal solution.\n\n\nOne of the advantages of starting with a two-asset example is that we can visualize the entire search space and see how the PSO algorithm explores it. Let’s create a 3D visualization of the objective function landscape and the path each particle took during optimization:\n\n# Create a fine grid of points covering the feasible region (all possible weight combinations)\ngrid &lt;- expand.grid(x = seq(0, 1, by = 0.01),  # First asset weight from 0 to 1\n                    y = seq(0, 1, by = 0.01))   # Second asset weight from 0 to 1\n\n# Evaluate the objective function at each grid point to create the landscape\ngrid$obj &lt;- apply(grid, 1, obj_func, \n                  ret_vec = mean_returns_small, \n                  cov_mat = cov_mat_small, \n                  lambda1 = 10, risk_av = 100)\n\n# Create an interactive 3D plot showing both the objective function surface\n# and the particle trajectories throughout the optimization\np &lt;- plot_ly() %&gt;% \n  # Add the objective function surface as a mesh\n  add_mesh(data = grid, x = ~x, y = ~y, z = ~obj, \n           inherit = FALSE, color = \"red\") %&gt;% \n  \n  # Add particles as markers, colored by iteration to show progression\n  add_markers(data = out$X, x = ~X1, y = ~X2, z = ~obj, \n              color = ~ iter, inherit = FALSE, \n              marker = list(size = 2))\n\nThis visualization shows: 1. The objective function landscape as a 3D surface 2. The particles (small dots) exploring the search space 3. How the swarm converges toward the optimal solution over iterations (color gradient)\nThe concentration of particles in certain regions shows where the algorithm found promising solutions. The global best solution is where the particles ultimately converge.\n\n\n\n\nNow that we understand the basic principles, let’s scale up to optimize a portfolio containing all the assets in our dataset. Instead of using our custom PSO implementation, we’ll leverage the more efficient psoptim function from the pso package:\n\n# Get the number of stocks in our dataset\nn_stocks &lt;- ncol(returns_df)\n\n# Run the PSO optimization for the full portfolio\nopt &lt;- psoptim(\n  # Initial particle positions (starting with equal weights)\n  par = rep(0, n_stocks),\n  \n  # Objective function to minimize\n  fn = obj_func,\n  \n  # Pass the expected returns and covariance matrix\n  ret_vec = mean_returns, \n  cov_mat = cov_mat,\n  \n  # Set constraint parameters\n  lambda1 = 10,  # Weight for full investment constraint\n  risk_av = 1000,  # Higher risk aversion for a more conservative portfolio\n  \n  # Set bounds for weights (no short selling allowed)\n  lower = rep(0, n_stocks),\n  upper = rep(1, n_stocks),\n  \n  # Configure the PSO algorithm\n  control = list(\n    maxit = 200,          # Maximum iterations\n    s = 100,               # Swarm size (number of particles)\n    maxit.stagnate = 500   # Stop if no improvement after this many iterations\n  )\n)\n\n# Calculate and display the expected return of the optimized portfolio\npaste(\"Portfolio returns:\", round(opt$par %*% mean_returns, 5))\n\n[1] \"Portfolio returns: 0.00063\"\n\n# Calculate and display the standard deviation (risk) of the optimized portfolio\npaste(\"Portfolio Std dev:\", round(sqrt(opt$par %*% cov_mat %*% opt$par), 5))\n\n[1] \"Portfolio Std dev: 0.00888\"\n\n# Verify that the weights sum to approximately 1 (full investment constraint)\nsum(opt$par)\n\n[1] 0.9915935\n\n\nThe optimization has found a portfolio allocation that balances return and risk according to our specified risk aversion parameter. The high risk aversion value (1000) means we’re prioritizing risk reduction over return maximization.\n\n\n\nOne of the advantages of PSO is its flexibility in handling various constraints. Let’s demonstrate this by adding a tracking error constraint, which is common in institutional portfolio management. Tracking error measures how closely a portfolio follows a benchmark:\n\n# Define benchmark portfolio (equally weighted across all stocks)\nbench_wts &lt;- rep(1/n_stocks, n_stocks)\n\n# Calculate the time series of benchmark returns\nbench_returns &lt;- as.matrix(returns_df) %*% t(t(bench_wts))\n\n# Create a new objective function that includes tracking error\nobj_func_TE &lt;- function(wts,  \n                        risk_av = 10,     # Risk aversion parameter\n                        lambda1 = 10,    # Full investment constraint weight\n                        lambda2 = 50,    # Tracking error constraint weight\n                        ret_vec, cov_mat){\n  \n  # Calculate portfolio metrics\n  port_returns &lt;- ret_vec %*% wts                      # Expected portfolio return\n  port_risk &lt;- t(wts) %*% cov_mat %*% wts             # Portfolio variance\n  port_returns_ts &lt;- as.matrix(returns_df) %*% t(t(wts))  # Time series of portfolio returns\n  \n  # Original mean-variance objective\n  obj &lt;- port_returns - risk_av * port_risk\n  \n  # Full investment constraint (weights sum to 1)\n  obj &lt;- obj - lambda1 * (sum(wts) - 1)^2\n  \n  # Tracking error constraint (penalize deviation from benchmark)\n  # Tracking error is measured as the standard deviation of the difference\n  # between portfolio returns and benchmark returns\n  obj &lt;- obj - lambda2 * sd(port_returns_ts - bench_returns)\n  \n  return(-obj)  # Return negative for minimization\n}\n\n# Run optimization with the tracking error constraint\nopt &lt;- psoptim(\n  # Initial particle positions\n  par = rep(0, n_stocks),\n  \n  # Use our new objective function with tracking error\n  fn = obj_func_TE,\n  \n  # Pass the expected returns and covariance matrix\n  ret_vec = mean_returns, \n  cov_mat = cov_mat,\n  \n  # Set constraint parameters\n  lambda1 = 10,    # Weight for full investment constraint\n  risk_av = 1000,  # Risk aversion parameter\n  \n  # Set bounds for weights\n  lower = rep(0, n_stocks),\n  upper = rep(1, n_stocks),\n  \n  # Configure the PSO algorithm\n  control = list(\n    maxit = 200,          # Maximum iterations\n    s = 100,               # Swarm size\n    maxit.stagnate = 500   # Stop if no improvement after this many iterations\n  )\n)\n\n# Calculate and display the expected return of the optimized portfolio\npaste(\"Portfolio returns:\", round(opt$par %*% mean_returns, 5))\n\n[1] \"Portfolio returns: 0.00076\"\n\n# Calculate and display the standard deviation (risk) of the optimized portfolio\npaste(\"Portfolio Std dev:\", round(sqrt(opt$par %*% cov_mat %*% opt$par), 5))\n\n[1] \"Portfolio Std dev: 0.01102\"\n\n# Verify that the weights sum to approximately 1\nsum(opt$par)\n\n[1] 0.9944118\n\n\nBy adding the tracking error constraint, we’ve created a portfolio that not only balances risk and return but also tracks the performance of an equally-weighted benchmark to a specified degree. The lambda2 parameter controls how closely we want to track the benchmark - higher values will result in portfolios that more closely resemble the benchmark.\n\n\n\n\n\n\nFlexibility: PSO can handle non-convex, non-differentiable objective functions, making it suitable for complex portfolio constraints that traditional optimizers struggle with\nSimplicity: The algorithm is intuitive and relatively easy to implement compared to other global optimization techniques\nConstraints: Various constraints can be easily incorporated through penalty functions without reformulating the entire problem\nGlobal Search: PSO explores the search space more thoroughly and is less likely to get stuck in local optima compared to gradient-based methods\nParallelization: The algorithm is naturally parallelizable, as particles can be evaluated independently\n\n\n\n\n\nVariability: Results can vary between runs due to the stochastic nature of the algorithm, potentially leading to inconsistent portfolio recommendations\nParameter Tuning: Performance significantly depends on parameters like inertia weight and acceleration coefficients, which may require careful tuning\nConvergence: There’s no mathematical guarantee of convergence to the global optimum, unlike some convex optimization methods\nComputational Cost: Can be computationally intensive for high-dimensional problems with many assets\nConstraint Handling: While flexible, the penalty function approach may not always satisfy constraints exactly\n\n\n\n\n\nPSO-based portfolio optimization is particularly valuable in scenarios where:\n\nTraditional quadratic programming approaches fail due to complex constraints\nThe objective function includes non-linear terms like higher moments (skewness, kurtosis)\nMultiple competing objectives need to be balanced\nThe portfolio needs to satisfy regulatory or client-specific constraints\n\n\n\n\nParticle Swarm Optimization provides a powerful and flexible approach to portfolio optimization that can overcome many limitations of traditional methods. It can handle complex objective functions and constraints that might be difficult to solve with classical optimization techniques.\nThe approach demonstrated in this post can be extended to include additional constraints such as:\n\nSector or industry exposure limits\nMaximum position sizes\nTurnover or transaction cost constraints\nRisk factor exposures and limits\nCardinality constraints (limiting the number of assets)\n\nFor more robust results in practice, consider these enhancements:\n\nRun the algorithm multiple times with different random seeds and average the results\nImplement a hybrid approach that uses PSO for global exploration followed by a local optimizer for refinement\nAdd constraints gradually to better understand their impact on the portfolio\n\nPSO represents just one of many metaheuristic approaches that can be applied to portfolio optimization. Other techniques like genetic algorithms, simulated annealing, or differential evolution might also be worth exploring depending on your specific requirements."
  },
  {
    "objectID": "posts/particle-swarm-optimization.html#required-libraries",
    "href": "posts/particle-swarm-optimization.html#required-libraries",
    "title": "Portfolio Optimization Using Particle Swarm Optimization in R",
    "section": "",
    "text": "Before we begin, let’s load the R packages we’ll need for this analysis:\n\n# Load required packages\nlibrary(pso)       # For PSO implementation (provides psoptim function)\nlibrary(ggplot2)   # For data visualization\nlibrary(dplyr)     # For data manipulation and transformation\nlibrary(quantmod)  # For downloading financial data\nlibrary(tidyr)     # For reshaping data (pivot_wider, gather functions)\nlibrary(plotly)    # For creating interactive 3D visualizations"
  },
  {
    "objectID": "posts/particle-swarm-optimization.html#data-collection-and-preparation",
    "href": "posts/particle-swarm-optimization.html#data-collection-and-preparation",
    "title": "Portfolio Optimization Using Particle Swarm Optimization in R",
    "section": "",
    "text": "The first step in portfolio optimization is gathering the necessary data. We’ll need historical price data to calculate returns and risk metrics.\n\n\nFor this demonstration, we’ll use stocks from the NIFTY50 index, which includes the 50 largest Indian companies by market capitalization:\n\n# Read ticker list from NSE (National Stock Exchange of India) website\nticker_list &lt;- read.csv(\"https://raw.githubusercontent.com/royr2/datasets/refs/heads/main/ind_nifty50list.csv\")\n\n# View the first few rows to understand the data structure\nhead(ticker_list[,1:3], 5)\n\n                                Company.Name           Industry     Symbol\n1                     Adani Enterprises Ltd.    Metals & Mining   ADANIENT\n2 Adani Ports and Special Economic Zone Ltd.           Services ADANIPORTS\n3           Apollo Hospitals Enterprise Ltd.         Healthcare APOLLOHOSP\n4                          Asian Paints Ltd.  Consumer Durables ASIANPAINT\n5                             Axis Bank Ltd. Financial Services   AXISBANK\n\n\n\n\n\nNow we’ll download historical price data for these stocks using the quantmod package, which provides an interface to Yahoo Finance:\n\n# Append \".NS\" to tickers for Yahoo Finance format (NS = National Stock Exchange)\ntickers &lt;- paste0(ticker_list$Symbol, \".NS\")\ntickers &lt;- tickers[!tickers %in% c(\"ETERNAL.NS\", \"JIOFIN.NS\")]\n\n# Initialize empty dataframe to store all ticker data\nticker_df &lt;- data.frame()\n\n# Create a progress bar to monitor the download process\n# pb &lt;- txtProgressBar(min = 1, max = length(tickers), style = 3)\n\n# Loop through each ticker and download its historical data\nfor(nms in tickers){\n  # Download data from Yahoo Finance\n  df &lt;- getSymbols(Symbols = nms, verbose = FALSE, auto.assign = FALSE)\n  \n  # Rename columns for clarity\n  colnames(df) &lt;- c(\"open\", \"high\", \"low\", \"close\", \"volume\", \"adjusted\")\n  df$date = rownames(df)\n  \n  # Convert to dataframe and add ticker and date information\n  df &lt;- data.frame(df)\n  df$ticker &lt;- nms\n  df$date &lt;- rownames(df)\n  \n  # Append to the main dataframe\n  ticker_df &lt;- rbind(ticker_df, df)\n  \n  Sys.sleep(0.2)\n  \n  # Update progress bar\n  # setTxtProgressBar(pb, which(tickers == nms))\n}\n\n# Reshape data to wide format with dates as rows and tickers as columns\n# This makes it easier to calculate returns across all stocks\nprices_df &lt;- pivot_wider(data = ticker_df, id_cols = \"date\", names_from = \"ticker\", values_from = \"close\")\n\n# Remove rows with missing values to ensure complete data\nprices_df &lt;- na.omit(prices_df)\n\n# Check the date range of our data\nrange(prices_df$date)\n\n[1] \"2017-11-17\" \"2025-04-17\"\n\n# Check dimensions (number of trading days × number of stocks + date column)\ndim(prices_df)\n\n[1] 1830   49\n\n\n\n\n\nBefore proceeding with analysis, it’s always good practice to visualize the data to check for anomalies and understand the general trends. Let’s visualize the price data for a subset of stocks (focusing on the metals industry):\n\n# Plot closing prices for metal stocks\nprices_df %&gt;% \n  # Convert from wide to long format for easier plotting with ggplot2\n  pivot_longer(-date, names_to = \"ticker\", values_to = \"price\") %&gt;% \n  \n  # Attach industry information from our original ticker list\n  left_join(ticker_list %&gt;% \n              mutate(ticker = paste0(Symbol, \".NS\")) %&gt;% \n              select(ticker, industry = Industry),\n            by = \"ticker\") %&gt;% \n  \n  # Convert date strings to Date objects\n  mutate(date = as.Date(date)) %&gt;% \n  \n  # Filter to show only metal industry stocks for clarity\n  filter(stringr::str_detect(tolower(industry), \"metal\")) %&gt;% \n  \n  # Create the line plot\n  ggplot(aes(x = date, y = price, color = ticker)) + \n  geom_line(linewidth = 0.8) + \n  theme_minimal() + \n  scale_color_brewer(palette = \"RdBu\") +  # Use a color-blind friendly palette\n  labs(title = \"Closing Prices\", \n       subtitle = \"Nifty 50 metal stocks\",\n       x = \"Date\", \n       y = \"Closing Price\") + \n  theme(legend.position = \"top\", \n        legend.title = element_text(colour = \"transparent\"), \n        axis.title.x = element_text(face = \"bold\"), \n        axis.title.y = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\nClosing prices for metal stocks\n\n\nThe visualization shows the price movements of metal stocks over time. We can observe periods of both correlation and divergence between different stocks, highlighting the importance of diversification in portfolio construction.\n\n\n\nFor portfolio optimization, we need to work with returns rather than prices. Returns better represent the investment performance and have more desirable statistical properties (like stationarity):\n\n# Calculate daily returns for all stocks\n# Formula: (Price_today / Price_yesterday) - 1\nreturns_df &lt;- apply(prices_df[,-1], 2, function(vec){\n  ret &lt;- vec/lag(vec) - 1  # Simple returns calculation\n  return(ret)\n})\n\n# Convert to dataframe for easier manipulation\nreturns_df &lt;- as.data.frame(returns_df)\n\n# Remove first row which contains NA values (no previous day to calculate return)\nreturns_df &lt;- returns_df[-1,]  \n\n# Pre-compute average returns and covariance matrix for optimization\n# These are key inputs to the mean-variance optimization\nmean_returns &lt;- sapply(returns_df, mean)  # Expected returns\ncov_mat &lt;- cov(returns_df)  # Risk (covariance) matrix\n\nThe mean returns represent our expectations for each asset’s performance, while the covariance matrix captures both the individual volatilities and the relationships between assets. These will be the primary inputs to our optimization process."
  },
  {
    "objectID": "posts/particle-swarm-optimization.html#portfolio-optimization-framework",
    "href": "posts/particle-swarm-optimization.html#portfolio-optimization-framework",
    "title": "Portfolio Optimization Using Particle Swarm Optimization in R",
    "section": "",
    "text": "The core of portfolio optimization is the objective function, which defines what we’re trying to maximize or minimize. In mean-variance optimization, we balance three key components:\n\nExpected returns (reward): The weighted average of expected returns for each asset\nPortfolio variance (risk): A measure of the portfolio’s volatility, calculated using the covariance matrix\nRisk aversion parameter: Controls the trade-off between risk and return (higher values prioritize risk reduction)\n\nWe’ll also incorporate constraints through penalty terms:\n\nobj_func &lt;- function(wts, \n                     risk_av = 10,  # Risk aversion parameter\n                     lambda1 = 10,  # Penalty weight for full investment constraint\n                     lambda2 = 1e2,  # Reserved for additional constraints\n                     ret_vec, cov_mat){\n  \n  # Calculate expected portfolio return (weighted average of asset returns)\n  port_returns &lt;- ret_vec %*% wts\n  \n  # Calculate portfolio risk (quadratic form using covariance matrix)\n  port_risk &lt;- t(wts) %*% cov_mat %*% wts\n  \n  # Mean-variance utility function: return - risk_aversion * risk\n  # This is the core Markowitz portfolio optimization formula\n  obj &lt;- port_returns - risk_av * port_risk\n  \n  # Add penalty for violating the full investment constraint (sum of weights = 1)\n  # The squared term ensures the penalty increases quadratically with violation size\n  obj &lt;- obj - lambda1 * (sum(wts) - 1)^2\n  \n  # Return negative value since PSO minimizes by default, but we want to maximize\n  # our objective (higher returns, lower risk)\n  return(-obj)\n}\n\nThis objective function implements the classic mean-variance utility with a quadratic penalty for the full investment constraint. The risk aversion parameter allows us to move along the efficient frontier to find portfolios with different risk-return profiles."
  },
  {
    "objectID": "posts/particle-swarm-optimization.html#two-asset-example",
    "href": "posts/particle-swarm-optimization.html#two-asset-example",
    "title": "Portfolio Optimization Using Particle Swarm Optimization in R",
    "section": "",
    "text": "Before tackling the full portfolio optimization problem, let’s start with a simple two-asset example. This will help us visualize how PSO works and validate our approach:\n\n# Use only the first two assets for this example\n# Calculate their average returns and covariance matrix\nmean_returns_small &lt;- apply(returns_df[,1:2], 2, mean)\ncov_mat_small &lt;- cov(returns_df[,1:2])\n\n# Define a custom PSO optimizer function to track the optimization process\npso_optim &lt;- function(obj_func,\n                      c1 = 0.05,      # Cognitive parameter (personal best influence)\n                      c2 = 0.05,      # Social parameter (global best influence)\n                      w = 0.8,        # Inertia weight (controls momentum)\n                      init_fact = 0.1, # Initial velocity factor\n                      n_particles = 20, # Number of particles in the swarm\n                      n_dim = 2,       # Dimensionality (number of assets)\n                      n_iter = 50,     # Maximum iterations\n                      upper = 1,       # Upper bound for weights\n                      lower = 0,       # Lower bound for weights (no short selling)\n                      n_avg = 10,      # Number of iterations for averaging\n                      ...){\n  \n  # Initialize particle positions randomly within bounds\n  X &lt;- matrix(runif(n_particles * n_dim), nrow = n_particles)\n  X &lt;- X * (upper - lower) + lower  # Scale to fit within bounds\n  \n  # Initialize particle velocities (movement speeds)\n  dX &lt;- matrix(runif(n_particles * n_dim) * init_fact, ncol = n_dim)\n  dX &lt;- dX * (upper - lower) + lower\n  \n  # Initialize personal best positions and objective values\n  pbest &lt;- X  # Each particle's best position so far\n  pbest_obj &lt;- apply(X, 1, obj_func, ...)  # Objective value at personal best\n  \n  # Initialize global best position and objective value\n  gbest &lt;- pbest[which.min(pbest_obj),]  # Best position across all particles\n  gbest_obj &lt;- min(pbest_obj)  # Best objective value found\n  \n  # Store initial positions for visualization\n  loc_df &lt;- data.frame(X, iter = 0, obj = pbest_obj)\n  iter &lt;- 1\n  \n  # Main PSO loop\n  while(iter &lt; n_iter){\n    \n    # Update velocities using PSO formula:\n    # New velocity = inertia + cognitive component + social component\n    dX &lt;- w * dX +                         # Inertia (continue in same direction)\n          c1*runif(1)*(pbest - X) +        # Pull toward personal best\n          c2*runif(1)*t(gbest - t(X))      # Pull toward global best\n    \n    # Update positions based on velocities\n    X &lt;- X + dX\n    \n    # Evaluate objective function at new positions\n    obj &lt;- apply(X, 1, obj_func, ...)\n    \n    # Update personal bests if new positions are better\n    idx &lt;- which(obj &lt;= pbest_obj)\n    pbest[idx,] &lt;- X[idx,]\n    pbest_obj[idx] &lt;- obj[idx]\n    \n    # Update global best if a better solution is found\n    idx &lt;- which.min(pbest_obj)\n    gbest &lt;- pbest[idx,]\n    gbest_obj &lt;- min(pbest_obj)\n    \n    # Store current state for visualization\n    iter &lt;- iter + 1\n    loc_df &lt;- rbind(loc_df, data.frame(X, iter = iter, obj = pbest_obj))\n  }\n  \n  # Return optimization results\n  lst &lt;- list(X = loc_df,          # All particle positions throughout optimization\n              obj = gbest_obj,     # Best objective value found\n              obj_loc = gbest)     # Weights that achieved the best objective\n  return(lst)\n}\n\n# Run the optimization for our two-asset portfolio\nout &lt;- pso_optim(obj_func,\n                 ret_vec = mean_returns_small,  # Expected returns\n                 cov_mat = cov_mat_small,       # Covariance matrix\n                 lambda1 = 10, risk_av = 100,    # Constraint and risk parameters\n                 n_particles = 100,              # Use 100 particles for better coverage\n                 n_dim = 2,                      # Two-asset portfolio\n                 n_iter = 200,                   # Run for 200 iterations\n                 upper = 1, lower = 0,           # Bounds for weights\n                 c1 = 0.02, c2 = 0.02,           # Lower influence parameters for stability\n                 w = 0.05, init_fact = 0.01)     # Low inertia for better convergence\n\n# Verify that the weights sum to approximately 1 (full investment constraint)\nsum(out$obj_loc)\n\n[1] 0.9973249\n\n\nIn this implementation, we’re tracking the movement of all particles throughout the optimization process. This will allow us to visualize how the swarm converges toward the optimal solution.\n\n\nOne of the advantages of starting with a two-asset example is that we can visualize the entire search space and see how the PSO algorithm explores it. Let’s create a 3D visualization of the objective function landscape and the path each particle took during optimization:\n\n# Create a fine grid of points covering the feasible region (all possible weight combinations)\ngrid &lt;- expand.grid(x = seq(0, 1, by = 0.01),  # First asset weight from 0 to 1\n                    y = seq(0, 1, by = 0.01))   # Second asset weight from 0 to 1\n\n# Evaluate the objective function at each grid point to create the landscape\ngrid$obj &lt;- apply(grid, 1, obj_func, \n                  ret_vec = mean_returns_small, \n                  cov_mat = cov_mat_small, \n                  lambda1 = 10, risk_av = 100)\n\n# Create an interactive 3D plot showing both the objective function surface\n# and the particle trajectories throughout the optimization\np &lt;- plot_ly() %&gt;% \n  # Add the objective function surface as a mesh\n  add_mesh(data = grid, x = ~x, y = ~y, z = ~obj, \n           inherit = FALSE, color = \"red\") %&gt;% \n  \n  # Add particles as markers, colored by iteration to show progression\n  add_markers(data = out$X, x = ~X1, y = ~X2, z = ~obj, \n              color = ~ iter, inherit = FALSE, \n              marker = list(size = 2))\n\nThis visualization shows: 1. The objective function landscape as a 3D surface 2. The particles (small dots) exploring the search space 3. How the swarm converges toward the optimal solution over iterations (color gradient)\nThe concentration of particles in certain regions shows where the algorithm found promising solutions. The global best solution is where the particles ultimately converge."
  },
  {
    "objectID": "posts/particle-swarm-optimization.html#multi-asset-portfolio-optimization",
    "href": "posts/particle-swarm-optimization.html#multi-asset-portfolio-optimization",
    "title": "Portfolio Optimization Using Particle Swarm Optimization in R",
    "section": "",
    "text": "Now that we understand the basic principles, let’s scale up to optimize a portfolio containing all the assets in our dataset. Instead of using our custom PSO implementation, we’ll leverage the more efficient psoptim function from the pso package:\n\n# Get the number of stocks in our dataset\nn_stocks &lt;- ncol(returns_df)\n\n# Run the PSO optimization for the full portfolio\nopt &lt;- psoptim(\n  # Initial particle positions (starting with equal weights)\n  par = rep(0, n_stocks),\n  \n  # Objective function to minimize\n  fn = obj_func,\n  \n  # Pass the expected returns and covariance matrix\n  ret_vec = mean_returns, \n  cov_mat = cov_mat,\n  \n  # Set constraint parameters\n  lambda1 = 10,  # Weight for full investment constraint\n  risk_av = 1000,  # Higher risk aversion for a more conservative portfolio\n  \n  # Set bounds for weights (no short selling allowed)\n  lower = rep(0, n_stocks),\n  upper = rep(1, n_stocks),\n  \n  # Configure the PSO algorithm\n  control = list(\n    maxit = 200,          # Maximum iterations\n    s = 100,               # Swarm size (number of particles)\n    maxit.stagnate = 500   # Stop if no improvement after this many iterations\n  )\n)\n\n# Calculate and display the expected return of the optimized portfolio\npaste(\"Portfolio returns:\", round(opt$par %*% mean_returns, 5))\n\n[1] \"Portfolio returns: 0.00063\"\n\n# Calculate and display the standard deviation (risk) of the optimized portfolio\npaste(\"Portfolio Std dev:\", round(sqrt(opt$par %*% cov_mat %*% opt$par), 5))\n\n[1] \"Portfolio Std dev: 0.00888\"\n\n# Verify that the weights sum to approximately 1 (full investment constraint)\nsum(opt$par)\n\n[1] 0.9915935\n\n\nThe optimization has found a portfolio allocation that balances return and risk according to our specified risk aversion parameter. The high risk aversion value (1000) means we’re prioritizing risk reduction over return maximization."
  },
  {
    "objectID": "posts/particle-swarm-optimization.html#adding-tracking-error-constraint",
    "href": "posts/particle-swarm-optimization.html#adding-tracking-error-constraint",
    "title": "Portfolio Optimization Using Particle Swarm Optimization in R",
    "section": "",
    "text": "One of the advantages of PSO is its flexibility in handling various constraints. Let’s demonstrate this by adding a tracking error constraint, which is common in institutional portfolio management. Tracking error measures how closely a portfolio follows a benchmark:\n\n# Define benchmark portfolio (equally weighted across all stocks)\nbench_wts &lt;- rep(1/n_stocks, n_stocks)\n\n# Calculate the time series of benchmark returns\nbench_returns &lt;- as.matrix(returns_df) %*% t(t(bench_wts))\n\n# Create a new objective function that includes tracking error\nobj_func_TE &lt;- function(wts,  \n                        risk_av = 10,     # Risk aversion parameter\n                        lambda1 = 10,    # Full investment constraint weight\n                        lambda2 = 50,    # Tracking error constraint weight\n                        ret_vec, cov_mat){\n  \n  # Calculate portfolio metrics\n  port_returns &lt;- ret_vec %*% wts                      # Expected portfolio return\n  port_risk &lt;- t(wts) %*% cov_mat %*% wts             # Portfolio variance\n  port_returns_ts &lt;- as.matrix(returns_df) %*% t(t(wts))  # Time series of portfolio returns\n  \n  # Original mean-variance objective\n  obj &lt;- port_returns - risk_av * port_risk\n  \n  # Full investment constraint (weights sum to 1)\n  obj &lt;- obj - lambda1 * (sum(wts) - 1)^2\n  \n  # Tracking error constraint (penalize deviation from benchmark)\n  # Tracking error is measured as the standard deviation of the difference\n  # between portfolio returns and benchmark returns\n  obj &lt;- obj - lambda2 * sd(port_returns_ts - bench_returns)\n  \n  return(-obj)  # Return negative for minimization\n}\n\n# Run optimization with the tracking error constraint\nopt &lt;- psoptim(\n  # Initial particle positions\n  par = rep(0, n_stocks),\n  \n  # Use our new objective function with tracking error\n  fn = obj_func_TE,\n  \n  # Pass the expected returns and covariance matrix\n  ret_vec = mean_returns, \n  cov_mat = cov_mat,\n  \n  # Set constraint parameters\n  lambda1 = 10,    # Weight for full investment constraint\n  risk_av = 1000,  # Risk aversion parameter\n  \n  # Set bounds for weights\n  lower = rep(0, n_stocks),\n  upper = rep(1, n_stocks),\n  \n  # Configure the PSO algorithm\n  control = list(\n    maxit = 200,          # Maximum iterations\n    s = 100,               # Swarm size\n    maxit.stagnate = 500   # Stop if no improvement after this many iterations\n  )\n)\n\n# Calculate and display the expected return of the optimized portfolio\npaste(\"Portfolio returns:\", round(opt$par %*% mean_returns, 5))\n\n[1] \"Portfolio returns: 0.00076\"\n\n# Calculate and display the standard deviation (risk) of the optimized portfolio\npaste(\"Portfolio Std dev:\", round(sqrt(opt$par %*% cov_mat %*% opt$par), 5))\n\n[1] \"Portfolio Std dev: 0.01102\"\n\n# Verify that the weights sum to approximately 1\nsum(opt$par)\n\n[1] 0.9944118\n\n\nBy adding the tracking error constraint, we’ve created a portfolio that not only balances risk and return but also tracks the performance of an equally-weighted benchmark to a specified degree. The lambda2 parameter controls how closely we want to track the benchmark - higher values will result in portfolios that more closely resemble the benchmark."
  },
  {
    "objectID": "posts/particle-swarm-optimization.html#advantages-and-limitations-of-pso",
    "href": "posts/particle-swarm-optimization.html#advantages-and-limitations-of-pso",
    "title": "Portfolio Optimization Using Particle Swarm Optimization in R",
    "section": "",
    "text": "Flexibility: PSO can handle non-convex, non-differentiable objective functions, making it suitable for complex portfolio constraints that traditional optimizers struggle with\nSimplicity: The algorithm is intuitive and relatively easy to implement compared to other global optimization techniques\nConstraints: Various constraints can be easily incorporated through penalty functions without reformulating the entire problem\nGlobal Search: PSO explores the search space more thoroughly and is less likely to get stuck in local optima compared to gradient-based methods\nParallelization: The algorithm is naturally parallelizable, as particles can be evaluated independently\n\n\n\n\n\nVariability: Results can vary between runs due to the stochastic nature of the algorithm, potentially leading to inconsistent portfolio recommendations\nParameter Tuning: Performance significantly depends on parameters like inertia weight and acceleration coefficients, which may require careful tuning\nConvergence: There’s no mathematical guarantee of convergence to the global optimum, unlike some convex optimization methods\nComputational Cost: Can be computationally intensive for high-dimensional problems with many assets\nConstraint Handling: While flexible, the penalty function approach may not always satisfy constraints exactly"
  },
  {
    "objectID": "posts/particle-swarm-optimization.html#practical-applications",
    "href": "posts/particle-swarm-optimization.html#practical-applications",
    "title": "Portfolio Optimization Using Particle Swarm Optimization in R",
    "section": "",
    "text": "PSO-based portfolio optimization is particularly valuable in scenarios where:\n\nTraditional quadratic programming approaches fail due to complex constraints\nThe objective function includes non-linear terms like higher moments (skewness, kurtosis)\nMultiple competing objectives need to be balanced\nThe portfolio needs to satisfy regulatory or client-specific constraints"
  },
  {
    "objectID": "posts/particle-swarm-optimization.html#conclusion",
    "href": "posts/particle-swarm-optimization.html#conclusion",
    "title": "Portfolio Optimization Using Particle Swarm Optimization in R",
    "section": "",
    "text": "Particle Swarm Optimization provides a powerful and flexible approach to portfolio optimization that can overcome many limitations of traditional methods. It can handle complex objective functions and constraints that might be difficult to solve with classical optimization techniques.\nThe approach demonstrated in this post can be extended to include additional constraints such as:\n\nSector or industry exposure limits\nMaximum position sizes\nTurnover or transaction cost constraints\nRisk factor exposures and limits\nCardinality constraints (limiting the number of assets)\n\nFor more robust results in practice, consider these enhancements:\n\nRun the algorithm multiple times with different random seeds and average the results\nImplement a hybrid approach that uses PSO for global exploration followed by a local optimizer for refinement\nAdd constraints gradually to better understand their impact on the portfolio\n\nPSO represents just one of many metaheuristic approaches that can be applied to portfolio optimization. Other techniques like genetic algorithms, simulated annealing, or differential evolution might also be worth exploring depending on your specific requirements."
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html",
    "href": "posts/modeling-with-tidymodels.html",
    "title": "Building Models in R with tidymodels",
    "section": "",
    "text": "The tidymodels framework provides a cohesive set of packages for modeling and machine learning in R, following tidyverse principles. In this post, we’ll build a realistic credit scoring model using tidymodels.\nCredit scoring models are used by financial institutions to assess the creditworthiness of borrowers. These models predict the probability of default (failure to repay a loan) based on borrower characteristics and loan attributes. A good credit scoring model should effectively discriminate between high-risk and low-risk borrowers, be well-calibrated, and provide interpretable insights."
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#required-packages",
    "href": "posts/modeling-with-tidymodels.html#required-packages",
    "title": "Building Models in R with tidymodels",
    "section": "Required Packages",
    "text": "Required Packages\nFirst, let’s load all the required packages for our analysis:\n\nlibrary(tidymodels)\nlibrary(xgboost)\nlibrary(vip)        # For variable importance\nlibrary(stringr)    # For string manipulation functions\nlibrary(probably)   # For calibration plots\nlibrary(ROSE)       # For imbalanced data visualization\nlibrary(corrplot)   # For correlation visualization"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#creating-a-realistic-credit-scoring-dataset",
    "href": "posts/modeling-with-tidymodels.html#creating-a-realistic-credit-scoring-dataset",
    "title": "Building Models in R with tidymodels",
    "section": "Creating a Realistic Credit Scoring Dataset",
    "text": "Creating a Realistic Credit Scoring Dataset\nWe’ll simulate a realistic credit dataset with common variables found in credit scoring models. The data will include demographic information, loan characteristics, and credit history variables.\n\nset.seed(123)\nn &lt;- 10000  # Larger sample size for more realistic modeling\n\n# Create base features with realistic distributions\ndata &lt;- tibble(\n  customer_id = paste0(\"CUS\", formatC(1:n, width = 6, format = \"d\", flag = \"0\")),\n  \n  # Demographics - with realistic age distribution for credit applicants\n  age = pmax(18, pmin(80, round(rnorm(n, 38, 13)))),\n  income = pmax(12000, round(rlnorm(n, log(52000), 0.8))),\n  employment_length = pmax(0, round(rexp(n, 1/6))),  # Exponential distribution for job tenure\n  home_ownership = sample(c(\"RENT\", \"MORTGAGE\", \"OWN\"), n, replace = TRUE, prob = c(0.45, 0.40, 0.15)),\n  \n  # Loan characteristics - with more realistic correlations\n  loan_amount = round(rlnorm(n, log(15000), 0.7) / 100) * 100,  # Log-normal for loan amounts\n  loan_term = sample(c(36, 60, 120), n, replace = TRUE, prob = c(0.6, 0.3, 0.1)),\n  \n  # Credit history - with more realistic distributions\n  credit_score = round(pmin(850, pmax(300, rnorm(n, 700, 90)))),\n  dti_ratio = pmax(0, pmin(65, rlnorm(n, log(20), 0.4))),  # Debt-to-income ratio\n  delinq_2yrs = rpois(n, 0.4),  # Number of delinquencies in past 2 years\n  inq_last_6mths = rpois(n, 0.7),  # Number of inquiries in last 6 months\n  open_acc = pmax(1, round(rnorm(n, 10, 4))),  # Number of open accounts\n  pub_rec = rbinom(n, 2, 0.06),  # Number of public records\n  revol_util = pmin(100, pmax(0, rnorm(n, 40, 20))),  # Revolving utilization\n  total_acc = pmax(open_acc, open_acc + round(rnorm(n, 8, 6)))  # Total accounts\n)\n\n# Add realistic correlations between variables\ndata &lt;- data %&gt;%\n  mutate(\n    # Interest rate depends on credit score and loan term\n    interest_rate = 25 - (credit_score - 300) * (15/550) + \n                    ifelse(loan_term == 36, -1, ifelse(loan_term == 60, 0, 1.5)) +\n                    rnorm(n, 0, 1.5),\n    \n    # Loan purpose with realistic probabilities\n    loan_purpose = sample(\n      c(\"debt_consolidation\", \"credit_card\", \"home_improvement\", \"major_purchase\", \"medical\", \"other\"), \n      n, replace = TRUE, \n      prob = c(0.45, 0.25, 0.10, 0.08, 0.07, 0.05)\n    ),\n    \n    # Add some derived features that have predictive power\n    payment_amount = (loan_amount * (interest_rate/100/12) * (1 + interest_rate/100/12)^loan_term) / \n                    ((1 + interest_rate/100/12)^loan_term - 1),\n    payment_to_income_ratio = (payment_amount * 12) / income\n  )\n\n# Create a more realistic default probability model with non-linear effects\nlogit_default &lt;- with(data, {\n  -4.5 +  # Base intercept for ~10% default rate\n    -0.03 * (age - 18) +  # Age effect (stronger for younger borrowers)\n    -0.2 * log(income/10000) +  # Log-transformed income effect\n    -0.08 * employment_length +  # Employment length effect\n    ifelse(home_ownership == \"OWN\", -0.7, ifelse(home_ownership == \"MORTGAGE\", -0.3, 0)) +  # Home ownership\n    0.3 * log(loan_amount/1000) +  # Log-transformed loan amount\n    ifelse(loan_term == 36, 0, ifelse(loan_term == 60, 0.4, 0.8)) +  # Loan term\n    0.15 * interest_rate +  # Interest rate effect\n    ifelse(loan_purpose == \"debt_consolidation\", 0.5, \n           ifelse(loan_purpose == \"credit_card\", 0.4, \n                  ifelse(loan_purpose == \"medical\", 0.6, 0))) +  # Loan purpose\n    -0.01 * (credit_score - 300) +  # Credit score (stronger effect at lower scores)\n    0.06 * dti_ratio +  # DTI ratio effect\n    0.4 * delinq_2yrs +  # Delinquencies effect (stronger effect for first delinquency)\n    0.3 * inq_last_6mths +  # Inquiries effect\n    -0.1 * log(open_acc + 1) +  # Open accounts (log-transformed)\n    0.8 * pub_rec +  # Public records (strong effect)\n    0.02 * revol_util +  # Revolving utilization\n    1.2 * payment_to_income_ratio +  # Payment to income ratio (strong effect)\n    rnorm(n, 0, 0.8)  # Add some noise for realistic variation\n})\n\n# Generate default flag with realistic default rate\nprob_default &lt;- plogis(logit_default)\ndata$default &lt;- factor(rbinom(n, 1, prob_default), levels = c(0, 1), labels = c(\"no\", \"yes\"))\n\n# Check class distribution\ntable(data$default)\n\n\n  no  yes \n9425  575 \n\n\n\nprop.table(table(data$default))\n\n\n    no    yes \n0.9425 0.0575 \n\n\n\n# Visualize the default rate\nggplot(data, aes(x = default, fill = default)) +\n  geom_bar(aes(y = ..prop.., group = 1)) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"Class Distribution in Credit Dataset\",\n       y = \"Percentage\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Visualize the relationship between key variables and default rate\nggplot(data, aes(x = credit_score, y = as.numeric(default) - 1)) +\n  geom_smooth(method = \"loess\") +\n  labs(title = \"Default Rate by Credit Score\", \n       x = \"Credit Score\", y = \"Default Probability\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Examine correlation between numeric predictors\ncredit_cors &lt;- data %&gt;%\n  select(age, income, employment_length, loan_amount, interest_rate, \n         credit_score, dti_ratio, delinq_2yrs, revol_util, payment_to_income_ratio) %&gt;%\n  cor()\n\ncorrplot(credit_cors, method = \"circle\", type = \"upper\", \n         tl.col = \"black\", tl.srt = 45, tl.cex = 0.7)"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#data-splitting",
    "href": "posts/modeling-with-tidymodels.html#data-splitting",
    "title": "Building Models in R with tidymodels",
    "section": "Data Splitting",
    "text": "Data Splitting\nCredit default datasets are typically imbalanced, with defaults being the minority class. We’ll use a stratified split to maintain the class distribution.\n\n# Create initial train/test split (80/20)\nset.seed(456)\ninitial_split &lt;- initial_split(data, prop = 0.8, strata = default)\ntrain_data &lt;- training(initial_split)\ntest_data &lt;- testing(initial_split)\n\n# Create validation set from training data (75% train, 25% validation)\nset.seed(789)\nvalidation_split &lt;- initial_split(train_data, prop = 0.75, strata = default)\n\n# Check class imbalance in training data\ntrain_class_counts &lt;- table(training(validation_split)$default)\ntrain_class_props &lt;- prop.table(train_class_counts)\n\ncat(\"Training data class distribution:\\n\")\n\nTraining data class distribution:\n\nprint(train_class_counts)\n\n\n  no  yes \n5661  339 \n\ncat(\"\\nPercentage:\\n\")\n\n\nPercentage:\n\nprint(train_class_props * 100)\n\n\n   no   yes \n94.35  5.65 \n\n# Visualize class imbalance\nROSE::roc.curve(training(validation_split)$default == \"yes\", \n                training(validation_split)$credit_score,\n                plotit = TRUE,\n                main = \"ROC Curve for Credit Score Alone\")\n\n\n\n\n\n\n\n\nArea under the curve (AUC): 0.753"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#feature-engineering-and-preprocessing",
    "href": "posts/modeling-with-tidymodels.html#feature-engineering-and-preprocessing",
    "title": "Building Models in R with tidymodels",
    "section": "Feature Engineering and Preprocessing",
    "text": "Feature Engineering and Preprocessing\nNow we’ll create a comprehensive recipe with feature engineering steps relevant to credit scoring, including handling the class imbalance.\n\n# Examine the distributions of key variables\npar(mfrow = c(2, 2))\nhist(training(validation_split)$credit_score, \n     main = \"Credit Score Distribution\", xlab = \"Credit Score\")\n\nhist(training(validation_split)$dti_ratio, \n     main = \"DTI Ratio Distribution\", xlab = \"DTI Ratio\")\n\nhist(training(validation_split)$payment_to_income_ratio, \n     main = \"Payment to Income Ratio\", \n     xlab = \"Payment to Income Ratio\")\n\nhist(log(training(validation_split)$income), \n     main = \"Log Income Distribution\", xlab = \"Log Income\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\n# Create a comprehensive recipe with domain knowledge\ncredit_recipe &lt;- recipe(default ~ ., data = training(validation_split)) %&gt;%\n  # Remove ID column\n  step_rm(customer_id) %&gt;%\n  \n  # Convert categorical variables to factors\n  step_string2factor(home_ownership, loan_purpose) %&gt;%\n  \n  # Create additional domain-specific features\n  step_mutate(\n    # We already have payment_to_income_ratio from data generation\n    # Add more credit risk indicators\n    credit_utilization = revol_util / 100,\n    acc_to_age_ratio = total_acc / age,\n    delinq_per_acc = ifelse(total_acc &gt; 0, delinq_2yrs / total_acc, 0),\n    inq_rate = inq_last_6mths / (open_acc + 0.1),  # Inquiry rate relative to open accounts\n    term_factor = loan_term / 12,  # Term in years\n    log_income = log(income),  # Log transform income\n    log_loan = log(loan_amount),  # Log transform loan amount\n    payment_ratio = payment_amount / (income / 12),  # Monthly payment to monthly income\n    util_to_income = (revol_util / 100) * (dti_ratio / 100)  # Interaction term\n  ) %&gt;%\n  \n  # Handle categorical variables\n  step_dummy(all_nominal_predictors()) %&gt;%\n  \n  # Impute missing values (if any)\n  step_impute_median(all_numeric_predictors()) %&gt;%\n  \n  # Transform highly skewed variables\n  step_YeoJohnson(income, loan_amount, payment_amount) %&gt;%\n  \n  # Remove highly correlated predictors\n  step_corr(all_numeric_predictors(), threshold = 0.85) %&gt;%\n  \n  # Normalize numeric predictors\n  step_normalize(all_numeric_predictors()) %&gt;%\n  \n  # Remove zero-variance predictors\n  step_zv(all_predictors())\n\n# Prep the recipe to examine the steps\nprepped_recipe &lt;- prep(credit_recipe)\nprepped_recipe\n\n# Check the transformed data\nrecipe_data &lt;- bake(prepped_recipe, new_data = NULL)\nglimpse(recipe_data)\n\nRows: 6,000\nColumns: 27\n$ age                             &lt;dbl&gt; -0.58938716, 1.60129128, 0.05970275, 0…\n$ employment_length               &lt;dbl&gt; 1.01545119, 0.17764322, 2.35594395, -0…\n$ loan_amount                     &lt;dbl&gt; -0.30772032, -1.64018559, 0.01897785, …\n$ credit_score                    &lt;dbl&gt; -1.66355858, -1.60583467, 0.02197934, …\n$ dti_ratio                       &lt;dbl&gt; -1.10745919, -0.21368748, -1.26746777,…\n$ delinq_2yrs                     &lt;dbl&gt; -0.6423758, -0.6423758, -0.6423758, -0…\n$ inq_last_6mths                  &lt;dbl&gt; -0.8324634, 1.5511111, -0.8324634, -0.…\n$ open_acc                        &lt;dbl&gt; -0.516984456, -1.282635289, -1.2826352…\n$ pub_rec                         &lt;dbl&gt; -0.3429372, -0.3429372, 2.7652549, -0.…\n$ total_acc                       &lt;dbl&gt; 0.39969129, 0.54625046, -0.47966374, 0…\n$ interest_rate                   &lt;dbl&gt; 1.47394527, 1.51546694, -0.11980960, 0…\n$ default                         &lt;fct&gt; no, no, no, no, no, no, no, no, yes, n…\n$ credit_utilization              &lt;dbl&gt; -1.841097091, 2.786415973, -1.09378697…\n$ acc_to_age_ratio                &lt;dbl&gt; 0.50174733, -0.54849606, -0.52980631, …\n$ delinq_per_acc                  &lt;dbl&gt; -0.44991964, -0.44991964, -0.44991964,…\n$ inq_rate                        &lt;dbl&gt; -0.520358012, 1.759991048, -0.52035801…\n$ term_factor                     &lt;dbl&gt; 0.3219163, -0.6274559, 0.3219163, -0.6…\n$ log_income                      &lt;dbl&gt; 2.44464243, 0.95097048, -0.59583257, 0…\n$ payment_ratio                   &lt;dbl&gt; -0.70055763, -0.66316187, -0.18762635,…\n$ util_to_income                  &lt;dbl&gt; -1.4271447, 1.7533431, -1.1717989, -1.…\n$ home_ownership_OWN              &lt;dbl&gt; -0.4238857, -0.4238857, -0.4238857, -0…\n$ home_ownership_RENT             &lt;dbl&gt; -0.8977787, -0.8977787, 1.1136746, 1.1…\n$ loan_purpose_debt_consolidation &lt;dbl&gt; 1.1204596, -0.8923421, -0.8923421, -0.…\n$ loan_purpose_home_improvement   &lt;dbl&gt; -0.3277222, -0.3277222, 3.0508567, -0.…\n$ loan_purpose_major_purchase     &lt;dbl&gt; -0.2968579, -0.2968579, -0.2968579, -0…\n$ loan_purpose_medical            &lt;dbl&gt; -0.299178, -0.299178, -0.299178, -0.29…\n$ loan_purpose_other              &lt;dbl&gt; -0.2208157, -0.2208157, -0.2208157, -0…\n\n# Verify class balance after SMOTE\ntable(recipe_data$default)\n\n\n  no  yes \n5661  339 \n\n\nThe feature engineering steps above incorporate domain knowledge specific to credit risk modeling:\n\nWe create derived features that capture payment capacity, credit utilization, and borrower stability\nWe transform highly skewed variables using Yeo-Johnson transformations\nWe normalize all numeric predictors to put them on the same scale"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#model-specification-and-tuning",
    "href": "posts/modeling-with-tidymodels.html#model-specification-and-tuning",
    "title": "Building Models in R with tidymodels",
    "section": "Model Specification and Tuning",
    "text": "Model Specification and Tuning\nFor credit scoring, we’ll compare two models: a logistic regression model (commonly used in the financial industry for its interpretability) and an XGBoost model (for its predictive power). We’ll tune both models using cross-validation.\n\n# Define the logistic regression model\nlog_reg_spec &lt;- logistic_reg(penalty = tune(), mixture = tune()) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"classification\")\n\n# Create a logistic regression workflow\nlog_reg_workflow &lt;- workflow() %&gt;%\n  add_recipe(credit_recipe) %&gt;%\n  add_model(log_reg_spec)\n\n# Define the tuning grid for logistic regression\nlog_reg_grid &lt;- grid_regular(\n  penalty(range = c(-5, 0), trans = log10_trans()),\n  mixture(range = c(0, 1)),\n  levels = c(10, 5)\n)\n\n# Define the XGBoost model with tunable parameters\nxgb_spec &lt;- boost_tree(\n  trees = tune(),\n  tree_depth = tune(),\n  min_n = tune(),\n  loss_reduction = tune(),\n  mtry = tune(),\n  learn_rate = tune()\n) %&gt;%\n  set_engine(\"xgboost\", objective = \"binary:logistic\", scale_pos_weight = 5) %&gt;%\n  set_mode(\"classification\")\n\n# Create an XGBoost workflow\nxgb_workflow &lt;- workflow() %&gt;%\n  add_recipe(credit_recipe) %&gt;%\n  add_model(xgb_spec)\n\n# Define the tuning grid for XGBoost\nxgb_grid &lt;- grid_latin_hypercube(\n  trees(range = c(100, 500)),\n  tree_depth(range = c(3, 10)),\n  min_n(range = c(2, 20)),\n  loss_reduction(range = c(0.001, 1.0)),\n  mtry(range = c(5, 20)),\n  learn_rate(range = c(-4, -1), trans = log10_trans()),\n  size = 15\n)\n\n# Create cross-validation folds with stratification\nset.seed(234)\ncv_folds &lt;- vfold_cv(training(validation_split), v = 3, strata = default)\n\n# Define the metrics to evaluate\nclassification_metrics &lt;- metric_set(\n  roc_auc,  # Area under the ROC curve\n  pr_auc,    # Area under the precision-recall curve\n)\n\n# Tune the logistic regression model\nset.seed(345)\nlog_reg_tuned &lt;- tune_grid(\n  log_reg_workflow,\n  resamples = cv_folds,\n  grid = log_reg_grid,\n  metrics = classification_metrics,\n  control = control_grid(save_pred = TRUE, verbose = TRUE)\n)\n\n# Tune the XGBoost model\nset.seed(456)\nxgb_tuned &lt;- tune_grid(\n  xgb_workflow,\n  resamples = cv_folds,\n  grid = xgb_grid,\n  metrics = classification_metrics,\n  control = control_grid(save_pred = TRUE, verbose = TRUE)\n)\n\n# Collect and visualize logistic regression tuning results\nlog_reg_results &lt;- log_reg_tuned %&gt;% collect_metrics()\nlog_reg_results %&gt;% filter(.metric == \"roc_auc\") %&gt;% arrange(desc(mean)) %&gt;% head()\n\n# A tibble: 6 × 8\n   penalty mixture .metric .estimator  mean     n std_err .config              \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 0.00167     1    roc_auc binary     0.853     3 0.00841 Preprocessor1_Model45\n2 0.00167     0.75 roc_auc binary     0.853     3 0.00841 Preprocessor1_Model35\n3 0.00599     0.25 roc_auc binary     0.853     3 0.00869 Preprocessor1_Model16\n4 0.00167     0.5  roc_auc binary     0.853     3 0.00836 Preprocessor1_Model25\n5 0.000464    1    roc_auc binary     0.852     3 0.00817 Preprocessor1_Model44\n6 0.00167     0.25 roc_auc binary     0.852     3 0.00837 Preprocessor1_Model15\n\n\nThe results above show how both models perform across different hyperparameter settings. XGBoost typically achieves higher predictive performance, while logistic regression offers better interpretability. For credit scoring applications, both aspects are important."
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#finalizing-and-evaluating-the-models",
    "href": "posts/modeling-with-tidymodels.html#finalizing-and-evaluating-the-models",
    "title": "Building Models in R with tidymodels",
    "section": "Finalizing and Evaluating the Models",
    "text": "Finalizing and Evaluating the Models\nWe’ll finalize both models using their best hyperparameters and evaluate them on the validation set.\n\n# Select best hyperparameters based on ROC AUC\nbest_log_reg_params &lt;- select_best(log_reg_tuned, metric = \"roc_auc\")\nbest_xgb_params &lt;- select_best(xgb_tuned, metric = \"roc_auc\")\n\n# Finalize workflows with best parameters\nfinal_log_reg_workflow &lt;- log_reg_workflow %&gt;%\n  finalize_workflow(best_log_reg_params)\n\nfinal_xgb_workflow &lt;- xgb_workflow %&gt;%\n  finalize_workflow(best_xgb_params)\n\n# Fit the final models on the full training data\nfinal_log_reg_model &lt;- final_log_reg_workflow %&gt;%\n  fit(data = training(validation_split))\n\nfinal_xgb_model &lt;- final_xgb_workflow %&gt;%\n  fit(data = training(validation_split))\n\n# Make predictions on the validation set with both models\nlog_reg_val_results &lt;- final_log_reg_model %&gt;%\n  predict(testing(validation_split)) %&gt;%\n  bind_cols(predict(final_log_reg_model, testing(validation_split), type = \"prob\")) %&gt;%\n  bind_cols(testing(validation_split) %&gt;% select(default, customer_id))\n\nxgb_val_results &lt;- final_xgb_model %&gt;%\n  predict(testing(validation_split)) %&gt;%\n  bind_cols(predict(final_xgb_model, testing(validation_split), type = \"prob\")) %&gt;%\n  bind_cols(testing(validation_split) %&gt;% select(default, customer_id))\n\n# Evaluate model performance on validation set\nlog_reg_val_metrics &lt;- log_reg_val_results %&gt;%\n  metrics(truth = default, estimate = .pred_class, .pred_yes)\n\nxgb_val_metrics &lt;- xgb_val_results %&gt;%\n  metrics(truth = default, estimate = .pred_class, .pred_yes)\n\ncat(\"Logistic Regression Validation Metrics:\\n\")\n\nLogistic Regression Validation Metrics:\n\nprint(log_reg_val_metrics)\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.950\n2 kap         binary         0.145\n3 mn_log_loss binary         3.64 \n4 roc_auc     binary         0.157\n\ncat(\"\\nXGBoost Validation Metrics:\\n\")\n\n\nXGBoost Validation Metrics:\n\nprint(xgb_val_metrics)\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary        0.948 \n2 kap         binary        0.0339\n3 mn_log_loss binary        4.82  \n4 roc_auc     binary        0.158"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#feature-importance-and-model-interpretation",
    "href": "posts/modeling-with-tidymodels.html#feature-importance-and-model-interpretation",
    "title": "Building Models in R with tidymodels",
    "section": "Feature Importance and Model Interpretation",
    "text": "Feature Importance and Model Interpretation\nUnderstanding which features drive the predictions is crucial for credit scoring models.\n#| label: feature-importance #| fig-width: 10 #| fig-height: 12"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#final-evaluation-on-test-set",
    "href": "posts/modeling-with-tidymodels.html#final-evaluation-on-test-set",
    "title": "Building Models in R with tidymodels",
    "section": "Final Evaluation on Test Set",
    "text": "Final Evaluation on Test Set\nNow let’s evaluate our best model (XGBoost) on the held-out test set for an unbiased assessment of its performance.\n#| label: final-evaluation"
  },
  {
    "objectID": "posts/modeling-with-tidymodels.html#conclusion",
    "href": "posts/modeling-with-tidymodels.html#conclusion",
    "title": "Building Models in R with tidymodels",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we’ve demonstrated how to build a credit scoring model using the tidymodels framework. We covered:\n\nCreating a realistic credit dataset with domain-specific features\nImplementing a comprehensive feature engineering pipeline\nTraining and tuning both traditional (logistic regression) and modern (XGBoost) models\nEvaluating model performance using industry-standard metrics\nCreating interpretable visualizations of model results\n\nThe tidymodels framework provides a consistent and modular approach to building machine learning models, making it easier to experiment with different algorithms and preprocessing steps while maintaining good statistical practices."
  }
]