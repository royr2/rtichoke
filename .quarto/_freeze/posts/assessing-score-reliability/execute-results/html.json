{
  "hash": "1815f59d8bd14dba3190041aa5e767bd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Understanding Variability in Credit Score Predictions\"\ndate: \"2024-11-15\"\ncategories: [R, Credit Risk Analytics, Bootstrapping]\nimage: \"../images/score_reliability.png\"\nexecute:\n  echo: true\n  warning: false\n  message: false\n  eval: true\n---\n\n\n\nCredit scoring models work well in the middle of the score distribution but often become less reliable at the extremes where data is sparse. This post shows how to use bootstrapping to measure prediction variability across different score ranges – helping you identify where your model is most dependable.\n\n## Why Estimation Variance Matters\n\nSmaller sample sizes lead to higher variance in estimates, especially for extreme values. While statistics like means remain stable with limited data, tail percentiles (95th, 99th) show much more volatility. This matters for credit scoring, where very high and very low scores represent these unstable tail regions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Number of samples to be drawn from a probability distribution\nn_samples <- 1000\n\n# Number of times, sampling should be repeated\nrepeats <- 100\n\n# Mean and std-dev for a standard normal distribution\nmu <- 5\nstd_dev <- 2\n\n# Sample\nsamples <- rnorm(n_samples * repeats, mean = 10)\n\n# Fit into a matrix like object with `n_samples' number of rows \n# and `repeats` number of columns\nsamples <- matrix(samples, nrow = n_samples, ncol = repeats)\n\n# Compute mean across each column\nsample_means <- apply(samples, 1, mean)\n\n# Similarly, compute 75% and 95% quantile across each column\nsample_75_quantile <- apply(samples, 1, quantile, p = 0.75)\nsample_95_quantile <- apply(samples, 1, quantile, p = 0.95)\nsample_99_quantile <- apply(samples, 1, quantile, p = 0.99)\n\n# Compare coefficient of variation\nsd(sample_means)/mean(sample_means)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01017306\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(sample_75_quantile)/mean(sample_75_quantile)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0127586\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(sample_95_quantile)/mean(sample_75_quantile)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01873297\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot the distributions\ncombined_vec <- c(sample_means, sample_75_quantile, sample_95_quantile, sample_99_quantile)\n\nplot(density(sample_means), \n     col = \"#6F69AC\", \n     lwd = 3, \n     main = \"Estimating the mean vs tail quantiles\", \n     xlab = \"\", \n     xlim = c(min(combined_vec), max(combined_vec)))\n\nlines(density(sample_75_quantile), col = \"#95DAC1\", lwd = 3)\nlines(density(sample_95_quantile), col = \"#FFEBA1\", lwd = 3)\nlines(density(sample_99_quantile), col = \"#FD6F96\", lwd = 3)\ngrid()\n\nlegend(\"topright\", \n       fill = c(\"#6F69AC\", \"#95DAC1\", \"#FFEBA1\", \"#FD6F96\"), \n       legend = c(\"Mean\", \"75% Quantile\", \"95% Quantile\", \"99% Quantile\"), \n       cex = 0.7)\n```\n\n::: {.cell-output-display}\n![](assessing-score-reliability_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\nThe plot shows how uncertainty increases dramatically when estimating extreme values. The distribution for the mean (purple) is much narrower than for the 99th percentile (pink). This directly translates to credit scoring – where very high or low scores have greater uncertainty.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required packages\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(rsample)\nlibrary(ggplot2)\n```\n:::\n\n\n\n## Data Acquisition and Preprocessing\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load sample data (sample of the lending club data)\nsample <- read.csv(\"http://bit.ly/42ypcnJ\")\n\n# Mark which loan status will be tagged as default\ncodes <- c(\"Charged Off\", \"Does not meet the credit policy. Status:Charged Off\")\n\n# Apply above codes and create target\nsample %<>% mutate(bad_flag = ifelse(loan_status %in% codes, 1, 0))\n\n# Replace missing values with a default value\nsample[is.na(sample)] <- -1\n\n# Get summary tally\ntable(sample$bad_flag)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   0    1 \n8838 1162 \n```\n\n\n:::\n:::\n\n\n\nWe're using Lending Club data with charged-off loans marked as defaults. The class imbalance shown is typical in credit portfolios and contributes to prediction challenges at distribution extremes.\n\n## Implementing Bootstrap Resampling Strategy\n\nWe'll create 100 bootstrap samples to measure how model predictions vary across the score range. This technique creates multiple simulated datasets to measure prediction uncertainty without collecting additional data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create 100 samples\nboot_sample <- bootstraps(data = sample, times = 100)\n\nhead(boot_sample, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  splits               id          \n  <list>               <chr>       \n1 <split [10000/3707]> Bootstrap001\n2 <split [10000/3688]> Bootstrap002\n3 <split [10000/3694]> Bootstrap003\n```\n\n\n:::\n\n```{.r .cell-code}\n# Each row represents a separate bootstrapped sample with an analysis set and assessment set\nboot_sample$splits[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Analysis/Assess/Total>\n<10000/3707/10000>\n```\n\n\n:::\n\n```{.r .cell-code}\n# Show the first 5 rows and 5 columns of the first sample\nanalysis(boot_sample$splits[[1]]) %>% .[1:5, 1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     V1       id member_id loan_amnt funded_amnt\n1 99086 11295404        -1     24000       24000\n2 88986   747883        -1     30000       30000\n3 69923 21741383        -1     14000       14000\n4 30931 10588167        -1      3500        3500\n5 64867 24044848        -1      6000        6000\n```\n\n\n:::\n:::\n\n\n\nEach bootstrap sample contains random draws (with replacement) from our original data, creating slight variations that reveal model sensitivity to different data compositions.\n\n## Developing the Predictive Model Framework\n\nWe'll use logistic regression – the standard for credit risk models due to its interpretability and regulatory acceptance. Our model includes typical credit variables like loan amount, income, and credit history metrics.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_model <- function(df){\n  \n  # Fit a simple model with a set specification\n  mdl <- glm(bad_flag ~\n               loan_amnt + funded_amnt + annual_inc + delinq_2yrs +\n               inq_last_6mths + mths_since_last_delinq + fico_range_low +\n               mths_since_last_record + revol_util + total_pymnt,\n             family = \"binomial\",\n             data = df)\n  \n  # Return fitted values\n  return(predict(mdl))\n}\n\n# Test the function\n# Retrieve a data frame\ntrain <- analysis(boot_sample$splits[[1]])\n\n# Predict\npred <- glm_model(train)\n\n# Check output\nrange(pred)  # Output is on log odds scale\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -20.022728   1.124628\n```\n\n\n:::\n:::\n\n\n\nThe function returns predictions in log-odds, which we'll later convert to a more intuitive credit score scale.\n\n## Iterative Model Training and Prediction Collection\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First apply the glm fitting function to each of the sample\n# Note the use of lapply\noutput <- lapply(boot_sample$splits, function(x){\n  train <- analysis(x)\n  pred <- glm_model(train)\n\n  return(pred)\n})\n\n# Collate all predictions into a vector \nboot_preds <- do.call(c, output)\nrange(boot_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -141.189195    8.873295\n```\n\n\n:::\n\n```{.r .cell-code}\n# Get outliers\nq_high <- quantile(boot_preds, 0.99)\nq_low <- quantile(boot_preds, 0.01)\n\n# Truncate the overall distribution to within the lower 1% and upper 1% quantiles\n# Doing this since it creates issues later on when scaling the output\nboot_preds[boot_preds > q_high] <- q_high\nboot_preds[boot_preds < q_low] <- q_low\n\nrange(boot_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -5.0581226 -0.2312987\n```\n\n\n:::\n\n```{.r .cell-code}\n# Convert to a data frame\nboot_preds <- data.frame(pred = boot_preds, \n                         id = rep(1:length(boot_sample$splits), each = nrow(sample)))\nhead(boot_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        pred id\n1 -3.5243889  1\n2 -5.0220877  1\n3 -1.7385793  1\n4 -1.9862655  1\n5 -1.5898113  1\n6 -0.5679697  1\n```\n\n\n:::\n:::\n\n\n\nWe apply our model to each bootstrap sample and collect the predictions, then truncate extreme values (beyond 1st and 99th percentiles) to remove outliers – similar to capping techniques used in production credit models.\n\n## Transforming Predictions to Credit Score Scale\n\nNow we'll convert log-odds to a recognizable credit score using the industry-standard Points to Double Odds (PDO) method. Using parameters similar to real credit systems (PDO=30, Anchor=700), we transform our predictions into intuitive scores where higher numbers indicate lower risk.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscaling_func <- function(vec, PDO = 30, OddsAtAnchor = 5, Anchor = 700){\n  beta <- PDO / log(2)\n  alpha <- Anchor - PDO * OddsAtAnchor\n  \n  # Simple linear scaling of the log odds\n  scr <- alpha - beta * vec  \n  \n  # Round off\n  return(round(scr, 0))\n}\n\nboot_preds$scores <- scaling_func(boot_preds$pred, 30, 2, 700)\n\n# Chart the distribution of predictions across all the samples\nggplot(boot_preds, aes(x = scores, color = factor(id))) + \n  geom_density() + \n  theme_minimal() + \n  theme(legend.position = \"none\") + \n  scale_color_grey() + \n  labs(title = \"Predictions from bootstrapped samples\", \n       subtitle = \"Density function\", \n       x = \"Predictions (Log odds)\", \n       y = \"Density\")\n```\n\n::: {.cell-output-display}\n![](assessing-score-reliability_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nEach gray line shows the score distribution from a different bootstrap sample. Where lines cluster tightly, our predictions are stable; where they diverge, we have higher uncertainty.\n\n## Quantifying Prediction Uncertainty Across Score Ranges\n\nNow we can directly measure how prediction reliability varies across score ranges by calculating standard deviation within each score bin. This approach quantifies uncertainty at different score levels.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create bins using quantiles\nbreaks <- quantile(boot_preds$scores, probs = seq(0, 1, length.out = 20))\nboot_preds$bins <- cut(boot_preds$scores, breaks = unique(breaks), include.lowest = T, right = T)\n\n# Chart standard deviation of model predictions across each score bin\nboot_preds %>%\n  group_by(bins) %>%\n  summarise(std_dev = sd(scores)) %>%\n  ggplot(aes(x = bins, y = std_dev)) +\n  geom_col(color = \"black\", fill = \"#90AACB\") +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 90)) + \n  theme(legend.position = \"none\") + \n  labs(title = \"Variability in model predictions across samples\", \n       subtitle = \"(measured using standard deviation)\", \n       x = \"Score Range\", \n       y = \"Standard Deviation\")\n```\n\n::: {.cell-output-display}\n![](assessing-score-reliability_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\nAs expected, the model's predictions are more reliable within a certain range of values (700-800) whereas there is significant variability in the model's predictions in the lowest and highest score buckets.\n\nThe chart reveals a clear \"U-shape\" pattern of prediction variability—a common phenomenon in credit risk modeling. The highest uncertainty appears in the extreme score ranges (very high and very low scores), while predictions in the middle range show greater stability. The chart confirms our hypothesis: variability is highest at score extremes and lowest in the middle range (600-800). This directly informs credit policy – scores in the middle range are most reliable, while decisions at the extremes should incorporate additional caution due to higher uncertainty.\n\nThese findings have direct business applications:\n\n1. For extremely high scores, add verification steps before auto-approval\n2. For very low scores, consider manual review rather than automatic rejection\n\n## Advanced Approach: Isolating Training Data Effects\n\n*Credit: Richard Warnung*\n\nFor a more controlled analysis, we can train models on bootstrap samples but evaluate them on the same validation set. This isolates the impact of training data variation:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nVs <- function(boot_split){\n  # Train model on the bootstrapped data\n  train <- analysis(boot_split)\n  \n  # Fit model\n  mdl <- glm(bad_flag ~\n               loan_amnt + funded_amnt + annual_inc + delinq_2yrs +\n               inq_last_6mths + mths_since_last_delinq + fico_range_low +\n               mths_since_last_record + revol_util + total_pymnt,\n             family = \"binomial\",\n             data = train)\n  \n  # Apply to a common validation set\n  validate_preds <- predict(mdl, newdata = validate_set)\n  \n  # Return predictions\n  return(validate_preds)\n}\n```\n:::\n\n\n\nThis method provides a clearer picture of how variations in training data affect model predictions, which is valuable when evaluating model updates in production.\n\n\n# Create overall training and testing datasets \nid <- sample(1:nrow(sample), size = nrow(sample)*0.8, replace = F)\n\ntrain_data <- sample[id,]\ntest_data <- sample[-id,]\n\n# Bootstrapped samples are now pulled only from the overall training dataset\nboot_sample <- bootstraps(data = train_data, times = 80)\n\n# Using the same function from before but predicting on the same test dataset\nglm_model <- function(train, test){\n  \n  mdl <- glm(bad_flag ~\n               loan_amnt + funded_amnt + annual_inc + delinq_2yrs +\n               inq_last_6mths + mths_since_last_delinq + fico_range_low +\n               mths_since_last_record + revol_util + total_pymnt,\n             family = \"binomial\",\n             data = train)\n  \n  # Return fitted values on the test dataset\n  return(predict(mdl, newdata = test))\n}\n\n# Apply the glm fitting function to each of the sample\n# But predict on the same test dataset\noutput <- lapply(boot_sample$splits, function(x){\n  train <- analysis(x)\n  pred <- glm_model(train, test_data)\n\n  return(pred)\n})\n",
    "supporting": [
      "assessing-score-reliability_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}