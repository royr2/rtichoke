{
  "hash": "4c83d9e139d2bea4a6962198ec395582",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Tutorial: Assessing Credit Score Prediction Reliability Using Bootstrap Resampling\"\ndate: \"2024-11-15\"\ncategories: [R, Credit Risk Analytics, Bootstrapping]\nimage: \"../images/score_reliability.png\"\nexecute:\n  echo: true\n  warning: false\n  message: false\n  eval: true\n---\n\n\n\n## Introduction\n\nCredit scoring models demonstrate optimal performance within the central regions of the score distribution, yet exhibit diminished reliability at the distribution extremes where data becomes sparse. This tutorial provides a comprehensive methodology for employing bootstrap resampling techniques to quantify prediction variability across different score ranges, thereby enabling practitioners to identify regions where their models demonstrate the highest degree of dependability.\n\n## Theoretical Foundation: Understanding Estimation Variance\n\nReduced sample sizes inherently result in increased variance in statistical estimates, particularly for extreme values within a distribution. While central tendency measures such as means demonstrate relative stability when estimated from limited data, tail percentiles (95th, 99th) exhibit substantially greater volatility. This phenomenon is of critical importance in credit scoring applications, where extremely high and low scores correspond to these unstable tail regions of the distribution.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Number of samples to be drawn from a probability distribution\nn_samples <- 1000\n\n# Number of times, sampling should be repeated\nrepeats <- 100\n\n# Mean and std-dev for a standard normal distribution\nmu <- 5\nstd_dev <- 2\n\n# Sample\nsamples <- rnorm(n_samples * repeats, mean = 10)\n\n# Fit into a matrix like object with `n_samples' number of rows \n# and `repeats' number of columns\nsamples <- matrix(samples, nrow = n_samples, ncol = repeats)\n\n# Compute mean across each column\nsample_means <- apply(samples, 1, mean)\n\n# Similarly, compute 75% and 95% quantile across each column\nsample_75_quantile <- apply(samples, 1, quantile, p = 0.75)\nsample_95_quantile <- apply(samples, 1, quantile, p = 0.95)\nsample_99_quantile <- apply(samples, 1, quantile, p = 0.99)\n\n# Compare coefficient of variation\nsd(sample_means)/mean(sample_means)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01066065\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(sample_75_quantile)/mean(sample_75_quantile)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01284219\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(sample_95_quantile)/mean(sample_75_quantile)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01930166\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot the distributions\ncombined_vec <- c(sample_means, sample_75_quantile, sample_95_quantile, sample_99_quantile)\n\nplot(density(sample_means), \n     col = \"#6F69AC\", \n     lwd = 3, \n     main = \"Estimating the mean vs tail quantiles\", \n     xlab = \"\", \n     xlim = c(min(combined_vec), max(combined_vec)))\n\nlines(density(sample_75_quantile), col = \"#95DAC1\", lwd = 3)\nlines(density(sample_95_quantile), col = \"#FFEBA1\", lwd = 3)\nlines(density(sample_99_quantile), col = \"#FD6F96\", lwd = 3)\ngrid()\n\nlegend(\"topright\", \n       fill = c(\"#6F69AC\", \"#95DAC1\", \"#FFEBA1\", \"#FD6F96\"), \n       legend = c(\"Mean\", \"75% Quantile\", \"95% Quantile\", \"99% Quantile\"), \n       cex = 0.7)\n```\n\n::: {.cell-output-display}\n![](assessing-score-reliability_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\nThe visualization demonstrates the substantial increase in uncertainty when estimating extreme values compared to central tendencies. The distribution corresponding to the mean (purple) exhibits considerably narrower dispersion than that of the 99th percentile (pink). This statistical principle directly applies to credit scoring applications, where scores at the extremes of the distribution inherently possess greater uncertainty.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required packages\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(rsample)\nlibrary(ggplot2)\n```\n:::\n\n\n\n## Data Acquisition and Preprocessing\n\nIn this tutorial, we will utilize a sample from the Lending Club dataset. Loans classified as \"Charged Off\" will be designated as defaults. The observed class imbalance represents a typical characteristic of credit portfolios and contributes significantly to prediction challenges at distribution extremes.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load sample data (sample of the lending club data)\nsample <- read.csv(\"http://bit.ly/42ypcnJ\")\n\n# Mark which loan status will be tagged as default\ncodes <- c(\"Charged Off\", \"Does not meet the credit policy. Status:Charged Off\")\n\n# Apply above codes and create target\nsample %<>% mutate(bad_flag = ifelse(loan_status %in% codes, 1, 0))\n\n# Replace missing values with a default value\nsample[is.na(sample)] <- -1\n\n# Get summary tally\ntable(sample$bad_flag)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   0    1 \n8838 1162 \n```\n\n\n:::\n:::\n\n\n\n## Implementing Bootstrap Resampling Strategy\n\nThis section demonstrates the creation of 100 bootstrap samples to quantify the variation in model predictions across different score ranges. Bootstrap resampling is a statistical technique that generates multiple simulated datasets to measure prediction uncertainty without requiring additional data collection.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create 100 samples\nboot_sample <- bootstraps(data = sample, times = 100)\n\nhead(boot_sample, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  splits               id          \n  <list>               <chr>       \n1 <split [10000/3687]> Bootstrap001\n2 <split [10000/3685]> Bootstrap002\n3 <split [10000/3708]> Bootstrap003\n```\n\n\n:::\n\n```{.r .cell-code}\n# Each row represents a separate bootstrapped sample with an analysis set and assessment set\nboot_sample$splits[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Analysis/Assess/Total>\n<10000/3687/10000>\n```\n\n\n:::\n\n```{.r .cell-code}\n# Show the first 5 rows and 5 columns of the first sample\nanalysis(boot_sample$splits[[1]]) %>% .[1:5, 1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     V1        id member_id loan_amnt funded_amnt\n1 84100 132076701        -1      5000        5000\n2 50645   8608023        -1     14000       14000\n3 85403 129369618        -1     24000       24000\n4 51995   6939230        -1      6000        6000\n5  6032 141964884        -1      5000        5000\n```\n\n\n:::\n:::\n\n\n\nEach bootstrap sample consists of random draws with replacement from the original dataset, creating controlled variations that effectively reveal model sensitivity to different data compositions.\n\n## Developing the Predictive Model Framework\n\nThis tutorial employs logistic regression as the predictive modeling technique. Logistic regression represents the industry standard for credit risk modeling due to its interpretability and regulatory acceptance. The model specification incorporates typical credit variables including loan amount, income, and credit history metrics.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_model <- function(df){\n  \n  # Fit a simple model with a set specification\n  mdl <- glm(bad_flag ~\n               loan_amnt + funded_amnt + annual_inc + delinq_2yrs +\n               inq_last_6mths + mths_since_last_delinq + fico_range_low +\n               mths_since_last_record + revol_util + total_pymnt,\n             family = \"binomial\",\n             data = df)\n  \n  # Return fitted values\n  return(predict(mdl))\n}\n\n# Test the function\n# Retrieve a data frame\ntrain <- analysis(boot_sample$splits[[1]])\n\n# Predict\npred <- glm_model(train)\n\n# Check output\nrange(pred)  # Output is on log odds scale\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -12.9580782   0.9594826\n```\n\n\n:::\n:::\n\n\n\nThe function returns predictions in log-odds format, which will subsequently be transformed to a more intuitive credit score scale in later steps.\n\n## Iterative Model Training and Prediction Collection\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First apply the glm fitting function to each of the sample\n# Note the use of lapply\noutput <- lapply(boot_sample$splits, function(x){\n  train <- analysis(x)\n  pred <- glm_model(train)\n\n  return(pred)\n})\n\n# Collate all predictions into a vector \nboot_preds <- do.call(c, output)\nrange(boot_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -131.960420    7.180581\n```\n\n\n:::\n\n```{.r .cell-code}\n# Get outliers\nq_high <- quantile(boot_preds, 0.99)\nq_low <- quantile(boot_preds, 0.01)\n\n# Truncate the overall distribution to within the lower 1% and upper 1% quantiles\n# Doing this since it creates issues later on when scaling the output\nboot_preds[boot_preds > q_high] <- q_high\nboot_preds[boot_preds < q_low] <- q_low\n\nrange(boot_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -5.0380956 -0.2176088\n```\n\n\n:::\n\n```{.r .cell-code}\n# Convert to a data frame\nboot_preds <- data.frame(pred = boot_preds, \n                         id = rep(1:length(boot_sample$splits), each = nrow(sample)))\nhead(boot_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       pred id\n1 -1.853830  1\n2 -2.345015  1\n3 -1.430748  1\n4 -2.312299  1\n5 -1.712506  1\n6 -1.101160  1\n```\n\n\n:::\n:::\n\n\n\nIn this step, we apply the logistic regression model to each bootstrap sample and systematically collect the resulting predictions. Subsequently, we truncate extreme values (beyond the 1st and 99th percentiles) to remove outliers—a procedure analogous to capping techniques commonly employed in production credit models.\n\n## Transforming Predictions to Credit Score Scale\n\nThis section demonstrates the conversion of log-odds predictions to a recognizable credit score format using the industry-standard Points to Double Odds (PDO) methodology. By employing parameters consistent with real-world credit systems (PDO=30, Anchor=700), we transform the model predictions into intuitive scores where higher numerical values indicate lower credit risk.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscaling_func <- function(vec, PDO = 30, OddsAtAnchor = 5, Anchor = 700){\n  beta <- PDO / log(2)\n  alpha <- Anchor - PDO * OddsAtAnchor\n  \n  # Simple linear scaling of the log odds\n  scr <- alpha - beta * vec  \n  \n  # Round off\n  return(round(scr, 0))\n}\n\nboot_preds$scores <- scaling_func(boot_preds$pred, 30, 2, 700)\n\n# Chart the distribution of predictions across all the samples\nggplot(boot_preds, aes(x = scores, color = factor(id))) + \n  geom_density() + \n  theme_minimal() + \n  theme(legend.position = \"none\") + \n  scale_color_grey() + \n  labs(title = \"Predictions from bootstrapped samples\", \n       subtitle = \"Density function\", \n       x = \"Predictions (Log odds)\", \n       y = \"Density\")\n```\n\n::: {.cell-output-display}\n![](assessing-score-reliability_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n## Quantifying Prediction Uncertainty Across Score Ranges\n\nThis section provides a methodology to directly measure prediction reliability variation across different score ranges through the calculation of standard deviation within each score bin. This approach enables precise quantification of uncertainty at different score levels.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create bins using quantiles\nbreaks <- quantile(boot_preds$scores, probs = seq(0, 1, length.out = 20))\nboot_preds$bins <- cut(boot_preds$scores, breaks = unique(breaks), include.lowest = T, right = T)\n\n# Chart standard deviation of model predictions across each score bin\nboot_preds %>%\n  group_by(bins) %>%\n  summarise(std_dev = sd(scores)) %>%\n  ggplot(aes(x = bins, y = std_dev)) +\n  geom_col(color = \"black\", fill = \"#90AACB\") +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 90)) + \n  theme(legend.position = \"none\") + \n  labs(title = \"Variability in model predictions across samples\", \n       subtitle = \"(measured using standard deviation)\", \n       x = \"Score Range\", \n       y = \"Standard Deviation\")\n```\n\n::: {.cell-output-display}\n![](assessing-score-reliability_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\nAs anticipated, the model's predictions demonstrate enhanced reliability within a specific range of values (700-800), while exhibiting significant variability in the lowest and highest score buckets.\n\nThe visualization reveals a characteristic \"U-shaped\" pattern of prediction variability—a well-documented phenomenon in credit risk modeling. The highest uncertainty manifests in the extreme score ranges (very high and very low scores), while predictions in the middle range demonstrate greater stability. The analysis confirms the initial hypothesis: variability reaches its maximum at score extremes and achieves its minimum in the middle range (600-800). This finding provides direct guidance for credit policy development—scores in the middle range demonstrate the highest reliability, while decisions at the extremes should incorporate additional caution due to elevated uncertainty.\n\n### Practical Business Applications\n\nThese findings yield direct business applications:\n\n1. **High Score Management**: For extremely high scores, implement additional verification steps before automated approval\n2. **Low Score Management**: For very low scores, consider manual review procedures rather than automatic rejection\n\n## Advanced Methodology: Isolating Training Data Effects\n\n*Credit: Richard Warnung*\n\nFor enhanced analytical control, this section presents an alternative approach where models are trained on bootstrap samples while being evaluated on a consistent validation set. This methodology effectively isolates the impact of training data variation on model predictions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nVs <- function(boot_split){\n  # Train model on the bootstrapped data\n  train <- analysis(boot_split)\n  \n  # Fit model\n  mdl <- glm(bad_flag ~\n               loan_amnt + funded_amnt + annual_inc + delinq_2yrs +\n               inq_last_6mths + mths_since_last_delinq + fico_range_low +\n               mths_since_last_record + revol_util + total_pymnt,\n             family = \"binomial\",\n             data = train)\n  \n  # Apply to a common validation set\n  validate_preds <- predict(mdl, newdata = validate_set)\n  \n  # Return predictions\n  return(validate_preds)\n}\n```\n:::\n\n\n\nThis methodology provides enhanced insight into how variations in training data composition affect model predictions, which proves valuable when evaluating model updates in production environments.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create overall training and testing datasets \nid <- sample(1:nrow(sample), size = nrow(sample)*0.8, replace = F)\n\ntrain_data <- sample[id,]\ntest_data <- sample[-id,]\n\n# Bootstrapped samples are now pulled only from the overall training dataset\nboot_sample <- bootstraps(data = train_data, times = 80)\n\n# Using the same function from before but predicting on the same test dataset\nglm_model <- function(train, test){\n  \n  mdl <- glm(bad_flag ~\n               loan_amnt + funded_amnt + annual_inc + delinq_2yrs +\n               inq_last_6mths + mths_since_last_delinq + fico_range_low +\n               mths_since_last_record + revol_util + total_pymnt,\n             family = \"binomial\",\n             data = train)\n  \n  # Return fitted values on the test dataset\n  return(predict(mdl, newdata = test))\n}\n\n# Apply the glm fitting function to each of the sample\n# But predict on the same test dataset\noutput <- lapply(boot_sample$splits, function(x){\n  train <- analysis(x)\n  pred <- glm_model(train, test_data)\n\n  return(pred)\n})\n```\n:::\n\n\n\n## Key Takeaways\n\n- **Prediction reliability varies significantly across score ranges**, with highest uncertainty at distribution extremes\n- **Bootstrap methodology provides a practical framework** for measuring model uncertainty without additional data requirements  \n- **Risk management strategies should be adjusted** based on score reliability, with enhanced verification for extreme scores\n- **The PDO transformation enables intuitive score interpretation** while preserving the underlying risk relationships\n\nThese techniques can be applied beyond credit scoring to any prediction problem where understanding model uncertainty is critical for decision-making.\n",
    "supporting": [
      "assessing-score-reliability_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}