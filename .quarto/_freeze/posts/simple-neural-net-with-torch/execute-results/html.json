{
  "hash": "bb25aedf889ce094e6f151a1edf2e787",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Building a Simple Neural Network in R with torch\"\ndate: \"2024-12-05\"\ncategories: [R, Deep Learning, torch]\nimage: \"../images/nn.png\"\nexecute:\n  echo: true\n  warning: false\n  message: false\n  eval: true\n---\n\n\n\n\nThe `torch` package brings the power of deep learning to R by providing bindings to the popular PyTorch library. In this post, you'll learn how to build and train a simple neural network using `torch` in R.\n\n## Installation\n\nTo get started, install the `torch` package from CRAN and set up the backend:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"torch\")\nlibrary(torch)\n# torch::install_torch()\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n## Creating a Simple Neural Network\n\nLet's create a neural network to perform regression on a simple dataset (predicting `y` from `x`).\n\n### 1. Generate Sample Data\n\nWe'll start by creating some synthetic data with a linear relationship plus some noise:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed for reproducibility\nset.seed(42)\n\n# Generate training data: y = 3x + 2 + noise\nx <- torch_randn(100, 1)\ny <- 3 * x + 2 + torch_randn(100, 1) * 0.3\n\n# Display the first few data points\nhead(data.frame(\n  x = as.numeric(x$squeeze()),\n  y = as.numeric(y$squeeze())\n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           x          y\n1  1.9790213  7.8955870\n2 -1.1587162 -1.2231925\n3 -0.4262840  0.9012265\n4  0.6651266  3.9386432\n5  0.2999833  3.2916081\n6  0.1497405  2.1887689\n```\n\n\n:::\n:::\n\n\n\n\n### 2. Define the Neural Network Module\n\nNow we'll define our neural network architecture using `torch`'s module system:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define a simple feedforward neural network\nnet <- nn_module(\n  initialize = function() {\n    # Define layers\n    self$fc1 <- nn_linear(1, 8)  # Input layer to hidden layer (1 -> 8 neurons)\n    self$fc2 <- nn_linear(8, 1)  # Hidden layer to output layer (8 -> 1 neuron)\n  },\n  forward = function(x) {\n    # Define forward pass\n    x %>% \n      self$fc1() %>%     # First linear transformation\n      nnf_relu() %>%     # ReLU activation function\n      self$fc2()         # Second linear transformation\n  }\n)\n\n# Instantiate the model\nmodel <- net()\n\n# Display model structure\nprint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn `nn_module` containing 25 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────\n• fc1: <nn_linear> #16 parameters\n• fc2: <nn_linear> #9 parameters\n```\n\n\n:::\n:::\n\n\n\n\n### 3. Set Up the Optimizer and Loss Function\n\nWe need to define how our model will learn from the data:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set up optimizer (Adam optimizer with learning rate 0.01)\noptimizer <- optim_adam(model$parameters, lr = 0.01)\n\n# Define loss function (Mean Squared Error for regression)\nloss_fn <- nnf_mse_loss\n\ncat(\"Optimizer:\", class(optimizer)[1], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOptimizer: optim_adam \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Loss function:\", \"Mean Squared Error\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLoss function: Mean Squared Error\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Learning rate:\", 0.01, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLearning rate: 0.01 \n```\n\n\n:::\n:::\n\n\n\n\n### 4. Training Loop\n\nNow we'll train our neural network:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Store loss values for plotting\nloss_history <- numeric(300)\n\n# Training loop\nfor(epoch in 1:300) {\n  # Set model to training mode\n  model$train()\n  \n  # Reset gradients\n  optimizer$zero_grad()\n  \n  # Forward pass\n  y_pred <- model(x)\n  \n  # Calculate loss\n  loss <- loss_fn(y_pred, y)\n  \n  # Backward pass\n  loss$backward()\n  \n  # Update parameters\n  optimizer$step()\n  \n  # Store loss for plotting\n  loss_history[epoch] <- loss$item()\n  \n  # Print progress every 50 epochs\n  if(epoch %% 50 == 0) {\n    cat(sprintf(\"Epoch %d, Loss: %.6f\\n\", epoch, loss$item()))\n  }\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEpoch 50, Loss: 0.913185\nEpoch 100, Loss: 0.136346\nEpoch 150, Loss: 0.094510\nEpoch 200, Loss: 0.091030\nEpoch 250, Loss: 0.087645\nEpoch 300, Loss: 0.084433\n```\n\n\n:::\n:::\n\n\n\n\n### 5. Visualize the Training Progress\n\nLet's see how the loss decreased during training:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a data frame for plotting\ntraining_df <- data.frame(\n  epoch = 1:300,\n  loss = loss_history\n)\n\n# Plot training loss\nggplot(training_df, aes(x = epoch, y = loss)) +\n  geom_line(color = \"#2c3e50\", size = 1) +\n  labs(\n    title = \"Training Loss Over Time\",\n    subtitle = \"Neural Network Learning Progress\",\n    x = \"Epoch\",\n    y = \"Mean Squared Error Loss\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12, color = \"gray60\")\n  )\n```\n\n::: {.cell-output-display}\n![](simple-neural-net-with-torch_files/figure-html/training-progress-1.png){width=960}\n:::\n:::\n\n\n\n\n### 6. Visualize the Results\n\nNow let's see how well our trained model performs:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set model to evaluation mode\nmodel$eval()\n\n# Generate predictions\nwith_no_grad({\n  y_pred <- model(x)\n})\n\n# Convert to R vectors for plotting\nx_np <- as.numeric(x$squeeze())\ny_np <- as.numeric(y$squeeze())\ny_pred_np <- as.numeric(y_pred$squeeze())\n\n# Create data frame for ggplot\nplot_df <- data.frame(\n  x = x_np,\n  y_actual = y_np,\n  y_predicted = y_pred_np\n)\n\n# Create the plot\nggplot(plot_df, aes(x = x)) +\n  geom_point(aes(y = y_actual, color = \"Actual\"), alpha = 0.7, size = 2) +\n  geom_point(aes(y = y_predicted, color = \"Predicted\"), alpha = 0.7, size = 2) +\n  geom_smooth(aes(y = y_predicted), method = \"loess\", se = FALSE, \n              color = \"#e74c3c\", linetype = \"dashed\") +\n  labs(\n    title = \"Neural Network Regression Results\",\n    subtitle = \"Comparing actual vs predicted values\",\n    x = \"Input (x)\",\n    y = \"Output (y)\",\n    color = \"Data Type\"\n  ) +\n  scale_color_manual(values = c(\"Actual\" = \"#3498db\", \"Predicted\" = \"#e74c3c\")) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12, color = \"gray60\"),\n    legend.position = \"top\"\n  )\n```\n\n::: {.cell-output-display}\n![](simple-neural-net-with-torch_files/figure-html/visualize-results-1.png){width=960}\n:::\n:::\n\n\n\n\n### 7. Model Performance Analysis\n\nLet's analyze how well our model learned the underlying pattern:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate performance metrics\nmse <- mean((y_pred_np - y_np)^2)\nrmse <- sqrt(mse)\nmae <- mean(abs(y_pred_np - y_np))\nr_squared <- cor(y_pred_np, y_np)^2\n\n# Create performance summary\nperformance_summary <- data.frame(\n  Metric = c(\"Mean Squared Error\", \"Root Mean Squared Error\", \n             \"Mean Absolute Error\", \"R-squared\"),\n  Value = c(mse, rmse, mae, r_squared)\n)\n\nprint(performance_summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   Metric      Value\n1      Mean Squared Error 0.08440672\n2 Root Mean Squared Error 0.29052835\n3     Mean Absolute Error 0.23036660\n4               R-squared 0.99102243\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare with true relationship (y = 3x + 2)\n# Generate predictions on a grid for comparison\nx_grid <- torch_linspace(-3, 3, 100)$unsqueeze(2)\nwith_no_grad({\n  y_grid_pred <- model(x_grid)\n})\n\nx_grid_np <- as.numeric(x_grid$squeeze())\ny_grid_pred_np <- as.numeric(y_grid_pred$squeeze())\ny_grid_true <- 3 * x_grid_np + 2\n\n# Plot comparison\ncomparison_df <- data.frame(\n  x = x_grid_np,\n  y_true = y_grid_true,\n  y_predicted = y_grid_pred_np\n)\n\nggplot(comparison_df, aes(x = x)) +\n  geom_line(aes(y = y_true, color = \"True Function\"), size = 2) +\n  geom_line(aes(y = y_predicted, color = \"Neural Network\"), size = 2, linetype = \"dashed\") +\n  geom_point(data = plot_df, aes(y = y_actual), alpha = 0.3, color = \"gray50\") +\n  labs(\n    title = \"Neural Network vs True Function\",\n    subtitle = \"How well did our model learn the underlying pattern?\",\n    x = \"Input (x)\",\n    y = \"Output (y)\",\n    color = \"Function Type\"\n  ) +\n  scale_color_manual(values = c(\"True Function\" = \"#2c3e50\", \"Neural Network\" = \"#e74c3c\")) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12, color = \"gray60\"),\n    legend.position = \"top\"\n  )\n```\n\n::: {.cell-output-display}\n![](simple-neural-net-with-torch_files/figure-html/performance-analysis-1.png){width=672}\n:::\n:::\n\n\n\n\n## Understanding the Neural Network\n\nLet's examine what our network learned by looking at its parameters:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract learned parameters\nfc1_weight <- as.matrix(model$fc1$weight$detach())\nfc1_bias <- as.numeric(model$fc1$bias$detach())\nfc2_weight <- as.matrix(model$fc2$weight$detach())\nfc2_bias <- as.numeric(model$fc2$bias$detach())\n\ncat(\"First layer (fc1) parameters:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFirst layer (fc1) parameters:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Weight matrix shape:\", dim(fc1_weight), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWeight matrix shape: 8 1 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Bias vector length:\", length(fc1_bias), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBias vector length: 8 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Second layer (fc2) parameters:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSecond layer (fc2) parameters:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Weight matrix shape:\", dim(fc2_weight), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWeight matrix shape: 1 8 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Bias value:\", fc2_bias, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBias value: 0.4616328 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Display first layer weights and biases\ncat(\"First layer weights (sample):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFirst layer weights (sample):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(fc1_weight[1:5], 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.4869  0.6551  0.7985  1.2717 -0.2667\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nFirst layer biases (sample):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFirst layer biases (sample):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(fc1_bias[1:5], 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  0.2808  1.3207  1.0788 -0.1957 -0.8302\n```\n\n\n:::\n:::\n\n\n\n\n## Advanced: Experimenting with Different Architectures\n\nLet's compare our simple network with different architectures:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define different network architectures\ncreate_network <- function(hidden_sizes) {\n  nn_module(\n    initialize = function(hidden_sizes) {\n      self$layers <- nn_module_list()\n      \n      # Input layer\n      prev_size <- 1\n      for(i in seq_along(hidden_sizes)) {\n        self$layers$append(nn_linear(prev_size, hidden_sizes[i]))\n        prev_size <- hidden_sizes[i]\n      }\n      # Output layer\n      self$layers$append(nn_linear(prev_size, 1))\n    },\n    forward = function(x) {\n      for(i in 1:(length(self$layers) - 1)) {\n        x <- nnf_relu(self$layers[[i]](x))\n      }\n      # No activation on output layer\n      self$layers[[length(self$layers)]](x)\n    }\n  )\n}\n\n# Train different architectures\narchitectures <- list(\n  \"Simple (8)\" = c(8),\n  \"Deep (16-8)\" = c(16, 8),\n  \"Wide (32)\" = c(32),\n  \"Very Deep (16-16-8)\" = c(16, 16, 8)\n)\n\nresults <- list()\n\nfor(arch_name in names(architectures)) {\n  cat(\"Training\", arch_name, \"architecture...\\n\")\n  \n  # Create and train model\n  net_class <- create_network(architectures[[arch_name]])\n  model_temp <- net_class(architectures[[arch_name]])\n  optimizer_temp <- optim_adam(model_temp$parameters, lr = 0.01)\n  \n  # Quick training (fewer epochs for comparison)\n  for(epoch in 1:200) {\n    model_temp$train()\n    optimizer_temp$zero_grad()\n    y_pred_temp <- model_temp(x)\n    loss_temp <- loss_fn(y_pred_temp, y)\n    loss_temp$backward()\n    optimizer_temp$step()\n  }\n  \n  # Generate predictions\n  model_temp$eval()\n  with_no_grad({\n    y_pred_arch <- model_temp(x_grid)\n  })\n  \n  results[[arch_name]] <- data.frame(\n    x = x_grid_np,\n    y_pred = as.numeric(y_pred_arch$squeeze()),\n    architecture = arch_name\n  )\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining Simple (8) architecture...\nTraining Deep (16-8) architecture...\nTraining Wide (32) architecture...\nTraining Very Deep (16-16-8) architecture...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Combine results\nall_results <- do.call(rbind, results)\n\n# Plot comparison\nggplot(all_results, aes(x = x, y = y_pred, color = architecture)) +\n  geom_line(size = 1.2) +\n  geom_line(data = comparison_df, aes(y = y_true, color = \"True Function\"), \n            size = 2, linetype = \"solid\") +\n  geom_point(data = plot_df, aes(x = x, y = y_actual), \n             color = \"gray50\", alpha = 0.3, inherit.aes = FALSE) +\n  labs(\n    title = \"Comparison of Different Neural Network Architectures\",\n    subtitle = \"How network depth and width affect learning\",\n    x = \"Input (x)\",\n    y = \"Output (y)\",\n    color = \"Architecture\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12, color = \"gray60\"),\n    legend.position = \"top\"\n  )\n```\n\n::: {.cell-output-display}\n![](simple-neural-net-with-torch_files/figure-html/architecture-comparison-1.png){width=1152}\n:::\n:::\n\n\n\n\n## Key Takeaways\n\nThis example demonstrates several important concepts in neural network development with `torch`:\n\n1. **Simple Architecture**: Even a simple 2-layer network can learn complex patterns\n2. **Training Process**: The importance of proper training loops with gradient computation\n3. **Visualization**: How to effectively visualize both training progress and results\n4. **Model Evaluation**: Understanding model performance through multiple metrics\n5. **Architecture Comparison**: How different network structures affect learning\n\nThe `torch` package makes it straightforward to build and experiment with neural networks in R, bringing the power of deep learning to the R ecosystem. You can extend this approach to more complex datasets and deeper architectures as needed.\n\n## Next Steps\n\nTo further explore neural networks with `torch`, consider:\n\n- Experimenting with different activation functions (sigmoid, tanh, etc.)\n- Adding regularization techniques (dropout, weight decay)\n- Working with real-world datasets\n- Implementing convolutional or recurrent neural networks\n- Using GPU acceleration for larger models\n",
    "supporting": [
      "simple-neural-net-with-torch_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}