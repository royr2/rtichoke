{
  "hash": "604e8b6632eb239b64ac6162a1f83557",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Building a Simple Neural Network in R with torch\"\ndate: \"2024-12-05\"\ncategories: [R, Deep Learning, torch]\nimage: \"../images/nn.png\"\nexecute:\n  echo: true\n  warning: false\n  message: false\n  eval: true\n---\n\n\n\nThe `torch` package brings deep learning to R by providing bindings to the popular PyTorch library. This comprehensive tutorial demonstrates how to build and train a simple neural network using `torch` in R.\n\n## Installation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"torch\")\nlibrary(torch)\n# torch::install_torch()\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n## A Simple Neural Network\n\nThis section focuses on the creation of a neural network to perform a simple regression task.\n\n### 1. Sample Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed for reproducibility\nset.seed(42)\n\n# Generate training data: y = 3x + 2 + noise\nx <- torch_randn(100, 1)\ny <- 3 * x + 2 + torch_randn(100, 1) * 0.3\n\n# Display the first few data points\nhead(\n  data.frame(\n    x = as.numeric(x$squeeze()),\n    y = as.numeric(y$squeeze())\n  ))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             x          y\n1  0.196439013  3.0960894\n2 -1.425725937 -2.3177149\n3  0.910384119  4.8918929\n4 -0.232266411  1.4088905\n5 -0.268673807  0.9713775\n6  0.006964483  2.3886244\n```\n\n\n:::\n:::\n\n\n\n### 2. Neural Network Module\n\nThe next step involves defining the neural network architecture using `torch`'s module system:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define a simple feedforward neural network\nnnet <- nn_module(\n  initialize = function() {\n    # Define layers\n    self$layer1 <- nn_linear(1, 8)  # Input layer to hidden layer (1 -> 8 neurons)\n    self$layer2 <- nn_linear(8, 1)  # Hidden layer to output layer (8 -> 1 neuron)\n  },\n  forward = function(x) {\n    # Define forward pass\n    x %>% \n      self$layer1() %>%     # First linear transformation\n      nnf_relu() %>%     # ReLU activation function\n      self$layer2()         # Second linear transformation\n  }\n)\n\n# Instantiate the model\nmodel <- nnet()\n\n# Display model structure\nprint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn `nn_module` containing 25 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────\n• layer1: <nn_linear> #16 parameters\n• layer2: <nn_linear> #9 parameters\n```\n\n\n:::\n:::\n\n\n\n### 3. Set Up the Optimizer and Loss Function\n\nThe training process requires defining how the model will learn from the data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set up optimizer (Adam optimizer with learning rate 0.02)\noptimizer <- optim_adam(model$parameters, lr = 0.02)\n\n# Define loss function (Mean Squared Error for regression)\nloss_fn <- nnf_mse_loss\n```\n:::\n\n\n\n### 4. Training Loop\n\nThe neural network training process proceeds as follows:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Store loss values for plotting\nloss_history <- numeric(300)\n\n# Training loop\nfor(epoch in 1:300) {\n  \n  # Set model to training mode\n  model$train()\n  \n  # Reset gradients\n  optimizer$zero_grad()\n  \n  # Forward pass\n  y_pred <- model(x)\n  \n  # Calculate loss\n  loss <- loss_fn(y_pred, y)\n  \n  # Backward pass\n  loss$backward()\n  \n  # Update parameters\n  optimizer$step()\n  \n  # Store loss for plotting\n  loss_history[epoch] <- loss$item()\n}\n```\n:::\n\n\n\n### 5. Visualize the Training Progress\n\nThe following visualization demonstrates how the loss decreased during training:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a data frame for plotting\ntraining_df <- data.frame(\n  epoch = 1:300,\n  loss = loss_history\n)\n\n# Plot training loss\nggplot(training_df, aes(x = epoch, y = loss)) +\n  geom_line(color = \"#2c3e50\", size = 1) +\n  labs(\n    title = \"Training Loss Over Time\",\n    subtitle = \"Neural Network Learning Progress\",\n    x = \"Epoch\",\n    y = \"Mean Squared Error Loss\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12, color = \"gray60\")\n  )\n```\n\n::: {.cell-output-display}\n![](simple-neural-net-with-torch_files/figure-html/training-progress-1.png){width=672}\n:::\n:::\n\n\n\n### 6. Visualize the Results\n\nThe following analysis demonstrates how well the trained model performs:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set model to evaluation mode\nmodel$eval()\n\n# Generate predictions\nwith_no_grad({\n  y_pred <- model(x)\n})\n\n# Convert to R vectors for plotting\nx_np <- as.numeric(x$squeeze())\ny_np <- as.numeric(y$squeeze())\ny_pred_np <- as.numeric(y_pred$squeeze())\n\n# Create data frame for ggplot\nplot_df <- data.frame(\n  x = x_np,\n  y_actual = y_np,\n  y_predicted = y_pred_np\n)\n\n# Create the plot\nggplot(plot_df, aes(x = x)) +\n  geom_point(aes(y = y_actual, color = \"Actual\"), alpha = 0.7, size = 2) +\n  geom_point(aes(y = y_predicted, color = \"Predicted\"), alpha = 0.7, size = 2) +\n  geom_smooth(aes(y = y_predicted), method = \"loess\", se = FALSE, \n              color = \"#e74c3c\", linetype = \"dashed\") +\n  labs(\n    title = \"Neural Network Regression Results\",\n    subtitle = \"Comparing actual vs predicted values\",\n    x = \"Input (x)\",\n    y = \"Output (y)\",\n    color = \"Data Type\"\n  ) +\n  scale_color_manual(values = c(\"Actual\" = \"#3498db\", \"Predicted\" = \"#e74c3c\")) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12, color = \"gray60\"),\n    legend.position = \"top\"\n  )\n```\n\n::: {.cell-output-display}\n![](simple-neural-net-with-torch_files/figure-html/visualize-results-1.png){width=672}\n:::\n:::\n\n\n\n### 7. Model Performance Analysis\n\nThe following analysis examines how well the model learned the underlying pattern:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate performance metrics\nmse <- mean((y_pred_np - y_np)^2)\nrmse <- sqrt(mse)\nmae <- mean(abs(y_pred_np - y_np))\nr_squared <- cor(y_pred_np, y_np)^2\n\n# Create performance summary\nperformance_summary <- data.frame(\n  Metric = c(\"Mean Squared Error\", \"Root Mean Squared Error\", \n             \"Mean Absolute Error\", \"R-squared\"),\n  Value = c(mse, rmse, mae, r_squared)\n)\n\nprint(performance_summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   Metric      Value\n1      Mean Squared Error 0.07149047\n2 Root Mean Squared Error 0.26737702\n3     Mean Absolute Error 0.21254823\n4               R-squared 0.99080313\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare with true relationship (y = 3x + 2)\n# Generate predictions on a grid for comparison\nx_grid <- torch_linspace(-3, 3, 100)$unsqueeze(2)\nwith_no_grad({\n  y_grid_pred <- model(x_grid)\n})\n\nx_grid_np <- as.numeric(x_grid$squeeze())\ny_grid_pred_np <- as.numeric(y_grid_pred$squeeze())\ny_grid_true <- 3 * x_grid_np + 2\n\n# Plot comparison\ncomparison_df <- data.frame(\n  x = x_grid_np,\n  y_true = y_grid_true,\n  y_predicted = y_grid_pred_np\n)\n\nggplot(comparison_df, aes(x = x)) +\n  geom_line(aes(y = y_true, color = \"True Function\"), size = 2) +\n  geom_line(aes(y = y_predicted, color = \"Neural Network\"), size = 2, linetype = \"dashed\") +\n  geom_point(data = plot_df, aes(y = y_actual), alpha = 0.3, color = \"gray50\") +  labs(\n    title = \"Neural Network vs True Function\",\n    subtitle = \"Model learning assessment against the underlying pattern\",\n    x = \"Input (x)\",\n    y = \"Output (y)\",\n    color = \"Function Type\"\n  ) +\n  scale_color_manual(values = c(\"True Function\" = \"#2c3e50\", \"Neural Network\" = \"#e74c3c\")) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12, color = \"gray60\"),\n    legend.position = \"top\"\n  )\n```\n\n::: {.cell-output-display}\n![](simple-neural-net-with-torch_files/figure-html/performance-analysis-1.png){width=672}\n:::\n:::\n\n\n\n## Understanding the Neural Network\n\nThe following examination reveals what the network learned by analyzing its parameters:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract learned parameters\nlayer1_weight <- as.matrix(model$layer1$weight$detach())\nlayer1_bias <- as.numeric(model$layer1$bias$detach())\nlayer2_weight <- as.matrix(model$layer2$weight$detach())\nlayer2_bias <- as.numeric(model$layer1$bias$detach())\n\ncat(\"First layer (fc1) parameters:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFirst layer (fc1) parameters:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Weight matrix shape:\", dim(layer1_weight), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWeight matrix shape: 8 1 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Bias vector length:\", length(layer1_bias), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBias vector length: 8 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Second layer (fc2) parameters:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSecond layer (fc2) parameters:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Weight matrix shape:\", dim(layer2_weight), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWeight matrix shape: 1 8 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Bias value:\", layer2_bias, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBias value: 0.4443707 1.079916 -0.4165982 0.1215674 0.2644976 0.01709418 -0.4878989 1.520551 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Display first layer weights and biases\ncat(\"First layer weights:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFirst layer weights:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(layer1_weight, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        [,1]\n[1,]  0.1900\n[2,]  0.5403\n[3,]  1.0284\n[4,]  0.3823\n[5,] -1.8089\n[6,]  1.3698\n[7,] -0.0015\n[8,]  0.9134\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nFirst layer biases:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFirst layer biases:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(layer2_bias, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  0.4444  1.0799 -0.4166  0.1216  0.2645  0.0171 -0.4879  1.5206\n```\n\n\n:::\n:::\n\n\n\n## Experimenting with Different Architectures\n\nThe following section analyzes the simple network against different architectures:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define different network architectures\ncreate_network <- function(hidden_sizes) {\n  nn_module(\n    initialize = function(hidden_sizes) {\n      self$layers <- nn_module_list()\n      \n      # Input layer\n      prev_size <- 1\n      \n      for(i in seq_along(hidden_sizes)) {\n        self$layers$append(nn_linear(prev_size, hidden_sizes[i]))\n        prev_size <- hidden_sizes[i]\n      }\n      # Output layer\n      self$layers$append(nn_linear(prev_size, 1))\n    },\n    forward = function(x) {\n      for(i in 1:(length(self$layers) - 1)) {\n        x <- nnf_relu(self$layers[[i]](x))\n      }\n      # No activation on output layer\n      self$layers[[length(self$layers)]](x)\n    }\n  )\n}\n\n# Train different architectures\narchitectures <- list(\n  \"Simple (8)\" = c(8),\n  \"Deep (16-8)\" = c(16, 8),\n  \"Wide (32)\" = c(32),\n  \"Very Deep (16-16-8)\" = c(16, 16, 8)\n)\n\nresults <- list()\n\nfor(arch_name in names(architectures)) {\n\n  # Create and train model\n  net_class <- create_network(architectures[[arch_name]])\n  model_temp <- net_class(architectures[[arch_name]])\n  optimizer_temp <- optim_adam(model_temp$parameters, lr = 0.01)\n  \n  # Quick training (fewer epochs for comparison)\n  for(epoch in 1:200) {\n    model_temp$train()\n    optimizer_temp$zero_grad()\n    y_pred_temp <- model_temp(x)\n    loss_temp <- loss_fn(y_pred_temp, y)\n    loss_temp$backward()\n    optimizer_temp$step()\n  }\n  \n  # Generate predictions\n  model_temp$eval()\n  with_no_grad({\n    y_pred_arch <- model_temp(x_grid)\n  })\n  \n  results[[arch_name]] <- data.frame(\n    x = x_grid_np,\n    y_pred = as.numeric(y_pred_arch$squeeze()),\n    architecture = arch_name\n  )\n}\n\n# Combine results\nall_results <- do.call(rbind, results)\n\n# Plot comparison\nggplot(all_results, aes(x = x, y = y_pred, color = architecture)) +\n  geom_line(size = 1.2) +\n  geom_line(data = comparison_df, aes(y = y_true, color = \"True Function\"), \n            size = 2, linetype = \"solid\") +\n  geom_point(data = plot_df, aes(x = x, y = y_actual), \n             color = \"gray50\", alpha = 0.3, inherit.aes = FALSE) +  labs(\n               title = \"Comparison of Different Neural Network Architectures\",\n               subtitle = \"Effects of network depth and width on learning performance\",\n               x = \"Input (x)\",\n               y = \"Output (y)\",\n               color = \"Architecture\"\n             ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12, color = \"gray60\"),\n    legend.position = \"top\"\n  )\n```\n\n::: {.cell-output-display}\n![](simple-neural-net-with-torch_files/figure-html/architecture-comparison-1.png){width=672}\n:::\n:::\n\n\n\n## Key Takeaways\n\n1. **Simple Architecture**: Even a simple 2-layer network can learn complex patterns effectively\n2. **Training Process**: The importance of proper training loops with gradient computation\n3. **Visualization**: Effective methods for visualizing both training progress and results\n4. **Model Evaluation**: Understanding model performance through multiple metrics\n5. **Architecture Comparison**: How different network structures affect learning capabilities\n\nThe `torch` package provides a straightforward approach to building and experimenting with neural networks in R, bringing the power of deep learning to the R ecosystem. This approach can be extended to more complex datasets and deeper architectures as needed.\n",
    "supporting": [
      "simple-neural-net-with-torch_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}