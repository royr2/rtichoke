{
  "hash": "535cfa1b21400b16a80e70821974da61",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Multi-Task Learning with torch in R\"\ndate: \"2025-05-11\"\ncategories: [R, Deep Learning, torch, Multi-Task Learning]\nimage: \"../images/torch.png\"\nexecute:\n  echo: true\n  warning: false\n  message: false\n  eval: true\n---\n\n\n\n\nMulti-task learning (MTL) is an approach where a single neural network model is trained to perform multiple related tasks simultaneously. This methodology can improve model generalization, reduce overfitting, and leverage shared information across tasks. This post explores  how to implement a multi-task learning model using the `torch` package in R.\n\n## Introduction\n\nMulti-task learning operates by sharing representations between related tasks, enabling models to generalize more effectively. Instead of training separate models for each task, this approach develops a single model with:\n\n- **Shared layers** that learn common features across tasks\n- **Task-specific layers** that specialize for each individual task  \n- **Multiple loss functions**, one for each task\n\nThis approach is particularly valuable when dealing with related prediction problems that can benefit from shared feature representations.\n\n## Prerequisites\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(c(\"torch\", \"tidyverse\", \"corrplot\"))\nlibrary(torch)\nlibrary(tidyverse)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n## Creating a Multi-Task Learning Model\n\nThe implementation will construct a model that simultaneously performs two related tasks:\n\n1. **Regression**: Predicting a continuous value\n2. **Classification**: Predicting a binary outcome\n\n### 1. Generate Sample Data for Multiple Tasks\n\nThe first step involves creating a synthetic dataset that contains features relevant to both tasks:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed for reproducibility\nset.seed(123)\n\n# Number of samples\nn <- 1000\n\n# Create a dataset with 5 features\nx <- torch_randn(n, 5)\n\n# Task 1 (Regression): Predict continuous value\n# Create a target that's a function of the input features plus some noise\ny_regression <- x[, 1] * 0.7 + x[, 2] * 0.3 - x[, 3] * 0.5 + torch_randn(n) * 0.2\n\n# Task 2 (Classification): Predict binary outcome\n# Create a classification target based on a nonlinear combination of features\nlogits <- x[, 1] * 0.8 - x[, 4] * 0.4 + x[, 5] * 0.6\ny_classification <- (logits > 0)$to(torch_float())\n\n# Split into training (70%) and testing (30%) sets\ntrain_idx <- 1:round(0.7 * n)\ntest_idx <- (round(0.7 * n) + 1):n\n\n# Training data\nx_train <- x[train_idx, ]\ny_reg_train <- y_regression[train_idx]\ny_cls_train <- y_classification[train_idx]\n\n# Testing data\nx_test <- x[test_idx, ]\ny_reg_test <- y_regression[test_idx]\ny_cls_test <- y_classification[test_idx]\n```\n:::\n\n\n\n\n### 2. Exploratory Data Analysis\n\nThe next step examines the relationships between features and targets:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert to R matrices for analysis\nx_r <- as.matrix(x)\ny_reg_r <- as.numeric(y_regression)\ny_cls_r <- as.numeric(y_classification)\n\n# Create a comprehensive dataset for analysis\nanalysis_df <- data.frame(\n  Feature1 = x_r[, 1],\n  Feature2 = x_r[, 2],\n  Feature3 = x_r[, 3],\n  Feature4 = x_r[, 4],\n  Feature5 = x_r[, 5],\n  Regression_Target = y_reg_r,\n  Classification_Target = y_cls_r\n)\n\n# Correlation analysis\ncor_matrix <- cor(analysis_df)\ncorrplot(cor_matrix, method = \"color\", type = \"upper\", \n         tl.col = \"black\", tl.srt = 45, addCoef.col = \"black\",\n         title = \"Feature and Target Correlations\", mar = c(0,0,1,0))\n```\n\n::: {.cell-output-display}\n![](multi-task-learning-with-torch_files/figure-html/exploratory-analysis-1.png){width=1152}\n:::\n\n```{.r .cell-code}\n# Visualize feature distributions by classification target\nlong_features <- analysis_df %>%\n  select(-Regression_Target) %>%\n  pivot_longer(cols = starts_with(\"Feature\"), \n               names_to = \"Feature\", values_to = \"Value\") %>%\n  mutate(Class = factor(Classification_Target, labels = c(\"Class 0\", \"Class 1\")))\n\nggplot(long_features, aes(x = Value, fill = Class)) +\n  geom_histogram(alpha = 0.7, bins = 20, position = \"identity\") +\n  facet_wrap(~Feature, scales = \"free\") +\n  labs(title = \"Feature Distributions by Classification Target\",\n       subtitle = \"Understanding how features relate to the binary classification task\",\n       x = \"Feature Value\", y = \"Count\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\")\n```\n\n::: {.cell-output-display}\n![](multi-task-learning-with-torch_files/figure-html/exploratory-analysis-2.png){width=1152}\n:::\n:::\n\n\n\n\n### 3. Define the Multi-Task Neural Network\n\nThe architecture design creates a neural network with shared layers and task-specific branches:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the multi-task neural network\nmulti_task_net <- nn_module(\n  \"MultiTaskNet\",\n  \n  initialize = function(input_size, \n                        hidden_size, \n                        reg_output_size = 1, \n                        cls_output_size = 1) {\n    \n    self$input_size <- input_size\n    self$hidden_size <- hidden_size\n    self$reg_output_size <- reg_output_size\n    self$cls_output_size <- cls_output_size\n    \n    # Shared layers - these learn representations useful for both tasks\n    self$shared_layer1 <- nn_linear(input_size, hidden_size)\n    self$shared_layer2 <- nn_linear(hidden_size, hidden_size)\n    self$dropout <- nn_dropout(0.2)  # Add regularization\n    \n    # Task-specific layers\n    # Regression branch\n    self$regression_layer <- nn_linear(hidden_size, reg_output_size)\n    \n    # Classification branch\n    self$classification_layer <- nn_linear(hidden_size, cls_output_size)\n  },\n  \n  forward = function(x) {\n    # Shared feature extraction\n    shared_features <- x %>%\n      self$shared_layer1() %>%\n      nnf_relu() %>%\n      self$dropout() %>%\n      self$shared_layer2() %>%\n      nnf_relu() %>%\n      self$dropout()\n    \n    # Task-specific predictions\n    regression_output <- self$regression_layer(shared_features)\n    classification_logits <- self$classification_layer(shared_features)\n    \n    list(\n      regression = regression_output,\n      classification = classification_logits\n    )\n  }\n)\n\n# Create model instance\nmodel <- multi_task_net(\n  input_size = 5,\n  hidden_size = 30\n)\n\n# Print model architecture\nprint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn `nn_module` containing 1,172 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────\n• shared_layer1: <nn_linear> #180 parameters\n• shared_layer2: <nn_linear> #930 parameters\n• dropout: <nn_dropout> #0 parameters\n• regression_layer: <nn_linear> #31 parameters\n• classification_layer: <nn_linear> #31 parameters\n```\n\n\n:::\n\n```{.r .cell-code}\n# Count parameters\ntotal_params <- sum(sapply(model$parameters, function(p) prod(p$shape)))\ncat(\"\\nTotal parameters:\", total_params, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nTotal parameters: 1172 \n```\n\n\n:::\n:::\n\n\n\n\n### 4. Define Loss Functions and Optimizer\n\nMulti-task learning requires separate loss functions for each task. To address potential overfitting, particularly in the classification task, several regularization strategies will be implemented:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Loss functions\nregression_loss_fn <- nnf_mse_loss  # Mean squared error for regression\nclassification_loss_fn <- nnf_binary_cross_entropy_with_logits  # Binary cross-entropy for classification\n\n# Optimizer with weight decay for L2 regularization\noptimizer <- optim_adam(model$parameters, lr = 0.01, weight_decay = 1e-4)\n\n# Task weights - these control the relative importance of each task\ntask_weights <- c(regression = 0.5, classification = 0.5)\n\n# Early stopping parameters\npatience <- 15\nbest_val_loss <- Inf\npatience_counter <- 0\nbest_model_state <- NULL\n\n# Validation split from training data\nval_size <- round(0.2 * length(train_idx))\nval_indices <- sample(train_idx, val_size)\ntrain_indices <- setdiff(train_idx, val_indices)\n\n# Create validation sets\nx_val <- x[val_indices, ]\ny_reg_val <- y_regression[val_indices]\ny_cls_val <- y_classification[val_indices]\n\n# Update training sets\nx_train <- x[train_indices, ]\ny_reg_train <- y_regression[train_indices]\ny_cls_train <- y_classification[train_indices]\n\ncat(\"Updated split - Training:\", length(train_indices), \n    \"Validation:\", length(val_indices), \n    \"Testing:\", length(test_idx), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUpdated split - Training: 560 Validation: 140 Testing: 300 \n```\n\n\n:::\n:::\n\n\n\n\n### 5. Enhanced Training Loop with Overfitting Prevention\n\nThe training process now incorporates validation monitoring and early stopping to prevent overfitting:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Hyperparameters\nepochs <- 200  # Increased epochs since we have early stopping\n\n# Enhanced training history tracking\ntraining_history <- data.frame(\n  epoch = integer(),\n  train_reg_loss = numeric(),\n  train_cls_loss = numeric(),\n  train_total_loss = numeric(),\n  val_reg_loss = numeric(),\n  val_cls_loss = numeric(),\n  val_total_loss = numeric(),\n  val_accuracy = numeric()\n)\n\ncat(\"Starting enhanced training with overfitting prevention...\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStarting enhanced training with overfitting prevention...\n```\n\n\n:::\n\n```{.r .cell-code}\nfor (epoch in 1:epochs) {\n  # Training phase\n  model$train()\n  optimizer$zero_grad()\n  \n  # Forward pass on training data\n  outputs <- model(x_train)\n  \n  # Calculate training loss for each task\n  train_reg_loss <- regression_loss_fn(\n    outputs$regression$squeeze(), \n    y_reg_train\n  )\n  \n  train_cls_loss <- classification_loss_fn(\n    outputs$classification$squeeze(), \n    y_cls_train\n  )\n  \n  # Weighted combined training loss\n  train_total_loss <- task_weights[\"regression\"] * train_reg_loss + \n                     task_weights[\"classification\"] * train_cls_loss\n    # Backward pass and optimize\n  train_total_loss$backward()\n  \n  # Gradient clipping to prevent exploding gradients\n  nn_utils_clip_grad_norm_(model$parameters, max_norm = 1.0)\n  \n  optimizer$step()\n  \n  # Validation phase\n  model$eval()\n  with_no_grad({\n    val_outputs <- model(x_val)\n    \n    # Calculate validation losses\n    val_reg_loss <- regression_loss_fn(\n      val_outputs$regression$squeeze(), \n      y_reg_val\n    )\n    \n    val_cls_loss <- classification_loss_fn(\n      val_outputs$classification$squeeze(), \n      y_cls_val\n    )\n    \n    val_total_loss <- task_weights[\"regression\"] * val_reg_loss + \n                     task_weights[\"classification\"] * val_cls_loss\n    \n    # Calculate validation accuracy\n    val_cls_probs <- nnf_sigmoid(val_outputs$classification$squeeze())\n    val_cls_preds <- (val_cls_probs > 0.5)$to(torch_int())\n    val_accuracy <- (val_cls_preds == y_cls_val$to(torch_int()))$sum()$item() / length(val_indices)\n  })\n  \n  # Record history\n  training_history <- rbind(\n    training_history,\n    data.frame(\n      epoch = epoch,\n      train_reg_loss = as.numeric(train_reg_loss$item()),\n      train_cls_loss = as.numeric(train_cls_loss$item()),\n      train_total_loss = as.numeric(train_total_loss$item()),\n      val_reg_loss = as.numeric(val_reg_loss$item()),\n      val_cls_loss = as.numeric(val_cls_loss$item()),\n      val_total_loss = as.numeric(val_total_loss$item()),\n      val_accuracy = val_accuracy\n    )\n  )\n  \n  # Early stopping logic\n  current_val_loss <- as.numeric(val_total_loss$item())\n  if (current_val_loss < best_val_loss) {\n    best_val_loss <- current_val_loss\n    patience_counter <- 0\n    # Save best model state\n    best_model_state <- model$state_dict()\n  } else {\n    patience_counter <- patience_counter + 1\n  }\n  \n  # Print progress every 25 epochs\n  if (epoch %% 25 == 0 || epoch == 1) {\n    cat(sprintf(\"Epoch %d - Train Loss: %.4f, Val Loss: %.4f, Val Acc: %.3f, Patience: %d/%d\\n\", \n                epoch, train_total_loss$item(), val_total_loss$item(), \n                val_accuracy, patience_counter, patience))\n  }\n  \n  # Early stopping\n  if (patience_counter >= patience) {\n    cat(sprintf(\"\\nEarly stopping at epoch %d. Best validation loss: %.4f\\n\", epoch, best_val_loss))\n    break\n  }\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEpoch 1 - Train Loss: 0.8349, Val Loss: 0.6657, Val Acc: 0.507, Patience: 0/15\nEpoch 25 - Train Loss: 0.2110, Val Loss: 0.1678, Val Acc: 0.950, Patience: 0/15\nEpoch 50 - Train Loss: 0.0987, Val Loss: 0.0626, Val Acc: 0.979, Patience: 2/15\nEpoch 75 - Train Loss: 0.0788, Val Loss: 0.0484, Val Acc: 0.993, Patience: 0/15\n\nEarly stopping at epoch 95. Best validation loss: 0.0424\n```\n\n\n:::\n\n```{.r .cell-code}\n# Load best model state\nif (!is.null(best_model_state)) {\n  model$load_state_dict(best_model_state)\n  cat(\"Loaded best model from validation\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLoaded best model from validation\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Training completed!\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining completed!\n```\n\n\n:::\n:::\n\n\n\n\n### 6. Model Evaluation\n\nThe evaluation process assesses the model's performance on both tasks:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set model to evaluation mode\nmodel$eval()\n\n# Make predictions on test set\nwith_no_grad({\n  outputs <- model(x_test)\n  \n  # Regression evaluation\n  reg_preds <- outputs$regression$squeeze()\n  reg_test_loss <- regression_loss_fn(reg_preds, y_reg_test)\n  \n  # Classification evaluation\n  cls_preds <- outputs$classification$squeeze()\n  cls_probs <- nnf_sigmoid(cls_preds)\n  cls_test_loss <- classification_loss_fn(cls_preds, y_cls_test)\n  \n  # Convert predictions to binary (threshold = 0.5)\n  cls_pred_labels <- (cls_probs > 0.5)$to(torch_int())\n  \n  # Calculate accuracy\n  accuracy <- (cls_pred_labels == y_cls_test$to(torch_int()))$sum()$item() / length(test_idx)\n})\n\n# Calculate additional metrics\nreg_preds_r <- as.numeric(reg_preds)\ny_reg_test_r <- as.numeric(y_reg_test)\ncls_probs_r <- as.numeric(cls_probs)\ny_cls_test_r <- as.numeric(y_cls_test)\n\n# Regression metrics\nrmse <- sqrt(mean((reg_preds_r - y_reg_test_r)^2))\nmae <- mean(abs(reg_preds_r - y_reg_test_r))\nr_squared <- cor(reg_preds_r, y_reg_test_r)^2\n\n# Classification metrics\nauc <- try({\n  if(require(pROC, quietly = TRUE)) {\n    pROC::auc(pROC::roc(y_cls_test_r, cls_probs_r, quiet = TRUE))\n  } else {\n    NA\n  }\n}, silent = TRUE)\n\n# Display results\nperformance_results <- data.frame(\n  Task = c(\"Regression\", \"Regression\", \"Regression\", \"Classification\", \"Classification\", \"Classification\"),\n  Metric = c(\"Test Loss (MSE)\", \"RMSE\", \"R-squared\", \"Test Loss (BCE)\", \"Accuracy\", \"AUC\"),\n  Value = c(\n    round(reg_test_loss$item(), 4),\n    round(rmse, 4),\n    round(r_squared, 4),\n    round(cls_test_loss$item(), 4),\n    round(accuracy * 100, 2), \n    round(auc * 100, 2)\n  )\n)\n\nprint(performance_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Task          Metric   Value\n1     Regression Test Loss (MSE)  0.0562\n2     Regression            RMSE  0.2370\n3     Regression       R-squared  0.9411\n4 Classification Test Loss (BCE)  0.0440\n5 Classification        Accuracy 98.6700\n6 Classification             AUC 99.9600\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nClassification AUC:\", round(as.numeric(auc), 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nClassification AUC: 0.9996 \n```\n\n\n:::\n:::\n\n\n\n\n### 7. Enhanced Visualization with Overfitting Analysis\n\nComprehensive visualizations demonstrate the model's performance and overfitting prevention:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot enhanced training history with overfitting detection\np1 <- training_history %>%\n  select(epoch, train_total_loss, val_total_loss) %>%\n  pivot_longer(cols = c(train_total_loss, val_total_loss), \n               names_to = \"split\", values_to = \"loss\") %>%\n  mutate(split = case_when(\n    split == \"train_total_loss\" ~ \"Training\",\n    split == \"val_total_loss\" ~ \"Validation\"\n  )) %>%\n  ggplot(aes(x = epoch, y = loss, color = split)) +\n  geom_line(size = 1) +\n  geom_vline(xintercept = which.min(training_history$val_total_loss), \n             linetype = \"dashed\", color = \"red\", alpha = 0.7) +\n  labs(title = \"Training vs Validation Loss\",\n       subtitle = \"Red line shows optimal stopping point\",\n       x = \"Epoch\", y = \"Total Loss\", color = \"Dataset\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n# Separate task losses\np2 <- training_history %>%\n  select(epoch, train_reg_loss, val_reg_loss, train_cls_loss, val_cls_loss) %>%\n  pivot_longer(cols = -epoch, names_to = \"metric\", values_to = \"loss\") %>%\n  separate(metric, into = c(\"split\", \"task\", \"loss_type\"), sep = \"_\") %>%\n  mutate(\n    split = ifelse(split == \"train\", \"Training\", \"Validation\"),\n    task = ifelse(task == \"reg\", \"Regression\", \"Classification\"),\n    metric_name = paste(split, task)\n  ) %>%\n  ggplot(aes(x = epoch, y = loss, color = metric_name)) +\n  geom_line(size = 1) +\n  facet_wrap(~task, scales = \"free_y\") +\n  labs(title = \"Task-Specific Loss Curves\",\n       subtitle = \"Monitoring overfitting in individual tasks\",\n       x = \"Epoch\", y = \"Loss\", color = \"Split & Task\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set2\")\n\n# Validation accuracy progression\np3 <- ggplot(training_history, aes(x = epoch, y = val_accuracy)) +\n  geom_line(color = \"#2c3e50\", size = 1) +\n  geom_hline(yintercept = max(training_history$val_accuracy), \n             linetype = \"dashed\", color = \"red\", alpha = 0.7) +\n  labs(title = \"Validation Accuracy Progression\",\n       subtitle = paste(\"Peak accuracy:\", round(max(training_history$val_accuracy), 3)),\n       x = \"Epoch\", y = \"Validation Accuracy\") +\n  theme_minimal()\n\n# Overfitting analysis\ntraining_history$overfitting_gap <- training_history$train_total_loss - training_history$val_total_loss\n\np4 <- ggplot(training_history, aes(x = epoch, y = overfitting_gap)) +\n  geom_line(color = \"#e74c3c\", size = 1) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  labs(title = \"Overfitting Gap Analysis\",\n       subtitle = \"Difference between training and validation loss\",\n       x = \"Epoch\", y = \"Training Loss - Validation Loss\") +\n  theme_minimal()\n\n# Regression predictions vs actual values\nregression_results <- data.frame(\n  Actual = y_reg_test_r,\n  Predicted = reg_preds_r\n)\n\np5 <- ggplot(regression_results, aes(x = Actual, y = Predicted)) +\n  geom_point(alpha = 0.6, color = \"#2c3e50\") +\n  geom_abline(slope = 1, intercept = 0, color = \"#e74c3c\", linetype = \"dashed\", size = 1) +\n  geom_smooth(method = \"lm\", color = \"#3498db\", se = TRUE) +\n  labs(title = \"Regression Task: Actual vs Predicted Values\",\n       subtitle = paste(\"R² =\", round(r_squared, 3), \", RMSE =\", round(rmse, 3)),\n       x = \"Actual Values\", y = \"Predicted Values\") +\n  theme_minimal()\n\n# Classification probability distribution\ncls_results <- data.frame(\n  Probability = cls_probs_r,\n  Actual_Class = factor(y_cls_test_r, labels = c(\"Class 0\", \"Class 1\"))\n)\n\np6 <- ggplot(cls_results, aes(x = Probability, fill = Actual_Class)) +\n  geom_histogram(alpha = 0.7, bins = 20, position = \"identity\") +\n  geom_vline(xintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Classification Task: Predicted Probabilities\",\n       subtitle = paste(\"Accuracy =\", round(accuracy * 100, 1), \"%\"),\n       x = \"Predicted Probability\", y = \"Count\", fill = \"Actual Class\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\")\n\n# Combine plots\nlibrary(patchwork)\n(p1 | p3) / (p2) / (p4) / (p5 | p6)\n```\n\n::: {.cell-output-display}\n![](multi-task-learning-with-torch_files/figure-html/enhanced-visualize-training-1.png){width=1344}\n:::\n:::\n\n\n\n\n### 8. Feature Importance Analysis\n\nAnalyse what the shared layers learned:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract weights from the first shared layer\nshared_weights <- as.matrix(model$shared_layer1$weight$detach())\n\n# Create feature importance visualization\nfeature_importance_df <- data.frame(\n  Feature = rep(paste0(\"Feature_\", 1:5), each = 30),\n  Neuron = rep(1:30, times = 5),\n  Weight = as.vector(t(shared_weights))\n)\n\n# Calculate average absolute importance per feature\nfeature_avg_importance <- feature_importance_df %>%\n  group_by(Feature) %>%\n  summarise(\n    Avg_Abs_Weight = mean(abs(Weight)),\n    Std_Weight = sd(Weight)\n  ) %>%\n  arrange(desc(Avg_Abs_Weight))\n\n# Plot feature importance\np5 <- ggplot(feature_avg_importance, aes(x = reorder(Feature, Avg_Abs_Weight), \n                                        y = Avg_Abs_Weight)) +\n  geom_col(fill = \"#3498db\", alpha = 0.8) +\n  geom_errorbar(aes(ymin = Avg_Abs_Weight - Std_Weight, \n                    ymax = Avg_Abs_Weight + Std_Weight),\n                width = 0.2, color = \"#2c3e50\") +\n  coord_flip() +\n  labs(title = \"Feature Importance in Shared Layers\",\n       subtitle = \"Average absolute weights from first shared layer\",\n       x = \"Features\", y = \"Average Absolute Weight\") +\n  theme_minimal()\n\n# Weight distribution heatmap\np6 <- ggplot(feature_importance_df, aes(x = Neuron, y = Feature, fill = Weight)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"#e74c3c\", mid = \"white\", high = \"#2c3e50\", \n                       midpoint = 0, name = \"Weight\") +\n  labs(title = \"Shared Layer Weight Distribution\",\n       subtitle = \"How each feature connects to shared neurons\",\n       x = \"Neuron Index\", y = \"Input Features\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank())\n\np5 | p6\n```\n\n::: {.cell-output-display}\n![](multi-task-learning-with-torch_files/figure-html/feature-importance-1.png){width=960}\n:::\n:::\n\n\n\n\n## Key Takeaways\n\nThis multi-task learning implementation demonstrates several important concepts:\n\n1. **Architecture Design**: The shared-private paradigm enables models to learn both common and task-specific representations\n\n2. **Overfitting Prevention**: Multiple regularization techniques (dropout, weight decay, early stopping, gradient clipping) are essential for robust multi-task learning\n\n3. **Validation Monitoring**: Tracking validation performance for each task helps identify overfitting early and enables optimal model selection\n\n4. **Loss Combination**: Properly weighting multiple loss functions proves crucial for balanced learning across tasks\n\n5. **Evaluation Strategy**: Each task requires appropriate metrics, and overall model success depends on performance across all tasks\n\n6. **Parameter Efficiency**: Multi-task models can achieve comparable performance with fewer total parameters when properly regularized\n\n7. **Knowledge Transfer**: Related tasks can benefit from shared feature learning, especially when data is limited\n\n## Anti-Overfitting Strategies Summary\n\nThe tutorial demonstrates several key techniques for preventing overfitting in multi-task learning:\n\n- **Dropout**: Applied in shared layers (20% rate) to prevent co-adaptation of neurons\n- **Weight Decay**: L2 regularization in optimizer (1e-4) to prevent large weights\n- **Early Stopping**: Monitoring validation loss with patience to stop at optimal point\n- **Gradient Clipping**: Preventing exploding gradients (max norm = 1.0)\n- **Validation Split**: Proper train/validation/test separation for unbiased evaluation\n- **Model Selection**: Saving and loading best performing model state based on validation performance",
    "supporting": [
      "multi-task-learning-with-torch_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}