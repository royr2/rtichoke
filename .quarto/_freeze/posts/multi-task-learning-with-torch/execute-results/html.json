{
  "hash": "2c6311fca66d3d381127cf46fd7dd768",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Multi-Task Learning with torch in R\"\ndate: \"2025-05-11\"\ncategories: [R, Deep Learning, torch, Multi-Task Learning]\nimage: \"../images/torch.png\"\nexecute:\n  echo: true\n  warning: false\n  message: false\n  eval: true\n---\n\n\n\nMulti-task learning (MTL) is an approach where a single neural network model is trained to perform multiple related tasks simultaneously. This approach can improve model generalization, reduce overfitting, and leverage shared information across tasks. In this post, we'll implement a multi-task learning model using the `torch` package in R.\n\n## Introduction to Multi-Task Learning\n\nMulti-task learning works by sharing representations between related tasks, allowing the model to generalize better. Instead of training separate models for each task, we train a single model with:\n\n- **Shared layers** that learn common features across tasks\n- **Task-specific layers** that specialize for each individual task  \n- **Multiple loss functions**, one for each task\n\nThis approach is particularly useful when you have related prediction problems that can benefit from shared feature representations.\n\n## Installation and Setup\n\nFirst, let's install and load the required packages:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(c(\"torch\", \"tidyverse\", \"corrplot\"))\nlibrary(torch)\nlibrary(tidyverse)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n## Creating a Multi-Task Learning Model\n\nWe'll build a model that simultaneously performs two related tasks:\n1. **Regression**: Predicting a continuous value\n2. **Classification**: Predicting a binary outcome\n\n### 1. Generate Sample Data for Multiple Tasks\n\nLet's create a synthetic dataset that contains features relevant to both tasks:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed for reproducibility\nset.seed(123)\n\n# Number of samples\nn <- 1000\n\n# Create a dataset with 5 features\nx <- torch_randn(n, 5)\n\n# Task 1 (Regression): Predict continuous value\n# Create a target that's a function of the input features plus some noise\ny_regression <- x[, 1] * 0.7 + x[, 2] * 0.3 - x[, 3] * 0.5 + torch_randn(n) * 0.2\n\n# Task 2 (Classification): Predict binary outcome\n# Create a classification target based on a nonlinear combination of features\nlogits <- x[, 1] * 0.8 - x[, 4] * 0.4 + x[, 5] * 0.6\ny_classification <- (logits > 0)$to(torch_float())\n\n# Display data summary\ncat(\"Dataset created with\", n, \"samples and 5 features\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDataset created with 1000 samples and 5 features\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Regression target range:\", round(as.numeric(y_regression$min()), 3), \"to\", round(as.numeric(y_regression$max()), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRegression target range: -2.809 to 2.641 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Classification target distribution:\", \n    round(as.numeric(y_classification$mean()), 3), \"positive class ratio\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nClassification target distribution: 0.5 positive class ratio\n```\n\n\n:::\n\n```{.r .cell-code}\n# Split into training (70%) and testing (30%) sets\ntrain_idx <- 1:round(0.7 * n)\ntest_idx <- (round(0.7 * n) + 1):n\n\n# Training data\nx_train <- x[train_idx, ]\ny_reg_train <- y_regression[train_idx]\ny_cls_train <- y_classification[train_idx]\n\n# Testing data\nx_test <- x[test_idx, ]\ny_reg_test <- y_regression[test_idx]\ny_cls_test <- y_classification[test_idx]\n\ncat(\"Training samples:\", length(train_idx), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining samples: 700 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing samples:\", length(test_idx), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting samples: 300 \n```\n\n\n:::\n:::\n\n\n\n### 2. Exploratory Data Analysis\n\nLet's examine the relationships between features and targets:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert to R matrices for analysis\nx_r <- as.matrix(x)\ny_reg_r <- as.numeric(y_regression)\ny_cls_r <- as.numeric(y_classification)\n\n# Create a comprehensive dataset for analysis\nanalysis_df <- data.frame(\n  Feature1 = x_r[, 1],\n  Feature2 = x_r[, 2],\n  Feature3 = x_r[, 3],\n  Feature4 = x_r[, 4],\n  Feature5 = x_r[, 5],\n  Regression_Target = y_reg_r,\n  Classification_Target = y_cls_r\n)\n\n# Correlation analysis\ncor_matrix <- cor(analysis_df)\ncorrplot(cor_matrix, method = \"color\", type = \"upper\", \n         tl.col = \"black\", tl.srt = 45, addCoef.col = \"black\",\n         title = \"Feature and Target Correlations\", mar = c(0,0,1,0))\n```\n\n::: {.cell-output-display}\n![](multi-task-learning-with-torch_files/figure-html/exploratory-analysis-1.png){width=1152}\n:::\n\n```{.r .cell-code}\n# Visualize feature distributions by classification target\nlong_features <- analysis_df %>%\n  select(-Regression_Target) %>%\n  pivot_longer(cols = starts_with(\"Feature\"), \n               names_to = \"Feature\", values_to = \"Value\") %>%\n  mutate(Class = factor(Classification_Target, labels = c(\"Class 0\", \"Class 1\")))\n\nggplot(long_features, aes(x = Value, fill = Class)) +\n  geom_histogram(alpha = 0.7, bins = 20, position = \"identity\") +\n  facet_wrap(~Feature, scales = \"free\") +\n  labs(title = \"Feature Distributions by Classification Target\",\n       subtitle = \"Understanding how features relate to the binary classification task\",\n       x = \"Feature Value\", y = \"Count\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\")\n```\n\n::: {.cell-output-display}\n![](multi-task-learning-with-torch_files/figure-html/exploratory-analysis-2.png){width=1152}\n:::\n:::\n\n\n\n### 3. Define the Multi-Task Neural Network\n\nNow we'll create a neural network with shared layers and task-specific branches:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the multi-task neural network\nmulti_task_net <- nn_module(\n  \"MultiTaskNet\",\n  \n  initialize = function(input_size, \n                        hidden_size, \n                        reg_output_size = 1, \n                        cls_output_size = 1) {\n    \n    self$input_size <- input_size\n    self$hidden_size <- hidden_size\n    self$reg_output_size <- reg_output_size\n    self$cls_output_size <- cls_output_size\n    \n    # Shared layers - these learn representations useful for both tasks\n    self$shared_layer1 <- nn_linear(input_size, hidden_size)\n    self$shared_layer2 <- nn_linear(hidden_size, hidden_size)\n    self$dropout <- nn_dropout(0.2)  # Add regularization\n    \n    # Task-specific layers\n    # Regression branch\n    self$regression_layer <- nn_linear(hidden_size, reg_output_size)\n    \n    # Classification branch\n    self$classification_layer <- nn_linear(hidden_size, cls_output_size)\n  },\n  \n  forward = function(x) {\n    # Shared feature extraction\n    shared_features <- x %>%\n      self$shared_layer1() %>%\n      nnf_relu() %>%\n      self$dropout() %>%\n      self$shared_layer2() %>%\n      nnf_relu() %>%\n      self$dropout()\n    \n    # Task-specific predictions\n    regression_output <- self$regression_layer(shared_features)\n    classification_logits <- self$classification_layer(shared_features)\n    \n    list(\n      regression = regression_output,\n      classification = classification_logits\n    )\n  }\n)\n\n# Create model instance\nmodel <- multi_task_net(\n  input_size = 5,\n  hidden_size = 64\n)\n\n# Print model architecture\nprint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn `nn_module` containing 4,674 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────\n• shared_layer1: <nn_linear> #384 parameters\n• shared_layer2: <nn_linear> #4,160 parameters\n• dropout: <nn_dropout> #0 parameters\n• regression_layer: <nn_linear> #65 parameters\n• classification_layer: <nn_linear> #65 parameters\n```\n\n\n:::\n\n```{.r .cell-code}\n# Count parameters\ntotal_params <- sum(sapply(model$parameters, function(p) prod(p$shape)))\ncat(\"\\nTotal parameters:\", total_params, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nTotal parameters: 4674 \n```\n\n\n:::\n:::\n\n\n\n### 4. Define Loss Functions and Optimizer\n\nFor multi-task learning, we need separate loss functions for each task:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Loss functions\nregression_loss_fn <- nnf_mse_loss  # Mean squared error for regression\nclassification_loss_fn <- nnf_binary_cross_entropy_with_logits  # Binary cross-entropy for classification\n\n# Optimizer\noptimizer <- optim_adam(model$parameters, lr = 0.01)\n\n# Task weights - these control the relative importance of each task\ntask_weights <- c(regression = 0.5, classification = 0.5)\n\ncat(\"Regression loss function: Mean Squared Error\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRegression loss function: Mean Squared Error\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Classification loss function: Binary Cross-Entropy with Logits\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nClassification loss function: Binary Cross-Entropy with Logits\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Optimizer: Adam with learning rate 0.01\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOptimizer: Adam with learning rate 0.01\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Task weights - Regression:\", task_weights[\"regression\"], \n    \", Classification:\", task_weights[\"classification\"], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTask weights - Regression: 0.5 , Classification: 0.5 \n```\n\n\n:::\n:::\n\n\n\n### 5. Training Loop for Multi-Task Learning\n\nWe'll train the model by combining the losses from both tasks:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Hyperparameters\nepochs <- 200\n\n# Training history tracking\ntraining_history <- data.frame(\n  epoch = integer(),\n  reg_loss = numeric(),\n  cls_loss = numeric(),\n  total_loss = numeric()\n)\n\n# Training loop\ncat(\"Starting training...\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStarting training...\n```\n\n\n:::\n\n```{.r .cell-code}\nfor (epoch in 1:epochs) {\n  model$train()\n  optimizer$zero_grad()\n  \n  # Forward pass\n  outputs <- model(x_train)\n  \n  # Calculate loss for each task\n  reg_loss <- regression_loss_fn(\n    outputs$regression$squeeze(), \n    y_reg_train\n  )\n  \n  cls_loss <- classification_loss_fn(\n    outputs$classification$squeeze(), \n    y_cls_train\n  )\n  \n  # Weighted combined loss\n  total_loss <- task_weights[\"regression\"] * reg_loss + \n               task_weights[\"classification\"] * cls_loss\n  \n  # Backward pass and optimize\n  total_loss$backward()\n  optimizer$step()\n  \n  # Record history (every 20 epochs)\n  if (epoch %% 20 == 0 || epoch == 1) {\n    training_history <- rbind(\n      training_history,\n      data.frame(\n        epoch = epoch,\n        reg_loss = as.numeric(reg_loss$item()),\n        cls_loss = as.numeric(cls_loss$item()),\n        total_loss = as.numeric(total_loss$item())\n      )\n    )\n    \n    cat(sprintf(\"Epoch %d - Regression Loss: %.4f, Classification Loss: %.4f, Total Loss: %.4f\\n\", \n                epoch, reg_loss$item(), cls_loss$item(), total_loss$item()))\n  }\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEpoch 1 - Regression Loss: 0.9342, Classification Loss: 0.6992, Total Loss: 0.8167\nEpoch 20 - Regression Loss: 0.1205, Classification Loss: 0.1459, Total Loss: 0.1332\nEpoch 40 - Regression Loss: 0.0863, Classification Loss: 0.0745, Total Loss: 0.0804\nEpoch 60 - Regression Loss: 0.0704, Classification Loss: 0.0392, Total Loss: 0.0548\nEpoch 80 - Regression Loss: 0.0681, Classification Loss: 0.0252, Total Loss: 0.0466\nEpoch 100 - Regression Loss: 0.0611, Classification Loss: 0.0160, Total Loss: 0.0385\nEpoch 120 - Regression Loss: 0.0619, Classification Loss: 0.0138, Total Loss: 0.0379\nEpoch 140 - Regression Loss: 0.0656, Classification Loss: 0.0140, Total Loss: 0.0398\nEpoch 160 - Regression Loss: 0.0634, Classification Loss: 0.0092, Total Loss: 0.0363\nEpoch 180 - Regression Loss: 0.0615, Classification Loss: 0.0113, Total Loss: 0.0364\nEpoch 200 - Regression Loss: 0.0635, Classification Loss: 0.0045, Total Loss: 0.0340\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Training completed!\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining completed!\n```\n\n\n:::\n:::\n\n\n\n### 6. Model Evaluation\n\nLet's evaluate the model's performance on both tasks:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set model to evaluation mode\nmodel$eval()\n\n# Make predictions on test set\nwith_no_grad({\n  outputs <- model(x_test)\n  \n  # Regression evaluation\n  reg_preds <- outputs$regression$squeeze()\n  reg_test_loss <- regression_loss_fn(reg_preds, y_reg_test)\n  \n  # Classification evaluation\n  cls_preds <- outputs$classification$squeeze()\n  cls_probs <- nnf_sigmoid(cls_preds)\n  cls_test_loss <- classification_loss_fn(cls_preds, y_cls_test)\n  \n  # Convert predictions to binary (threshold = 0.5)\n  cls_pred_labels <- (cls_probs > 0.5)$to(torch_int())\n  \n  # Calculate accuracy\n  accuracy <- (cls_pred_labels == y_cls_test$to(torch_int()))$sum()$item() / length(test_idx)\n})\n\n# Calculate additional metrics\nreg_preds_r <- as.numeric(reg_preds)\ny_reg_test_r <- as.numeric(y_reg_test)\ncls_probs_r <- as.numeric(cls_probs)\ny_cls_test_r <- as.numeric(y_cls_test)\n\n# Regression metrics\nrmse <- sqrt(mean((reg_preds_r - y_reg_test_r)^2))\nmae <- mean(abs(reg_preds_r - y_reg_test_r))\nr_squared <- cor(reg_preds_r, y_reg_test_r)^2\n\n# Classification metrics\nauc <- try({\n  if(require(pROC, quietly = TRUE)) {\n    pROC::auc(pROC::roc(y_cls_test_r, cls_probs_r, quiet = TRUE))\n  } else {\n    NA\n  }\n}, silent = TRUE)\n\n# Display results\nperformance_results <- data.frame(\n  Task = c(\"Regression\", \"Regression\", \"Regression\", \"Classification\", \"Classification\"),\n  Metric = c(\"Test Loss (MSE)\", \"RMSE\", \"R-squared\", \"Test Loss (BCE)\", \"Accuracy\"),\n  Value = c(\n    round(reg_test_loss$item(), 4),\n    round(rmse, 4),\n    round(r_squared, 4),\n    round(cls_test_loss$item(), 4),\n    round(accuracy * 100, 2)\n  )\n)\n\nprint(performance_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Task          Metric    Value\n1     Regression Test Loss (MSE)   0.0438\n2     Regression            RMSE   0.2092\n3     Regression       R-squared   0.9551\n4 Classification Test Loss (BCE)   0.0054\n5 Classification        Accuracy 100.0000\n```\n\n\n:::\n\n```{.r .cell-code}\nif(!is.na(auc)) {\n  cat(\"\\nClassification AUC:\", round(as.numeric(auc), 4), \"\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nClassification AUC: 1 \n```\n\n\n:::\n:::\n\n\n\n### 7. Visualize Training Progress and Results\n\nLet's create comprehensive visualizations of our model's performance:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot training history\np1 <- ggplot(training_history, aes(x = epoch)) +\n  geom_line(aes(y = reg_loss, color = \"Regression Loss\"), size = 1) +\n  geom_line(aes(y = cls_loss, color = \"Classification Loss\"), size = 1) +\n  geom_line(aes(y = total_loss, color = \"Total Loss\"), size = 1, linetype = \"dashed\") +\n  labs(title = \"Multi-Task Training Progress\",\n       subtitle = \"Loss curves for both tasks during training\",\n       x = \"Epoch\", y = \"Loss\", color = \"Loss Type\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n# Regression predictions vs actual values\nregression_results <- data.frame(\n  Actual = y_reg_test_r,\n  Predicted = reg_preds_r\n)\n\np2 <- ggplot(regression_results, aes(x = Actual, y = Predicted)) +\n  geom_point(alpha = 0.6, color = \"#2c3e50\") +\n  geom_abline(slope = 1, intercept = 0, color = \"#e74c3c\", linetype = \"dashed\", size = 1) +\n  geom_smooth(method = \"lm\", color = \"#3498db\", se = TRUE) +\n  labs(title = \"Regression Task: Actual vs Predicted Values\",\n       subtitle = paste(\"R² =\", round(r_squared, 3), \", RMSE =\", round(rmse, 3)),\n       x = \"Actual Values\", y = \"Predicted Values\") +\n  theme_minimal()\n\n# Classification probability distribution\ncls_results <- data.frame(\n  Probability = cls_probs_r,\n  Actual_Class = factor(y_cls_test_r, labels = c(\"Class 0\", \"Class 1\"))\n)\n\np3 <- ggplot(cls_results, aes(x = Probability, fill = Actual_Class)) +\n  geom_histogram(alpha = 0.7, bins = 20, position = \"identity\") +\n  geom_vline(xintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Classification Task: Predicted Probabilities\",\n       subtitle = paste(\"Accuracy =\", round(accuracy * 100, 1), \"%\"),\n       x = \"Predicted Probability\", y = \"Count\", fill = \"Actual Class\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\")\n\n# Confusion matrix\ncls_pred_labels_r <- as.integer(cls_pred_labels)\nconfusion_data <- table(\n  Predicted = cls_pred_labels_r,\n  Actual = y_cls_test_r\n)\n\nconfusion_df <- as.data.frame(confusion_data)\n\np4 <- ggplot(confusion_df, aes(x = factor(Actual), y = factor(Predicted), fill = Freq)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = Freq), color = \"white\", size = 8, fontface = \"bold\") +\n  scale_fill_gradient(low = \"#3498db\", high = \"#2c3e50\") +\n  labs(title = \"Classification Task: Confusion Matrix\",\n       x = \"Actual Class\", y = \"Predicted Class\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Combine plots\nlibrary(patchwork)\n(p1 | p2) / (p3 | p4)\n```\n\n::: {.cell-output-display}\n![](multi-task-learning-with-torch_files/figure-html/visualize-training-1.png){width=1152}\n:::\n:::\n\n\n\n### 8. Feature Importance Analysis\n\nLet's analyze what the shared layers learned:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract weights from the first shared layer\nshared_weights <- as.matrix(model$shared_layer1$weight$detach())\n\n# Create feature importance visualization\nfeature_importance_df <- data.frame(\n  Feature = rep(paste0(\"Feature_\", 1:5), each = 64),\n  Neuron = rep(1:64, times = 5),\n  Weight = as.vector(t(shared_weights))\n)\n\n# Calculate average absolute importance per feature\nfeature_avg_importance <- feature_importance_df %>%\n  group_by(Feature) %>%\n  summarise(\n    Avg_Abs_Weight = mean(abs(Weight)),\n    Std_Weight = sd(Weight)\n  ) %>%\n  arrange(desc(Avg_Abs_Weight))\n\n# Plot feature importance\np5 <- ggplot(feature_avg_importance, aes(x = reorder(Feature, Avg_Abs_Weight), \n                                        y = Avg_Abs_Weight)) +\n  geom_col(fill = \"#3498db\", alpha = 0.8) +\n  geom_errorbar(aes(ymin = Avg_Abs_Weight - Std_Weight, \n                    ymax = Avg_Abs_Weight + Std_Weight),\n                width = 0.2, color = \"#2c3e50\") +\n  coord_flip() +\n  labs(title = \"Feature Importance in Shared Layers\",\n       subtitle = \"Average absolute weights from first shared layer\",\n       x = \"Features\", y = \"Average Absolute Weight\") +\n  theme_minimal()\n\n# Weight distribution heatmap\np6 <- ggplot(feature_importance_df, aes(x = Neuron, y = Feature, fill = Weight)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"#e74c3c\", mid = \"white\", high = \"#2c3e50\", \n                       midpoint = 0, name = \"Weight\") +\n  labs(title = \"Shared Layer Weight Distribution\",\n       subtitle = \"How each feature connects to shared neurons\",\n       x = \"Neuron Index\", y = \"Input Features\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank())\n\np5 | p6\n```\n\n::: {.cell-output-display}\n![](multi-task-learning-with-torch_files/figure-html/feature-importance-1.png){width=960}\n:::\n:::\n\n\n\n## Comparing Multi-Task vs Single-Task Performance\n\nLet's compare our multi-task model with individual single-task models:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define single-task networks\nsingle_task_regression <- nn_module(\n  \"SingleTaskRegression\",\n  initialize = function(input_size, hidden_size) {\n    self$layer1 <- nn_linear(input_size, hidden_size)\n    self$layer2 <- nn_linear(hidden_size, hidden_size)\n    self$output <- nn_linear(hidden_size, 1)\n    self$dropout <- nn_dropout(0.2)\n  },\n  forward = function(x) {\n    x %>%\n      self$layer1() %>%\n      nnf_relu() %>%\n      self$dropout() %>%\n      self$layer2() %>%\n      nnf_relu() %>%\n      self$dropout() %>%\n      self$output()\n  }\n)\n\nsingle_task_classification <- nn_module(\n  \"SingleTaskClassification\", \n  initialize = function(input_size, hidden_size) {\n    self$layer1 <- nn_linear(input_size, hidden_size)\n    self$layer2 <- nn_linear(hidden_size, hidden_size)\n    self$output <- nn_linear(hidden_size, 1)\n    self$dropout <- nn_dropout(0.2)\n  },\n  forward = function(x) {\n    x %>%\n      self$layer1() %>%\n      nnf_relu() %>%\n      self$dropout() %>%\n      self$layer2() %>%\n      nnf_relu() %>%\n      self$dropout() %>%\n      self$output()\n  }\n)\n\n# Train single-task models quickly for comparison\ntrain_single_task <- function(model, loss_fn, target, task_type = \"regression\", epochs = 100) {\n  optimizer <- optim_adam(model$parameters, lr = 0.01)\n  \n  for(epoch in 1:epochs) {\n    model$train()\n    optimizer$zero_grad()\n    output <- model(x_train)\n    loss <- loss_fn(output$squeeze(), target)\n    loss$backward()\n    optimizer$step()\n  }\n  \n  # Evaluate\n  model$eval()\n  with_no_grad({\n    test_output <- model(x_test)\n    if(task_type == \"regression\") {\n      test_loss <- loss_fn(test_output$squeeze(), y_reg_test)\n      return(list(loss = test_loss$item(), type = \"regression\"))\n    } else {\n      test_loss <- loss_fn(test_output$squeeze(), y_cls_test)\n      probs <- nnf_sigmoid(test_output$squeeze())\n      preds <- (probs > 0.5)$to(torch_int())\n      accuracy <- (preds == y_cls_test$to(torch_int()))$sum()$item() / length(test_idx)\n      return(list(loss = test_loss$item(), accuracy = accuracy, type = \"classification\"))\n    }\n  })\n}\n\n# Train single-task models\ncat(\"Training single-task models for comparison...\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining single-task models for comparison...\n```\n\n\n:::\n\n```{.r .cell-code}\nreg_model <- single_task_regression(5, 64)\ncls_model <- single_task_classification(5, 64)\n\nreg_results <- train_single_task(reg_model, nnf_mse_loss, y_reg_train, \"regression\")\ncls_results <- train_single_task(cls_model, nnf_binary_cross_entropy_with_logits, y_cls_train, \"classification\")\n\n# Create comparison\ncomparison_df <- data.frame(\n  Model = c(\"Multi-Task\", \"Single-Task\", \"Multi-Task\", \"Single-Task\"),\n  Task = c(\"Regression\", \"Regression\", \"Classification\", \"Classification\"),\n  Metric = c(\"MSE Loss\", \"MSE Loss\", \"Accuracy (%)\", \"Accuracy (%)\"),\n  Value = c(\n    reg_test_loss$item(),\n    reg_results$loss,\n    accuracy * 100,\n    cls_results$accuracy * 100\n  )\n)\n\nprint(comparison_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        Model           Task       Metric        Value\n1  Multi-Task     Regression     MSE Loss   0.04375131\n2 Single-Task     Regression     MSE Loss   0.04800936\n3  Multi-Task Classification Accuracy (%) 100.00000000\n4 Single-Task Classification Accuracy (%) 100.00000000\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize comparison\nggplot(comparison_df, aes(x = Task, y = Value, fill = Model)) +\n  geom_col(position = \"dodge\", alpha = 0.8) +\n  facet_wrap(~Metric, scales = \"free_y\") +\n  labs(title = \"Multi-Task vs Single-Task Performance Comparison\",\n       subtitle = \"Lower loss and higher accuracy are better\",\n       x = \"Task\", y = \"Performance Value\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\")\n```\n\n::: {.cell-output-display}\n![](multi-task-learning-with-torch_files/figure-html/comparison-analysis-1.png){width=1152}\n:::\n:::\n\n\n\n\n## Key Takeaways\n\nOur multi-task learning implementation demonstrates several important concepts:\n\n1. **Architecture Design**: The shared-private paradigm allows models to learn both common and task-specific representations\n\n2. **Loss Combination**: Properly weighting multiple loss functions is crucial for balanced learning across tasks\n\n3. **Evaluation Strategy**: Each task requires appropriate metrics, and overall model success depends on performance across all tasks\n\n4. **Parameter Efficiency**: Multi-task models can achieve comparable performance with fewer total parameters\n\n5. **Knowledge Transfer**: Related tasks can benefit from shared feature learning, especially when data is limited",
    "supporting": [
      "multi-task-learning-with-torch_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}