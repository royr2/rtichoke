{
  "hash": "7f5aee93caf4cbf7745c54310ed148bf",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Getting Started with Parallel Computing in R\"\ndescription: \"Learn how to speed up your R code by using multiple CPU cores\"\ndate: \"2025-04-26\"\ncategories: [R, Performance, Parallel Computing]\nexecute:\n  echo: true\n  warning: false\n  message: false\n  eval: true\n---\n\n\n\n\n## What is Parallel Computing and Why Use It?\n\nHave you ever run an R script that took hours to complete? Parallel computing can help! Instead of running calculations one after another (sequentially), parallel computing allows you to run multiple calculations at the same time by using all available CPU cores on your computer.\n\n**Benefits:**\n- **Speed**: Run code significantly faster\n- **Efficiency**: Make better use of your computer's resources\n- **Scalability**: Handle larger datasets and more complex models\n\n## Quick Setup: Required Packages\n\nLet's start by installing and loading the packages we'll need:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install packages if needed (uncomment to run)\n# install.packages(c(\"parallel\", \"foreach\", \"doParallel\", \"tictoc\"))\n\n# Load the essential packages\nlibrary(parallel)    # Base R parallel functions\nlibrary(foreach)     # For parallel loops\nlibrary(doParallel)  # Backend for foreach\nlibrary(tictoc)      # For timing comparisons\n```\n:::\n\n\n\n\n## How Many Cores Do You Have?\n\nThe first step is to check how many CPU cores are available on your computer:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Detect the number of CPU cores\ndetectCores()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 16\n```\n\n\n:::\n:::\n\n\n\n\nIt's usually good practice to leave one core free for your operating system, so we'll typically use `detectCores() - 1` for our parallel operations.\n\n## Your First Parallel Code: The Basics\n\nLet's create a simple function that takes some time to run, then compare how long it takes to run sequentially versus in parallel:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A function that takes time to execute\nslow_function <- function(x) {\n  Sys.sleep(0.5)  # Simulate computation time (half a second)\n  return(x^2)     # Return the square of x\n}\n\n# Create a list of numbers to process\nnumbers <- 1:10\n```\n:::\n\n\n\n\n### Method 1: Using parLapply (Works on All Systems)\n\nThis method works on all operating systems including Windows:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 1: Create a cluster of workers\ncl <- makeCluster(detectCores() - 1)\n\n# Step 2: Export any functions our workers need\nclusterExport(cl, \"slow_function\")\n\n# Run the sequential version and time it\ntic(\"Sequential version\")\nresult_sequential <- lapply(numbers, slow_function)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSequential version: 5.08 sec elapsed\n```\n\n\n:::\n\n```{.r .cell-code}\n# Run the parallel version and time it\ntic(\"Parallel version\")\nresult_parallel <- parLapply(cl, numbers, slow_function)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nParallel version: 0.52 sec elapsed\n```\n\n\n:::\n\n```{.r .cell-code}\n# Step 3: Always stop the cluster when done!\nstopCluster(cl)\n\n# Verify both methods give the same results\nall.equal(result_sequential, result_parallel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n\n\n### Method 2: Using mclapply (Unix/Mac Only)\n\nIf you're on Mac or Linux, you can use this simpler approach:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# For Mac/Linux users only\ntic(\"Parallel mclapply (Mac/Linux only)\")\nresult_parallel <- mclapply(numbers, slow_function, mc.cores = detectCores() - 1)\ntoc()\n```\n:::\n\n\n\n\n## The foreach Package: A More Intuitive Approach\n\nMany R users find the `foreach` package easier to understand and use. It works like a loop but can run in parallel:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 1: Create and register a parallel backend\ncl <- makeCluster(detectCores() - 1)\nregisterDoParallel(cl)\n\n# Run sequential foreach with %do%\ntic(\"Sequential foreach\")\nresult_sequential <- foreach(i = 1:10) %do% {\n  slow_function(i)\n}\ntoc()\n\n# Run parallel foreach with %dopar%\ntic(\"Parallel foreach\")\nresult_parallel <- foreach(i = 1:10) %dopar% {\n  slow_function(i)\n}\ntoc()\n\n# Always stop the cluster when done\nstopCluster(cl)\n\n# Verify results\nall.equal(result_sequential, result_parallel)\n```\n:::\n\n\n\n\n### Combining Results with foreach\n\nOne of the great features of `foreach` is how easily you can combine results:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create and register a parallel backend\ncl <- makeCluster(detectCores() - 1)\nregisterDoParallel(cl)\n\n# Sum all results automatically with .combine='+'\ntic(\"Parallel sum of squares\")\ntotal <- foreach(i = 1:100, .combine = '+') %dopar% {\n  i^2\n}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nParallel sum of squares: 0.13 sec elapsed\n```\n\n\n:::\n\n```{.r .cell-code}\n# Stop the cluster\nstopCluster(cl)\n\n# Verify the result\nprint(paste(\"Our result:\", total))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Our result: 338350\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Correct answer:\", sum((1:100)^2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Correct answer: 338350\"\n```\n\n\n:::\n:::\n\n\n\n\n## A Real Example: Matrix Operations\n\nLet's try something more realistic. Matrix operations are perfect for parallelization:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A more computationally intensive function\nmatrix_function <- function(n) {\n  # Create a random n×n matrix\n  m <- matrix(rnorm(n*n), ncol = n)\n  # Calculate eigenvalues (computationally expensive)\n  eigen(m)\n  return(sum(diag(m)))\n}\n\n# Let's process 8 matrices of size 300×300\nmatrix_sizes <- rep(300, 8)\n```\n:::\n\n\n\n\n### Comparing Methods\n\nLet's compare how different methods perform:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sequential execution\ntic(\"Sequential\")\nsequential_result <- lapply(matrix_sizes, matrix_function)\nsequential_time <- toc(quiet = TRUE)\nsequential_time <- sequential_time$toc - sequential_time$tic\n\n# Parallel with parLapply\ncl <- makeCluster(detectCores() - 1)\nclusterExport(cl, \"matrix_function\")\ntic(\"parLapply\")\nparlapply_result <- parLapply(cl, matrix_sizes, matrix_function)\nparlapply_time <- toc(quiet = TRUE)\nparlapply_time <- parlapply_time$toc - parlapply_time$tic\nstopCluster(cl)\n\n# Parallel with foreach\ncl <- makeCluster(detectCores() - 1)\nregisterDoParallel(cl)\ntic(\"foreach\")\nforeach_result <- foreach(s = matrix_sizes) %dopar% {\n  matrix_function(s)\n}\nforeach_time <- toc(quiet = TRUE)\nforeach_time <- foreach_time$toc - foreach_time$tic\nstopCluster(cl)\n\n# Create a results table\nresults <- data.frame(\n  Method = c(\"Sequential\", \"parLapply\", \"foreach\"),\n  Time = c(sequential_time, parlapply_time, foreach_time),\n  Speedup = c(1, sequential_time/parlapply_time, sequential_time/foreach_time)\n)\n\n# Display the results\nresults\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Method Time  Speedup\n1 Sequential 2.49 1.000000\n2  parLapply 0.31 8.032258\n3    foreach 0.42 5.928571\n```\n\n\n:::\n:::\n\n\n\n\n### Visualizing the Results\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load ggplot2 for visualization\nlibrary(ggplot2)\n\n# Plot execution times\nggplot(results, aes(x = reorder(Method, -Time), y = Time, fill = Method)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Execution Time Comparison\",\n       x = \"Method\", y = \"Time (seconds)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](parallel-computing-in-r_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot speedup\nggplot(results, aes(x = reorder(Method, Speedup), y = Speedup, fill = Method)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Speedup Comparison\",\n       x = \"Method\", y = \"Times faster than sequential\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](parallel-computing-in-r_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n:::\n\n\n\n\n## When to Use Parallel Computing\n\nParallel computing isn't always the right choice. Here's when to use it:\n\n✅ **Good for parallelization:**\n- Independent calculations (like applying the same function to different data chunks)\n- Computationally intensive tasks (simulations, bootstrap resampling)\n- Tasks that take more than a few seconds to run sequentially\n\n❌ **Not good for parallelization:**\n- Very quick operations (parallelization overhead may exceed the time saved)\n- Tasks with heavy dependencies between steps\n- I/O-bound operations (reading/writing files)\n\n## Quick Tips for Success\n\n1. **Always stop your clusters** with `stopCluster(cl)` when you're done\n2. **Leave one core free** for your operating system\n3. **Start small** and test with a subset of your data\n4. **Watch your memory usage** - each worker needs its own copy of the data\n\n## Next Steps\n\nOnce you're comfortable with these basics, you can explore:\n\n- The `future` package for more advanced parallel computing\n- The `furrr` package for parallel versions of `purrr` functions\n- Parallel computing with large datasets using `data.table` or `dplyr`\n- Distributed computing across multiple machines\n",
    "supporting": [
      "parallel-computing-in-r_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}