<!doctype html>
<html lang="en-us">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name="hugo-theme" content="Axiom 0.8.0">



  <link rel="icon" type="image/png" sizes="32x32" href="/">
  <link rel="icon" type="image/x-icon" href="/">
  <link rel="apple-touch-icon" href="/">
  <link rel="canonical" href="https://rtichoke.netlify.app/post/bayesian_optimisation_xgboost/">
<link rel="preload" as="style" href="/bundle.css?v=1679930341" media="all">
<link rel="stylesheet" href="/bundle.css?v=1679930341" media="all">
<style>.cdata pre{background-color:#1f2937;color:#e5e7eb}.cdata :not(pre)>code{background-color:#f3f4f6;color:#7c3aed}.chroma .err{background-color:#991b1b;color:#fecaca}.chroma .hl{background-color:#374151}.chroma .ln{color:#9ca3af}.chroma .k,.chroma .kc,.chroma .kd,.chroma .kn,.chroma .kp,.chroma .kr{color:#60a5fa}.chroma .kt{color:#a78bfa}.chroma .na,.chroma .nb{color:#fbbf24}.chroma .nc{color:#f87171}.chroma .no{color:#34d399}.chroma .nd{color:#f87171}.chroma .ne{color:#f87171}.chroma .nf{color:#fbbf24}.chroma .nt{color:#f87171}.chroma .l{color:#a78bfa}.chroma .dl,.chroma .ld,.chroma .s,.chroma .s2,.chroma .sa,.chroma .sb,.chroma .sc,.chroma .sd{color:#34d399}.chroma .se{color:#9ca3af}.chroma .s1,.chroma .sh,.chroma .si,.chroma .sr,.chroma .ss,.chroma .sx{color:#34d399}.chroma .il,.chroma .m,.chroma .mb,.chroma .mf,.chroma .mh,.chroma .mi,.chroma .mo{color:#a78bfa}.chroma .o,.chroma .ow{color:#93c5fd}.chroma .c,.chroma .c1,.chroma .ch,.chroma .cm,.chroma .cp,.chroma .cpf,.chroma .cs,.chroma .p{color:#9ca3af}.chroma .ge{font-style:italic}.chroma .gs{font-weight:700}
</style>



<title>Using bayesian optimisation to tune a XGBOOST model in R : R&#39;tichoke</title>

<meta property="og:title" content="Using bayesian optimisation to tune a XGBOOST model in R">
<meta property="og:site_name" content="R&#39;tichoke">
<meta property="og:url" content="https://rtichoke.netlify.app/post/bayesian_optimisation_xgboost/">
<link rel="image_src" href="https://rtichoke.netlify.app/">
<meta property="og:image" content="https://rtichoke.netlify.app/">
<meta property="og:image:width" content="">
<meta property="og:image:height" content="">
<meta property="og:type" content="article">
<meta property="og:locale" content="en_us">
<meta property="og:description" content="How to use bayesian optimisation to tune hyperparameters in a XGBOOST model in R">
<meta name="description" content="How to use bayesian optimisation to tune hyperparameters in a XGBOOST model in R">
<meta property="og:updated_time" content="2022-01-08T00:00:00Z">
<meta property="fb:app_id" content="">
<meta name="author" content="Riddhiman">
<meta property="article:author" content="https://linkedin.com/in/riddhimanr">
<meta property="article:published_time" content="2022-01-08T00:00:00Z">
<meta property="article:modified_time" content="2022-01-08T00:00:00Z">
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "Using bayesian optimisation to tune a XGBOOST model in R",
  "alternativeHeadline": "ML series (Post #1)",
  "url": "https://rtichoke.netlify.app/post/bayesian_optimisation_xgboost/",
  "image": "https://rtichoke.netlify.app/",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://rtichoke.netlify.app/post/bayesian_optimisation_xgboost/"
  },
  "description": "How to use bayesian optimisation to tune hyperparameters in a XGBOOST model in R",
  "author": {
    "@type": "Person",
    "name": "Riddhiman"
  },
  "publisher": {
    "@type": "Organization",
    "name": "R'tichoke",
    "logo": {
      "@type": "ImageObject",
      "url": "https://rtichoke.netlify.app/"
    }
  },
  "datePublished": "2022-01-08T00:00:00Z",
  "dateModified": "2022-01-08T00:00:00Z",
  "articleBody": "\r\n\r\n\r\n\u003cp\u003eMy first post in 2022! A very happy new year to anyone reading this. üòÑ\u003c/p\u003e\r\n\u003cp\u003eI was looking for a simple and effective way to tune \u003ccode\u003exgboost\u003c/code\u003e models in \u003ccode\u003eR\u003c/code\u003e and came across this package called \u003ca href=\"https://github.com/AnotherSamWilson/ParBayesianOptimization\"\u003eParBayesianOptimization\u003c/a\u003e. Here‚Äôs a quick tutorial on how to use it to tune a \u003ccode\u003exgboost\u003c/code\u003e model.\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Pacman is a package management tool \r\ninstall.packages(\u0026quot;pacman\u0026quot;)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003elibrary(pacman)\r\n\r\n# p_load automatically installs packages if needed\r\np_load(xgboost, ParBayesianOptimization, mlbench, dplyr, skimr, recipes, resample)\r\n## package \u0026#39;xgboost\u0026#39; successfully unpacked and MD5 sums checked\r\n## \r\n## The downloaded binary packages are in\r\n## \tC:\\Users\\riddh\\AppData\\Local\\Temp\\RtmpCy7FXq\\downloaded_packages\r\n## package \u0026#39;rprojroot\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;diffobj\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;rematch2\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;brio\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;callr\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;desc\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;pkgload\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;praise\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;processx\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;ps\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;waldo\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;testthat\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;numDeriv\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;SparseM\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;MatrixModels\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;minqa\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;nloptr\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;RcppEigen\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;backports\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;carData\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;abind\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;pbkrtest\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;quantreg\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;lme4\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;broom\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;corrplot\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;car\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;iterators\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;ggrepel\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;ggsci\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;cowplot\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;ggsignif\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;gridExtra\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;polynom\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;rstatix\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;DiceKriging\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;foreach\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;dbscan\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;lhs\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;ggpubr\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;ParBayesianOptimization\u0026#39; successfully unpacked and MD5 sums checked\r\n## \r\n## The downloaded binary packages are in\r\n## \tC:\\Users\\riddh\\AppData\\Local\\Temp\\RtmpCy7FXq\\downloaded_packages\r\n## package \u0026#39;mlbench\u0026#39; successfully unpacked and MD5 sums checked\r\n## \r\n## The downloaded binary packages are in\r\n## \tC:\\Users\\riddh\\AppData\\Local\\Temp\\RtmpCy7FXq\\downloaded_packages\r\n## package \u0026#39;repr\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;skimr\u0026#39; successfully unpacked and MD5 sums checked\r\n## \r\n## The downloaded binary packages are in\r\n## \tC:\\Users\\riddh\\AppData\\Local\\Temp\\RtmpCy7FXq\\downloaded_packages\r\n## package \u0026#39;future.apply\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;progressr\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;SQUAREM\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;lava\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;tzdb\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;prodlim\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;timechange\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;clock\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;gower\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;hardhat\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;ipred\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;lubridate\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;timeDate\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;recipes\u0026#39; successfully unpacked and MD5 sums checked\r\n## \r\n## The downloaded binary packages are in\r\n## \tC:\\Users\\riddh\\AppData\\Local\\Temp\\RtmpCy7FXq\\downloaded_packages\r\n## package \u0026#39;resample\u0026#39; successfully unpacked and MD5 sums checked\r\n## \r\n## The downloaded binary packages are in\r\n## \tC:\\Users\\riddh\\AppData\\Local\\Temp\\RtmpCy7FXq\\downloaded_packages\u003c/code\u003e\u003c/pre\u003e\r\n\u003cdiv id=\"data-prep\" class=\"section level2\"\u003e\r\n\u003ch2\u003eData prep\u003c/h2\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Load up some data\r\ndata(\u0026quot;BostonHousing2\u0026quot;)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Data summary\r\nskim(BostonHousing2)\u003c/code\u003e\u003c/pre\u003e\r\n\u003ctable\u003e\r\n\u003ccaption\u003e\u003cspan id=\"tab:unnamed-chunk-4\"\u003eTable 1: \u003c/span\u003eData summary\u003c/caption\u003e\r\n\u003ctbody\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd align=\"left\"\u003eName\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003eBostonHousing2\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"even\"\u003e\r\n\u003ctd align=\"left\"\u003eNumber of rows\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e506\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd align=\"left\"\u003eNumber of columns\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e19\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"even\"\u003e\r\n\u003ctd align=\"left\"\u003e_______________________\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd align=\"left\"\u003eColumn type frequency:\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"even\"\u003e\r\n\u003ctd align=\"left\"\u003efactor\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e2\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd align=\"left\"\u003enumeric\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e17\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"even\"\u003e\r\n\u003ctd align=\"left\"\u003e________________________\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd align=\"left\"\u003eGroup variables\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003eNone\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003c/tbody\u003e\r\n\u003c/table\u003e\r\n\u003cp\u003e\u003cstrong\u003eVariable type: factor\u003c/strong\u003e\u003c/p\u003e\r\n\u003ctable\u003e\r\n\u003ccolgroup\u003e\r\n\u003ccol width=\"15%\" /\u003e\r\n\u003ccol width=\"11%\" /\u003e\r\n\u003ccol width=\"15%\" /\u003e\r\n\u003ccol width=\"8%\" /\u003e\r\n\u003ccol width=\"10%\" /\u003e\r\n\u003ccol width=\"38%\" /\u003e\r\n\u003c/colgroup\u003e\r\n\u003cthead\u003e\r\n\u003ctr class=\"header\"\u003e\r\n\u003cth align=\"left\"\u003eskim_variable\u003c/th\u003e\r\n\u003cth align=\"right\"\u003en_missing\u003c/th\u003e\r\n\u003cth align=\"right\"\u003ecomplete_rate\u003c/th\u003e\r\n\u003cth align=\"left\"\u003eordered\u003c/th\u003e\r\n\u003cth align=\"right\"\u003en_unique\u003c/th\u003e\r\n\u003cth align=\"left\"\u003etop_counts\u003c/th\u003e\r\n\u003c/tr\u003e\r\n\u003c/thead\u003e\r\n\u003ctbody\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd align=\"left\"\u003etown\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003eFALSE\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e92\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003eCam: 30, Bos: 23, Lyn: 22, Bos: 19\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"even\"\u003e\r\n\u003ctd align=\"left\"\u003echas\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003eFALSE\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e2\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e0: 471, 1: 35\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003c/tbody\u003e\r\n\u003c/table\u003e\r\n\u003cp\u003e\u003cstrong\u003eVariable type: numeric\u003c/strong\u003e\u003c/p\u003e\r\n\u003ctable\u003e\r\n\u003ccolgroup\u003e\r\n\u003ccol width=\"14%\" /\u003e\r\n\u003ccol width=\"10%\" /\u003e\r\n\u003ccol width=\"14%\" /\u003e\r\n\u003ccol width=\"8%\" /\u003e\r\n\u003ccol width=\"8%\" /\u003e\r\n\u003ccol width=\"7%\" /\u003e\r\n\u003ccol width=\"8%\" /\u003e\r\n\u003ccol width=\"8%\" /\u003e\r\n\u003ccol width=\"8%\" /\u003e\r\n\u003ccol width=\"8%\" /\u003e\r\n\u003ccol width=\"6%\" /\u003e\r\n\u003c/colgroup\u003e\r\n\u003cthead\u003e\r\n\u003ctr class=\"header\"\u003e\r\n\u003cth align=\"left\"\u003eskim_variable\u003c/th\u003e\r\n\u003cth align=\"right\"\u003en_missing\u003c/th\u003e\r\n\u003cth align=\"right\"\u003ecomplete_rate\u003c/th\u003e\r\n\u003cth align=\"right\"\u003emean\u003c/th\u003e\r\n\u003cth align=\"right\"\u003esd\u003c/th\u003e\r\n\u003cth align=\"right\"\u003ep0\u003c/th\u003e\r\n\u003cth align=\"right\"\u003ep25\u003c/th\u003e\r\n\u003cth align=\"right\"\u003ep50\u003c/th\u003e\r\n\u003cth align=\"right\"\u003ep75\u003c/th\u003e\r\n\u003cth align=\"right\"\u003ep100\u003c/th\u003e\r\n\u003cth align=\"left\"\u003ehist\u003c/th\u003e\r\n\u003c/tr\u003e\r\n\u003c/thead\u003e\r\n\u003ctbody\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd align=\"left\"\u003etract\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e2700.36\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1380.04\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1.00\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1303.25\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e3393.50\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e3739.75\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e5082.00\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e‚ñÖ‚ñÇ‚ñÇ‚ñá‚ñÇ\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"even\"\u003e\r\n\u003ctd align=\"left\"\u003elon\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e-71.06\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0.08\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e-71.29\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e-71.09\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e-71.05\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e-71.02\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e-70.81\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e‚ñÅ‚ñÇ‚ñá‚ñÇ‚ñÅ\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd align=\"left\"\u003elat\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e42.22\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0.06\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e42.03\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e42.18\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e42.22\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e42.25\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e42.38\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñÅ\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"even\"\u003e\r\n\u003ctd align=\"left\"\u003emedv\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e22.53\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e9.20\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e5.00\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e17.02\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e21.20\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e25.00\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e50.00\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e‚ñÇ‚ñá‚ñÖ‚ñÅ‚ñÅ\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd align=\"left\"\u003ecmedv\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e22.53\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e9.18\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e5.00\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e17.02\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e21.20\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e25.00\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e50.00\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e‚ñÇ‚ñá‚ñÖ‚ñÅ‚ñÅ\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"even\"\u003e\r\n\u003ctd align=\"left\"\u003ecrim\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e3.61\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e8.60\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0.01\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0.08\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0.26\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e3.68\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e88.98\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd align=\"left\"\u003ezn\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e11.36\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e23.32\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0.00\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0.00\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0.00\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e12.50\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e100.00\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"even\"\u003e\r\n\u003ctd align=\"left\"\u003eindus\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e11.14\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e6.86\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0.46\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e5.19\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e9.69\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e18.10\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e27.74\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e‚ñá‚ñÜ‚ñÅ‚ñá‚ñÅ\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd align=\"left\"\u003enox\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0.55\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0.12\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0.38\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0.45\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0.54\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0.62\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0.87\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e‚ñá‚ñá‚ñÜ‚ñÖ‚ñÅ\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"even\"\u003e\r\n\u003ctd align=\"left\"\u003erm\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e6.28\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0.70\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e3.56\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e5.89\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e6.21\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e6.62\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e8.78\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e‚ñÅ‚ñÇ‚ñá‚ñÇ‚ñÅ\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd align=\"left\"\u003eage\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e68.57\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e28.15\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e2.90\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e45.02\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e77.50\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e94.07\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e100.00\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñá\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"even\"\u003e\r\n\u003ctd align=\"left\"\u003edis\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e3.80\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e2.11\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1.13\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e2.10\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e3.21\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e5.19\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e12.13\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e‚ñá‚ñÖ‚ñÇ‚ñÅ‚ñÅ\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd align=\"left\"\u003erad\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e9.55\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e8.71\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1.00\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e4.00\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e5.00\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e24.00\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e24.00\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÉ\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"even\"\u003e\r\n\u003ctd align=\"left\"\u003etax\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e408.24\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e168.54\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e187.00\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e279.00\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e330.00\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e666.00\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e711.00\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e‚ñá‚ñá‚ñÉ‚ñÅ‚ñá\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd align=\"left\"\u003eptratio\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e18.46\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e2.16\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e12.60\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e17.40\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e19.05\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e20.20\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e22.00\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñá\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"even\"\u003e\r\n\u003ctd align=\"left\"\u003eb\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e356.67\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e91.29\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0.32\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e375.38\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e391.44\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e396.22\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e396.90\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd align=\"left\"\u003elstat\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e0\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e12.65\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e7.14\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e1.73\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e6.95\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e11.36\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e16.96\u003c/td\u003e\r\n\u003ctd align=\"right\"\u003e37.97\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e‚ñá‚ñá‚ñÖ‚ñÇ‚ñÅ\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003c/tbody\u003e\r\n\u003c/table\u003e\r\n\u003cp\u003eLooks like there is are two factor variables. We‚Äôll need to convert them into numeric variables before we proceed. I‚Äôll use the \u003ccode\u003erecipes\u003c/code\u003e package to one-hot encode them.\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Predicting median house prices\r\nrec \u0026lt;- recipe(cmedv ~ ., data = BostonHousing2) %\u0026gt;%\r\n  \r\n  # Collapse categories where population is \u0026lt; 3%\r\n  step_other(town, chas, threshold = .03, other = \u0026quot;Other\u0026quot;) %\u0026gt;% \r\n  \r\n  # Create dummy variables for all factor variables \r\n  step_dummy(all_nominal_predictors())\r\n\r\n# Train the recipe on the data set\r\nprep \u0026lt;- prep(rec, training = BostonHousing2)\r\n\r\n# Create the final model matrix\r\nmodel_df \u0026lt;- bake(prep, new_data = BostonHousing2)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# All levels have been one hot encoded and separate columns have been appended to the model matrix\r\ncolnames(model_df)\r\n##  [1] \u0026quot;tract\u0026quot;                  \u0026quot;lon\u0026quot;                    \u0026quot;lat\u0026quot;                   \r\n##  [4] \u0026quot;medv\u0026quot;                   \u0026quot;crim\u0026quot;                   \u0026quot;zn\u0026quot;                    \r\n##  [7] \u0026quot;indus\u0026quot;                  \u0026quot;nox\u0026quot;                    \u0026quot;rm\u0026quot;                    \r\n## [10] \u0026quot;age\u0026quot;                    \u0026quot;dis\u0026quot;                    \u0026quot;rad\u0026quot;                   \r\n## [13] \u0026quot;tax\u0026quot;                    \u0026quot;ptratio\u0026quot;                \u0026quot;b\u0026quot;                     \r\n## [16] \u0026quot;lstat\u0026quot;                  \u0026quot;cmedv\u0026quot;                  \u0026quot;town_Boston.Savin.Hill\u0026quot;\r\n## [19] \u0026quot;town_Cambridge\u0026quot;         \u0026quot;town_Lynn\u0026quot;              \u0026quot;town_Newton\u0026quot;           \r\n## [22] \u0026quot;town_Other\u0026quot;             \u0026quot;chas_X1\u0026quot;\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003eNext, we can use the \u003ccode\u003eresample\u003c/code\u003e package to create test/train splits.\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003esplits \u0026lt;- rsample::initial_split(model_df, prop = 0.7)\r\n\r\n# Training set\r\ntrain_df \u0026lt;- rsample::training(splits)\r\n\r\n# Test set\r\ntest_df \u0026lt;- rsample::testing(splits)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003edim(train_df)\r\n## [1] 354  23\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003edim(test_df)\r\n## [1] 152  23\u003c/code\u003e\u003c/pre\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"finding-optimal-parameters\" class=\"section level2\"\u003e\r\n\u003ch2\u003eFinding optimal parameters\u003c/h2\u003e\r\n\u003cp\u003eNow we can start to run some optimisations using the \u003ccode\u003eParBayesianOptimization\u003c/code\u003e package.\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# The xgboost interface accepts matrices \r\nX \u0026lt;- train_df %\u0026gt;%\r\n  # Remove the target variable\r\n  select(!medv, !cmedv) %\u0026gt;%\r\n  as.matrix()\r\n\r\n# Get the target variable\r\ny \u0026lt;- train_df %\u0026gt;%\r\n  pull(cmedv)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Cross validation folds\r\nfolds \u0026lt;- list(fold1 = as.integer(seq(1, nrow(X), by = 5)),\r\n              fold2 = as.integer(seq(2, nrow(X), by = 5)))\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003eWe‚Äôll need an objective function which can be fed to the optimiser. We‚Äôll use the value of the evaluation metric from \u003ccode\u003exgb.cv()\u003c/code\u003e as the value that needs to be optimised.\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Function must take the hyper-parameters as inputs\r\nobj_func \u0026lt;- function(eta, max_depth, min_child_weight, subsample, lambda, alpha) {\r\n  \r\n  param \u0026lt;- list(\r\n    \r\n    # Hyter parameters \r\n    eta = eta,\r\n    max_depth = max_depth,\r\n    min_child_weight = min_child_weight,\r\n    subsample = subsample,\r\n    lambda = lambda,\r\n    alpha = alpha,\r\n    \r\n    # Tree model \r\n    booster = \u0026quot;gbtree\u0026quot;,\r\n    \r\n    # Regression problem \r\n    objective = \u0026quot;reg:squarederror\u0026quot;,\r\n    \r\n    # Use the Mean Absolute Percentage Error\r\n    eval_metric = \u0026quot;mape\u0026quot;)\r\n  \r\n  xgbcv \u0026lt;- xgb.cv(params = param,\r\n                  data = X,\r\n                  label = y,\r\n                  nround = 50,\r\n                  folds = folds,\r\n                  prediction = TRUE,\r\n                  early_stopping_rounds = 5,\r\n                  verbose = 0,\r\n                  maximize = F)\r\n  \r\n  lst \u0026lt;- list(\r\n    \r\n    # First argument must be named as \u0026quot;Score\u0026quot;\r\n    # Function finds maxima so inverting the output\r\n    Score = -min(xgbcv$evaluation_log$test_mape_mean),\r\n    \r\n    # Get number of trees for the best performing model\r\n    nrounds = xgbcv$best_iteration\r\n  )\r\n  \r\n  return(lst)\r\n}\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003eOnce we have the objective function, we‚Äôll need to define some bounds for the optimiser to search within.\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003ebounds \u0026lt;- list(eta = c(0.001, 0.2),\r\n               max_depth = c(1L, 10L),\r\n               min_child_weight = c(1, 50),\r\n               subsample = c(0.1, 1),\r\n               lambda = c(1, 10),\r\n               alpha = c(1, 10))\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003eWe can now run the optimiser to find a set of optimal hyper-parameters.\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003eset.seed(1234)\r\nbayes_out \u0026lt;- bayesOpt(FUN = obj_func, bounds = bounds, initPoints = length(bounds) + 2, iters.n = 3)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Show relevant columns from the summary object \r\nbayes_out$scoreSummary[1:5, c(3:8, 13)]\r\n##           eta max_depth min_child_weight subsample   lambda    alpha      Score\r\n## 1: 0.13392137         8         4.913332 0.2105925 4.721124 3.887629 -0.0902970\r\n## 2: 0.19400811         2        25.454160 0.9594105 9.329695 3.173695 -0.1402720\r\n## 3: 0.16079775         2        14.035652 0.5118349 1.229953 5.093530 -0.1475580\r\n## 4: 0.08957707         4        12.534842 0.3844404 4.358837 1.788342 -0.1410245\r\n## 5: 0.02876388         4        36.586761 0.8107181 6.137100 6.039125 -0.3061535\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Get best parameters\r\ndata.frame(getBestPars(bayes_out))\r\n##         eta max_depth min_child_weight subsample lambda alpha\r\n## 1 0.1905414         8         1.541476 0.8729207      1     1\u003c/code\u003e\u003c/pre\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"fitting-the-model\" class=\"section level2\"\u003e\r\n\u003ch2\u003eFitting the model\u003c/h2\u003e\r\n\u003cp\u003eWe can now fit a model and check how well these parameters work.\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Combine best params with base params\r\nopt_params \u0026lt;- append(list(booster = \u0026quot;gbtree\u0026quot;, \r\n                          objective = \u0026quot;reg:squarederror\u0026quot;, \r\n                          eval_metric = \u0026quot;mae\u0026quot;), \r\n                     getBestPars(bayes_out))\r\n\r\n# Run cross validation \r\nxgbcv \u0026lt;- xgb.cv(params = opt_params,\r\n                data = X,\r\n                label = y,\r\n                nround = 100,\r\n                folds = folds,\r\n                prediction = TRUE,\r\n                early_stopping_rounds = 5,\r\n                verbose = 0,\r\n                maximize = F)\r\n\r\n# Get optimal number of rounds\r\nnrounds = xgbcv$best_iteration\r\n\r\n# Fit a xgb model\r\nmdl \u0026lt;- xgboost(data = X, label = y, \r\n               params = opt_params, \r\n               maximize = F, \r\n               early_stopping_rounds = 5, \r\n               nrounds = nrounds, \r\n               verbose = 0)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Evaluate performance \r\nactuals \u0026lt;- test_df$cmedv\r\npredicted \u0026lt;- test_df %\u0026gt;%\r\n  select_at(mdl$feature_names) %\u0026gt;%\r\n  as.matrix %\u0026gt;%\r\n  predict(mdl, newdata = .)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Compute MAPE\r\nmean(abs(actuals - predicted)/actuals)\r\n## [1] 0.01648831\u003c/code\u003e\u003c/pre\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"compare-with-grid-search\" class=\"section level2\"\u003e\r\n\u003ch2\u003eCompare with grid search\u003c/h2\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003egrd \u0026lt;- expand.grid(\r\n  eta = seq(0.001, 0.2, length.out = 5),\r\n  max_depth = seq(2L, 10L, by = 1),\r\n  min_child_weight = seq(1, 25, length.out = 3),\r\n  subsample = c(0.25, 0.5, 0.75, 1),\r\n  lambda = c(1, 5, 10),\r\n  alpha = c(1, 5, 10))\r\n\r\ndim(grd)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003egrd_out \u0026lt;- apply(grd, 1, function(par){\r\n  \r\n    par \u0026lt;- append(par, list(booster = \u0026quot;gbtree\u0026quot;,objective = \u0026quot;reg:squarederror\u0026quot;,eval_metric = \u0026quot;mae\u0026quot;))\r\n    mdl \u0026lt;- xgboost(data = X, label = y, params = par, nrounds = 50, early_stopping_rounds = 5, maximize = F, verbose = 0)\r\n    lst \u0026lt;- data.frame(par, score = mdl$best_score)\r\n\r\n    return(lst)\r\n  })\r\n\r\ngrd_out \u0026lt;- do.call(rbind, grd_out)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003ebest_par \u0026lt;- grd_out %\u0026gt;%\r\n  data.frame() %\u0026gt;%\r\n  arrange(score) %\u0026gt;%\r\n  .[1,]\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Fit final model\r\nparams \u0026lt;- as.list(best_par[-length(best_par)])\r\nxgbcv \u0026lt;- xgb.cv(params = params,\r\n                data = X,\r\n                label = y,\r\n                nround = 100,\r\n                folds = folds,\r\n                prediction = TRUE,\r\n                early_stopping_rounds = 5,\r\n                verbose = 0,\r\n                maximize = F)\r\n\r\nnrounds = xgbcv$best_iteration\r\n\r\nmdl \u0026lt;- xgboost(data = X, \r\n               label = y, \r\n               params = params, \r\n               maximize = F, \r\n               early_stopping_rounds = 5, \r\n               nrounds = nrounds, \r\n               verbose = 0)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Evaluate on test set\r\nact \u0026lt;- test_df$medv\r\npred \u0026lt;- test_df %\u0026gt;%\r\n  select_at(mdl$feature_names) %\u0026gt;%\r\n  as.matrix %\u0026gt;%\r\n  predict(mdl, newdata = .)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003emean(abs(act - pred)/act)\r\n## [1] 0.01951497\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003eWhile both the methods offer similar final results, the bayesian optimiser completed its search in less than a minute where as the grid search took over seven minutes. Also, I find that I can use bayesian optimisation to search a larger parameter space more quickly than a traditional grid search.\u003c/p\u003e\r\n\u003cp\u003e\u003cem\u003eThoughts? Comments? Helpful? Not helpful? Like to see anything else added in here? Let me know!\u003c/em\u003e\u003c/p\u003e\r\n\u003c/div\u003e"
}
</script>

<link rel="preload" as="script" href="/bundle.js?v=1679930341">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://stats.g.doubleclick.net">
<link rel="preconnect" href="https://www.googleadservices.com">
<link rel="preload" as="script" href="https://www.googletagmanager.com/gtag/js?id=UA-57026003-1">
<script src="https://www.googletagmanager.com/gtag/js?id=UA-57026003-1"></script>
<script>
  window.dataLayer=window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js',new Date());
  gtag('config','UA-57026003-1');
</script>

</head>
<body>

  <header id="nav" class="header">
  <div class="ax-l-i max-w-6xl">
    <div class="ax-logo">
      <a class="block" href="/" title="R&#39;tichoke"><span class="font-semibold text-raven-900">R'tichoke</span></a>
    </div>
    <div class="ax-user">
      <a class="p-2 w-8 h-8 block text-raven-500 hover:text-gray-800 focus:text-gray-800 focus:outline-none" target="_blank" rel="noopener nofollow" href="https://www.google.com/search?q=site:https://rtichoke.netlify.app/" title="Search">
        <svg class="fill-current" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M2.67 12.804c0-5.6 4.544-10.134 10.133-10.134s10.134 4.544 10.134 10.134-4.544 10.133-10.134 10.133S2.67 18.393 2.67 12.804zm28.943 16.923l-8.868-8.868c4.287-5.3 3.68-13.012-1.378-17.57S8.564-1.066 3.75 3.75s-5.017 12.558-.46 17.618 12.28 5.665 17.57 1.378l8.868 8.868a1.33 1.33 0 0 0 2.231-.597c.123-.46-.008-.952-.345-1.29h0z"/></svg>

      </a>
      <a class="p-2 block text-base leading-none text-raven-500 hover:text-gray-800 focus:text-gray-800 focus:outline-none" href="/post/">
        Posts
      </a>
      <a class="p-2 block text-base leading-none text-raven-500 hover:text-gray-800 focus:text-gray-800 focus:outline-none" href="/about/">
        About
      </a>
      <a class="p-2 block text-base leading-none text-raven-500 hover:text-gray-800 focus:text-gray-800 focus:outline-none" href="/rbloggers/">
        R-Bloggers
      </a>
    </div>
  </div>

  
  
</header>


  <main>
<div class="default-single">
  <div class="ax-title ax-l-o">
    <div class="ax-l-i max-w-4xl">
      <h1 class="post-title font-content-title font-normal leading-tight tracking-default text-40">Using bayesian optimisation to tune a XGBOOST model in R</h1>
      <p class="post-subtitle font-content-sans font-light text-xl text-raven-500 mt-3">ML series (Post #1)</p>

      <div class="ax-meta flex items-center mt-5">
        <div class="flex-grow min-w-0">
          <div class="flex items-center">
  
  <div class="flex-shrink-0">
    <img
    class="w-12 h-12 sm:w-14 sm:h-14 object-cover p-3px rounded-full border border-blue-300"
    src="data:image/svg&#43;xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI&#43;PHBhdGggZmlsbD0iI2VjZWZmMSIgZD0iTTAgMGgzMnYzMkgweiIvPjxwYXRoIGZpbGw9IiM0NTVhNjQiIGQ9Ik0yOS4zMDUgMjQuNDA3Yy0uNzk1LS42My0xLjc2NS0xLjA4LTIuODE2LTEuM0wyMS40NzYgMjIuMWExLjEzIDEuMTMgMCAwIDEtLjkwNS0xLjEydi0xLjE1Yy4zMjItLjQ1My42MjYtMS4wNTQuOTQ0LTEuNjgyLjI0Ny0uNDg3LjYyLTEuMjIuODA1LTEuNCAxLjAxNS0xLjAyIDEuOTk1LTIuMTY1IDIuMy0zLjY0LjI4My0xLjM4NS4wMDUtMi4xMTItLjMyMi0yLjY5NyAwLTEuNDYtLjA0Ni0zLjI5LS4zOS00LjYyLS4wNC0xLjgtLjM2OC0yLjgxNC0xLjE5LTMuNy0uNTgtLjYzLTEuNDM1LS43NzUtMi4xMjMtLjg5LS4yNy0uMDQ2LS42NDItLjEtLjc4LS4xODNDMTguNTk0LjM0NyAxNy40LjAyNSAxNS45NTIgMGMtMyAuMTIzLTYuNzEgMi4wNC03Ljk1IDUuNDU0LS4zODQgMS4wNC0uMzQ1IDIuNzQ3LS4zMTMgNC4xMmwtLjAzLjgyNWMtLjI5NS41NzYtLjU4NSAxLjMwNy0uMyAyLjY5Ny4zMDIgMS40OCAxLjI4MiAyLjYyNiAyLjMxNSAzLjY2LjE3LjE3NC41NS45MTQuODAyIDEuNDAzbC45NSAxLjY3NXYxLjE1YzAgLjU0Ni0uMzgyIDEuMDE3LS45IDEuMTJMNS41IDIzLjExYy0xLjA0NS4yMjItMi4wMTQuNjctMi44MDcgMS4yOThhMS4xNSAxLjE1IDAgMCAwLS40MjcuODA1IDEuMTQgMS4xNCAwIDAgMCAuMjkzLjg1OUM1Ljk3NSAyOS44MzggMTAuODczIDMyIDE2IDMyczEwLjAyNy0yLjE2IDEzLjQ0LTUuOTNhMS4xNCAxLjE0IDAgMCAwLS4xMzUtMS42NjR6Ii8&#43;PC9zdmc&#43;DQo="
    alt="">
	</div>
    
  <div class="flex-shrink-0 ml-2 leading-tight font-content-sans">
    <a class="block text-sm text-raven-800 hover:text-raven-900 hover:underline focus:underline" target="_blank" rel="noopener nofollow" title="Riddhiman" href="https://linkedin.com/in/riddhimanr">Riddhiman</a>
    <time class="text-sm text-raven-500" datetime="2022-01-08T00:00:00Z">Jan 8, 2022</time>
  </div>
</div>

        </div>
        <div>
          

<div class="flex items-center">
    <a class="flex-shrink-0 block text-raven-800 hover:text-raven-900" target="_blank" rel="noopener nofollow" title="Share on Twitter" href="https://twitter.com/intent/tweet?text=Using%20bayesian%20optimisation%20to%20tune%20a%20XGBOOST%20model%20in%20R%20by%20%40%25%21s%28%3cnil%3e%29%20https%3a%2f%2frtichoke.netlify.app%2fpost%2fbayesian_optimisation_xgboost%2f"><svg class="w-6 h-6 fill-current" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M32 6.078c-1.2.522-2.458.868-3.78 1.036 1.36-.812 2.398-2.088 2.886-3.626a13.11 13.11 0 0 1-4.16 1.588C25.742 3.794 24.026 3 22.154 3a6.56 6.56 0 0 0-6.556 6.562c0 .52.044 1.02.152 1.496-5.454-.266-10.28-2.88-13.522-6.862-.566.982-.898 2.106-.898 3.316a6.57 6.57 0 0 0 2.914 5.452 6.48 6.48 0 0 1-2.964-.808v.072c0 3.188 2.274 5.836 5.256 6.446-.534.146-1.116.216-1.72.216-.42 0-.844-.024-1.242-.112.85 2.598 3.262 4.508 6.13 4.57a13.18 13.18 0 0 1-8.134 2.798c-.538 0-1.054-.024-1.57-.1C2.906 27.93 6.35 29 10.064 29c12.072 0 18.672-10 18.672-18.668 0-.3-.01-.57-.024-.848C30.014 8.56 31.108 7.406 32 6.078z"/></svg>
</a>
    <a class="ml-3 flex-shrink-0 block text-raven-800 hover:text-raven-900" target="_blank" rel="noopener nofollow" title="Share on Facebook" href="https://www.facebook.com/dialog/share?app_id=&display=page&href=https%3a%2f%2frtichoke.netlify.app%2fpost%2fbayesian_optimisation_xgboost%2f"><svg class="w-6 h-6 fill-current" viewBox="-7 -3.5 39 39" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M30.234 0H1.765C.8.001 0 .79 0 1.766v28.47C.001 31.2.79 32 1.766 32h15.328V19.625h-4.156V14.78h4.156v-3.564c0-4.134 2.523-6.384 6.21-6.384 1.766 0 3.284.13 3.726.2v4.32h-2.543c-2.006 0-2.394.953-2.394 2.352v3.085h4.797l-.625 4.844h-4.172V32h8.14C31.21 32 32 31.2 32 30.234V1.765C32 .8 31.21 0 30.234 0z"/></svg>
</a>
  
</div>
        </div>
      </div>
    </div>
  </div><div class="ax-content ax-l-o">
    <div class="ax-l-i max-w-4xl">
      <article class="cdata">



<p>My first post in 2022! A very happy new year to anyone reading this. üòÑ</p>
<p>I was looking for a simple and effective way to tune <code>xgboost</code> models in <code>R</code> and came across this package called <a href="https://github.com/AnotherSamWilson/ParBayesianOptimization">ParBayesianOptimization</a>. Here‚Äôs a quick tutorial on how to use it to tune a <code>xgboost</code> model.</p>
<pre class="r"><code># Pacman is a package management tool 
install.packages(&quot;pacman&quot;)</code></pre>
<pre class="r"><code>library(pacman)

# p_load automatically installs packages if needed
p_load(xgboost, ParBayesianOptimization, mlbench, dplyr, skimr, recipes, resample)
## package &#39;xgboost&#39; successfully unpacked and MD5 sums checked
## 
## The downloaded binary packages are in
## 	C:\Users\riddh\AppData\Local\Temp\RtmpCy7FXq\downloaded_packages
## package &#39;rprojroot&#39; successfully unpacked and MD5 sums checked
## package &#39;diffobj&#39; successfully unpacked and MD5 sums checked
## package &#39;rematch2&#39; successfully unpacked and MD5 sums checked
## package &#39;brio&#39; successfully unpacked and MD5 sums checked
## package &#39;callr&#39; successfully unpacked and MD5 sums checked
## package &#39;desc&#39; successfully unpacked and MD5 sums checked
## package &#39;pkgload&#39; successfully unpacked and MD5 sums checked
## package &#39;praise&#39; successfully unpacked and MD5 sums checked
## package &#39;processx&#39; successfully unpacked and MD5 sums checked
## package &#39;ps&#39; successfully unpacked and MD5 sums checked
## package &#39;waldo&#39; successfully unpacked and MD5 sums checked
## package &#39;testthat&#39; successfully unpacked and MD5 sums checked
## package &#39;numDeriv&#39; successfully unpacked and MD5 sums checked
## package &#39;SparseM&#39; successfully unpacked and MD5 sums checked
## package &#39;MatrixModels&#39; successfully unpacked and MD5 sums checked
## package &#39;minqa&#39; successfully unpacked and MD5 sums checked
## package &#39;nloptr&#39; successfully unpacked and MD5 sums checked
## package &#39;RcppEigen&#39; successfully unpacked and MD5 sums checked
## package &#39;backports&#39; successfully unpacked and MD5 sums checked
## package &#39;carData&#39; successfully unpacked and MD5 sums checked
## package &#39;abind&#39; successfully unpacked and MD5 sums checked
## package &#39;pbkrtest&#39; successfully unpacked and MD5 sums checked
## package &#39;quantreg&#39; successfully unpacked and MD5 sums checked
## package &#39;lme4&#39; successfully unpacked and MD5 sums checked
## package &#39;broom&#39; successfully unpacked and MD5 sums checked
## package &#39;corrplot&#39; successfully unpacked and MD5 sums checked
## package &#39;car&#39; successfully unpacked and MD5 sums checked
## package &#39;iterators&#39; successfully unpacked and MD5 sums checked
## package &#39;ggrepel&#39; successfully unpacked and MD5 sums checked
## package &#39;ggsci&#39; successfully unpacked and MD5 sums checked
## package &#39;cowplot&#39; successfully unpacked and MD5 sums checked
## package &#39;ggsignif&#39; successfully unpacked and MD5 sums checked
## package &#39;gridExtra&#39; successfully unpacked and MD5 sums checked
## package &#39;polynom&#39; successfully unpacked and MD5 sums checked
## package &#39;rstatix&#39; successfully unpacked and MD5 sums checked
## package &#39;DiceKriging&#39; successfully unpacked and MD5 sums checked
## package &#39;foreach&#39; successfully unpacked and MD5 sums checked
## package &#39;dbscan&#39; successfully unpacked and MD5 sums checked
## package &#39;lhs&#39; successfully unpacked and MD5 sums checked
## package &#39;ggpubr&#39; successfully unpacked and MD5 sums checked
## package &#39;ParBayesianOptimization&#39; successfully unpacked and MD5 sums checked
## 
## The downloaded binary packages are in
## 	C:\Users\riddh\AppData\Local\Temp\RtmpCy7FXq\downloaded_packages
## package &#39;mlbench&#39; successfully unpacked and MD5 sums checked
## 
## The downloaded binary packages are in
## 	C:\Users\riddh\AppData\Local\Temp\RtmpCy7FXq\downloaded_packages
## package &#39;repr&#39; successfully unpacked and MD5 sums checked
## package &#39;skimr&#39; successfully unpacked and MD5 sums checked
## 
## The downloaded binary packages are in
## 	C:\Users\riddh\AppData\Local\Temp\RtmpCy7FXq\downloaded_packages
## package &#39;future.apply&#39; successfully unpacked and MD5 sums checked
## package &#39;progressr&#39; successfully unpacked and MD5 sums checked
## package &#39;SQUAREM&#39; successfully unpacked and MD5 sums checked
## package &#39;lava&#39; successfully unpacked and MD5 sums checked
## package &#39;tzdb&#39; successfully unpacked and MD5 sums checked
## package &#39;prodlim&#39; successfully unpacked and MD5 sums checked
## package &#39;timechange&#39; successfully unpacked and MD5 sums checked
## package &#39;clock&#39; successfully unpacked and MD5 sums checked
## package &#39;gower&#39; successfully unpacked and MD5 sums checked
## package &#39;hardhat&#39; successfully unpacked and MD5 sums checked
## package &#39;ipred&#39; successfully unpacked and MD5 sums checked
## package &#39;lubridate&#39; successfully unpacked and MD5 sums checked
## package &#39;timeDate&#39; successfully unpacked and MD5 sums checked
## package &#39;recipes&#39; successfully unpacked and MD5 sums checked
## 
## The downloaded binary packages are in
## 	C:\Users\riddh\AppData\Local\Temp\RtmpCy7FXq\downloaded_packages
## package &#39;resample&#39; successfully unpacked and MD5 sums checked
## 
## The downloaded binary packages are in
## 	C:\Users\riddh\AppData\Local\Temp\RtmpCy7FXq\downloaded_packages</code></pre>
<div id="data-prep" class="section level2">
<h2>Data prep</h2>
<pre class="r"><code># Load up some data
data(&quot;BostonHousing2&quot;)</code></pre>
<pre class="r"><code># Data summary
skim(BostonHousing2)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-4">Table 1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">BostonHousing2</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">506</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">19</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">factor</td>
<td align="left">2</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">17</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: factor</strong></p>
<table>
<colgroup>
<col width="15%" />
<col width="11%" />
<col width="15%" />
<col width="8%" />
<col width="10%" />
<col width="38%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="left">ordered</th>
<th align="right">n_unique</th>
<th align="left">top_counts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">town</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">92</td>
<td align="left">Cam: 30, Bos: 23, Lyn: 22, Bos: 19</td>
</tr>
<tr class="even">
<td align="left">chas</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">2</td>
<td align="left">0: 471, 1: 35</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<colgroup>
<col width="14%" />
<col width="10%" />
<col width="14%" />
<col width="8%" />
<col width="8%" />
<col width="7%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="6%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">tract</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">2700.36</td>
<td align="right">1380.04</td>
<td align="right">1.00</td>
<td align="right">1303.25</td>
<td align="right">3393.50</td>
<td align="right">3739.75</td>
<td align="right">5082.00</td>
<td align="left">‚ñÖ‚ñÇ‚ñÇ‚ñá‚ñÇ</td>
</tr>
<tr class="even">
<td align="left">lon</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">-71.06</td>
<td align="right">0.08</td>
<td align="right">-71.29</td>
<td align="right">-71.09</td>
<td align="right">-71.05</td>
<td align="right">-71.02</td>
<td align="right">-70.81</td>
<td align="left">‚ñÅ‚ñÇ‚ñá‚ñÇ‚ñÅ</td>
</tr>
<tr class="odd">
<td align="left">lat</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">42.22</td>
<td align="right">0.06</td>
<td align="right">42.03</td>
<td align="right">42.18</td>
<td align="right">42.22</td>
<td align="right">42.25</td>
<td align="right">42.38</td>
<td align="left">‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñÅ</td>
</tr>
<tr class="even">
<td align="left">medv</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">22.53</td>
<td align="right">9.20</td>
<td align="right">5.00</td>
<td align="right">17.02</td>
<td align="right">21.20</td>
<td align="right">25.00</td>
<td align="right">50.00</td>
<td align="left">‚ñÇ‚ñá‚ñÖ‚ñÅ‚ñÅ</td>
</tr>
<tr class="odd">
<td align="left">cmedv</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">22.53</td>
<td align="right">9.18</td>
<td align="right">5.00</td>
<td align="right">17.02</td>
<td align="right">21.20</td>
<td align="right">25.00</td>
<td align="right">50.00</td>
<td align="left">‚ñÇ‚ñá‚ñÖ‚ñÅ‚ñÅ</td>
</tr>
<tr class="even">
<td align="left">crim</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3.61</td>
<td align="right">8.60</td>
<td align="right">0.01</td>
<td align="right">0.08</td>
<td align="right">0.26</td>
<td align="right">3.68</td>
<td align="right">88.98</td>
<td align="left">‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td>
</tr>
<tr class="odd">
<td align="left">zn</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">11.36</td>
<td align="right">23.32</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">12.50</td>
<td align="right">100.00</td>
<td align="left">‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td>
</tr>
<tr class="even">
<td align="left">indus</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">11.14</td>
<td align="right">6.86</td>
<td align="right">0.46</td>
<td align="right">5.19</td>
<td align="right">9.69</td>
<td align="right">18.10</td>
<td align="right">27.74</td>
<td align="left">‚ñá‚ñÜ‚ñÅ‚ñá‚ñÅ</td>
</tr>
<tr class="odd">
<td align="left">nox</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.55</td>
<td align="right">0.12</td>
<td align="right">0.38</td>
<td align="right">0.45</td>
<td align="right">0.54</td>
<td align="right">0.62</td>
<td align="right">0.87</td>
<td align="left">‚ñá‚ñá‚ñÜ‚ñÖ‚ñÅ</td>
</tr>
<tr class="even">
<td align="left">rm</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">6.28</td>
<td align="right">0.70</td>
<td align="right">3.56</td>
<td align="right">5.89</td>
<td align="right">6.21</td>
<td align="right">6.62</td>
<td align="right">8.78</td>
<td align="left">‚ñÅ‚ñÇ‚ñá‚ñÇ‚ñÅ</td>
</tr>
<tr class="odd">
<td align="left">age</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">68.57</td>
<td align="right">28.15</td>
<td align="right">2.90</td>
<td align="right">45.02</td>
<td align="right">77.50</td>
<td align="right">94.07</td>
<td align="right">100.00</td>
<td align="left">‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñá</td>
</tr>
<tr class="even">
<td align="left">dis</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3.80</td>
<td align="right">2.11</td>
<td align="right">1.13</td>
<td align="right">2.10</td>
<td align="right">3.21</td>
<td align="right">5.19</td>
<td align="right">12.13</td>
<td align="left">‚ñá‚ñÖ‚ñÇ‚ñÅ‚ñÅ</td>
</tr>
<tr class="odd">
<td align="left">rad</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">9.55</td>
<td align="right">8.71</td>
<td align="right">1.00</td>
<td align="right">4.00</td>
<td align="right">5.00</td>
<td align="right">24.00</td>
<td align="right">24.00</td>
<td align="left">‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÉ</td>
</tr>
<tr class="even">
<td align="left">tax</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">408.24</td>
<td align="right">168.54</td>
<td align="right">187.00</td>
<td align="right">279.00</td>
<td align="right">330.00</td>
<td align="right">666.00</td>
<td align="right">711.00</td>
<td align="left">‚ñá‚ñá‚ñÉ‚ñÅ‚ñá</td>
</tr>
<tr class="odd">
<td align="left">ptratio</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">18.46</td>
<td align="right">2.16</td>
<td align="right">12.60</td>
<td align="right">17.40</td>
<td align="right">19.05</td>
<td align="right">20.20</td>
<td align="right">22.00</td>
<td align="left">‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñá</td>
</tr>
<tr class="even">
<td align="left">b</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">356.67</td>
<td align="right">91.29</td>
<td align="right">0.32</td>
<td align="right">375.38</td>
<td align="right">391.44</td>
<td align="right">396.22</td>
<td align="right">396.90</td>
<td align="left">‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá</td>
</tr>
<tr class="odd">
<td align="left">lstat</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">12.65</td>
<td align="right">7.14</td>
<td align="right">1.73</td>
<td align="right">6.95</td>
<td align="right">11.36</td>
<td align="right">16.96</td>
<td align="right">37.97</td>
<td align="left">‚ñá‚ñá‚ñÖ‚ñÇ‚ñÅ</td>
</tr>
</tbody>
</table>
<p>Looks like there is are two factor variables. We‚Äôll need to convert them into numeric variables before we proceed. I‚Äôll use the <code>recipes</code> package to one-hot encode them.</p>
<pre class="r"><code># Predicting median house prices
rec &lt;- recipe(cmedv ~ ., data = BostonHousing2) %&gt;%
  
  # Collapse categories where population is &lt; 3%
  step_other(town, chas, threshold = .03, other = &quot;Other&quot;) %&gt;% 
  
  # Create dummy variables for all factor variables 
  step_dummy(all_nominal_predictors())

# Train the recipe on the data set
prep &lt;- prep(rec, training = BostonHousing2)

# Create the final model matrix
model_df &lt;- bake(prep, new_data = BostonHousing2)</code></pre>
<pre class="r"><code># All levels have been one hot encoded and separate columns have been appended to the model matrix
colnames(model_df)
##  [1] &quot;tract&quot;                  &quot;lon&quot;                    &quot;lat&quot;                   
##  [4] &quot;medv&quot;                   &quot;crim&quot;                   &quot;zn&quot;                    
##  [7] &quot;indus&quot;                  &quot;nox&quot;                    &quot;rm&quot;                    
## [10] &quot;age&quot;                    &quot;dis&quot;                    &quot;rad&quot;                   
## [13] &quot;tax&quot;                    &quot;ptratio&quot;                &quot;b&quot;                     
## [16] &quot;lstat&quot;                  &quot;cmedv&quot;                  &quot;town_Boston.Savin.Hill&quot;
## [19] &quot;town_Cambridge&quot;         &quot;town_Lynn&quot;              &quot;town_Newton&quot;           
## [22] &quot;town_Other&quot;             &quot;chas_X1&quot;</code></pre>
<p>Next, we can use the <code>resample</code> package to create test/train splits.</p>
<pre class="r"><code>splits &lt;- rsample::initial_split(model_df, prop = 0.7)

# Training set
train_df &lt;- rsample::training(splits)

# Test set
test_df &lt;- rsample::testing(splits)</code></pre>
<pre class="r"><code>dim(train_df)
## [1] 354  23</code></pre>
<pre class="r"><code>dim(test_df)
## [1] 152  23</code></pre>
</div>
<div id="finding-optimal-parameters" class="section level2">
<h2>Finding optimal parameters</h2>
<p>Now we can start to run some optimisations using the <code>ParBayesianOptimization</code> package.</p>
<pre class="r"><code># The xgboost interface accepts matrices 
X &lt;- train_df %&gt;%
  # Remove the target variable
  select(!medv, !cmedv) %&gt;%
  as.matrix()

# Get the target variable
y &lt;- train_df %&gt;%
  pull(cmedv)</code></pre>
<pre class="r"><code># Cross validation folds
folds &lt;- list(fold1 = as.integer(seq(1, nrow(X), by = 5)),
              fold2 = as.integer(seq(2, nrow(X), by = 5)))</code></pre>
<p>We‚Äôll need an objective function which can be fed to the optimiser. We‚Äôll use the value of the evaluation metric from <code>xgb.cv()</code> as the value that needs to be optimised.</p>
<pre class="r"><code># Function must take the hyper-parameters as inputs
obj_func &lt;- function(eta, max_depth, min_child_weight, subsample, lambda, alpha) {
  
  param &lt;- list(
    
    # Hyter parameters 
    eta = eta,
    max_depth = max_depth,
    min_child_weight = min_child_weight,
    subsample = subsample,
    lambda = lambda,
    alpha = alpha,
    
    # Tree model 
    booster = &quot;gbtree&quot;,
    
    # Regression problem 
    objective = &quot;reg:squarederror&quot;,
    
    # Use the Mean Absolute Percentage Error
    eval_metric = &quot;mape&quot;)
  
  xgbcv &lt;- xgb.cv(params = param,
                  data = X,
                  label = y,
                  nround = 50,
                  folds = folds,
                  prediction = TRUE,
                  early_stopping_rounds = 5,
                  verbose = 0,
                  maximize = F)
  
  lst &lt;- list(
    
    # First argument must be named as &quot;Score&quot;
    # Function finds maxima so inverting the output
    Score = -min(xgbcv$evaluation_log$test_mape_mean),
    
    # Get number of trees for the best performing model
    nrounds = xgbcv$best_iteration
  )
  
  return(lst)
}</code></pre>
<p>Once we have the objective function, we‚Äôll need to define some bounds for the optimiser to search within.</p>
<pre class="r"><code>bounds &lt;- list(eta = c(0.001, 0.2),
               max_depth = c(1L, 10L),
               min_child_weight = c(1, 50),
               subsample = c(0.1, 1),
               lambda = c(1, 10),
               alpha = c(1, 10))</code></pre>
<p>We can now run the optimiser to find a set of optimal hyper-parameters.</p>
<pre class="r"><code>set.seed(1234)
bayes_out &lt;- bayesOpt(FUN = obj_func, bounds = bounds, initPoints = length(bounds) + 2, iters.n = 3)</code></pre>
<pre class="r"><code># Show relevant columns from the summary object 
bayes_out$scoreSummary[1:5, c(3:8, 13)]
##           eta max_depth min_child_weight subsample   lambda    alpha      Score
## 1: 0.13392137         8         4.913332 0.2105925 4.721124 3.887629 -0.0902970
## 2: 0.19400811         2        25.454160 0.9594105 9.329695 3.173695 -0.1402720
## 3: 0.16079775         2        14.035652 0.5118349 1.229953 5.093530 -0.1475580
## 4: 0.08957707         4        12.534842 0.3844404 4.358837 1.788342 -0.1410245
## 5: 0.02876388         4        36.586761 0.8107181 6.137100 6.039125 -0.3061535</code></pre>
<pre class="r"><code># Get best parameters
data.frame(getBestPars(bayes_out))
##         eta max_depth min_child_weight subsample lambda alpha
## 1 0.1905414         8         1.541476 0.8729207      1     1</code></pre>
</div>
<div id="fitting-the-model" class="section level2">
<h2>Fitting the model</h2>
<p>We can now fit a model and check how well these parameters work.</p>
<pre class="r"><code># Combine best params with base params
opt_params &lt;- append(list(booster = &quot;gbtree&quot;, 
                          objective = &quot;reg:squarederror&quot;, 
                          eval_metric = &quot;mae&quot;), 
                     getBestPars(bayes_out))

# Run cross validation 
xgbcv &lt;- xgb.cv(params = opt_params,
                data = X,
                label = y,
                nround = 100,
                folds = folds,
                prediction = TRUE,
                early_stopping_rounds = 5,
                verbose = 0,
                maximize = F)

# Get optimal number of rounds
nrounds = xgbcv$best_iteration

# Fit a xgb model
mdl &lt;- xgboost(data = X, label = y, 
               params = opt_params, 
               maximize = F, 
               early_stopping_rounds = 5, 
               nrounds = nrounds, 
               verbose = 0)</code></pre>
<pre class="r"><code># Evaluate performance 
actuals &lt;- test_df$cmedv
predicted &lt;- test_df %&gt;%
  select_at(mdl$feature_names) %&gt;%
  as.matrix %&gt;%
  predict(mdl, newdata = .)</code></pre>
<pre class="r"><code># Compute MAPE
mean(abs(actuals - predicted)/actuals)
## [1] 0.01648831</code></pre>
</div>
<div id="compare-with-grid-search" class="section level2">
<h2>Compare with grid search</h2>
<pre class="r"><code>grd &lt;- expand.grid(
  eta = seq(0.001, 0.2, length.out = 5),
  max_depth = seq(2L, 10L, by = 1),
  min_child_weight = seq(1, 25, length.out = 3),
  subsample = c(0.25, 0.5, 0.75, 1),
  lambda = c(1, 5, 10),
  alpha = c(1, 5, 10))

dim(grd)</code></pre>
<pre class="r"><code>grd_out &lt;- apply(grd, 1, function(par){
  
    par &lt;- append(par, list(booster = &quot;gbtree&quot;,objective = &quot;reg:squarederror&quot;,eval_metric = &quot;mae&quot;))
    mdl &lt;- xgboost(data = X, label = y, params = par, nrounds = 50, early_stopping_rounds = 5, maximize = F, verbose = 0)
    lst &lt;- data.frame(par, score = mdl$best_score)

    return(lst)
  })

grd_out &lt;- do.call(rbind, grd_out)</code></pre>
<pre class="r"><code>best_par &lt;- grd_out %&gt;%
  data.frame() %&gt;%
  arrange(score) %&gt;%
  .[1,]</code></pre>
<pre class="r"><code># Fit final model
params &lt;- as.list(best_par[-length(best_par)])
xgbcv &lt;- xgb.cv(params = params,
                data = X,
                label = y,
                nround = 100,
                folds = folds,
                prediction = TRUE,
                early_stopping_rounds = 5,
                verbose = 0,
                maximize = F)

nrounds = xgbcv$best_iteration

mdl &lt;- xgboost(data = X, 
               label = y, 
               params = params, 
               maximize = F, 
               early_stopping_rounds = 5, 
               nrounds = nrounds, 
               verbose = 0)</code></pre>
<pre class="r"><code># Evaluate on test set
act &lt;- test_df$medv
pred &lt;- test_df %&gt;%
  select_at(mdl$feature_names) %&gt;%
  as.matrix %&gt;%
  predict(mdl, newdata = .)</code></pre>
<pre class="r"><code>mean(abs(act - pred)/act)
## [1] 0.01951497</code></pre>
<p>While both the methods offer similar final results, the bayesian optimiser completed its search in less than a minute where as the grid search took over seven minutes. Also, I find that I can use bayesian optimisation to search a larger parameter space more quickly than a traditional grid search.</p>
<p><em>Thoughts? Comments? Helpful? Not helpful? Like to see anything else added in here? Let me know!</em></p>
</div>

      </article>
      

      
<div id="disqus_thread"></div>
<script>
var disqus_config = function () {
  this.page.url = "https://rtichoke.netlify.app/post/bayesian_optimisation_xgboost/";
  this.page.identifier = "b8330eeb3c4ccc172570e3945338fe11";
  this.page.title = "Using bayesian optimisation to tune a XGBOOST model in R";
};

(function() {
  window.setTimeout(function() {
    var d = document;
    var s = d.createElement('script');
    s.src = 'https://rtichoke.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    s.setAttribute("defer", "");
    d.body.appendChild(s);
  }, 2500);
})();
</script>

    </div>
  </div>
</div>

  </main>
  <footer class="footer">
  <div class="ax-l-i max-w-6xl">
    <nav class="flex items-center justify-center">
    </nav>
    <div class="footer-social flex items-center justify-center mt-4">
      <a class="block mx-3 w-6 h-6 text-raven-700 hover:text-raven-900" target="_blank" rel="noopener nofollow" title="Twitter" href="https://twitter.com/r0yr2"><svg class="fill-current" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M32 6.078c-1.2.522-2.458.868-3.78 1.036 1.36-.812 2.398-2.088 2.886-3.626a13.11 13.11 0 0 1-4.16 1.588C25.742 3.794 24.026 3 22.154 3a6.56 6.56 0 0 0-6.556 6.562c0 .52.044 1.02.152 1.496-5.454-.266-10.28-2.88-13.522-6.862-.566.982-.898 2.106-.898 3.316a6.57 6.57 0 0 0 2.914 5.452 6.48 6.48 0 0 1-2.964-.808v.072c0 3.188 2.274 5.836 5.256 6.446-.534.146-1.116.216-1.72.216-.42 0-.844-.024-1.242-.112.85 2.598 3.262 4.508 6.13 4.57a13.18 13.18 0 0 1-8.134 2.798c-.538 0-1.054-.024-1.57-.1C2.906 27.93 6.35 29 10.064 29c12.072 0 18.672-10 18.672-18.668 0-.3-.01-.57-.024-.848C30.014 8.56 31.108 7.406 32 6.078z"/></svg></a>
      <a class="block mx-3 w-6 h-6 text-raven-700 hover:text-raven-900" target="_blank" rel="noopener nofollow" title="GitHub" href="https://github.com/royr2"><svg class="fill-current" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M15.998 0C7.164 0 0 7.35 0 16.417 0 23.67 4.584 29.82 10.944 31.994c.8.15 1.092-.356 1.092-.79l-.022-2.792c-4.45.99-5.4-2.202-5.4-2.202-.726-1.896-1.776-2.4-1.776-2.4-1.454-1.018.108-.998.108-.998 1.606.117 2.45 1.693 2.45 1.693 1.428 2.507 3.746 1.784 4.658 1.363.144-1.06.558-1.784 1.016-2.195-3.552-.415-7.288-1.823-7.288-8.113 0-1.792.624-3.258 1.648-4.406-.166-.415-.714-2.085.156-4.344 0 0 1.344-.44 4.4 1.683 1.276-.364 2.644-.546 4.006-.552a14.98 14.98 0 0 1 4.006.554C23.062 6.37 24.404 6.8 24.404 6.8c.872 2.26.324 3.93.16 4.344 1.026 1.148 1.644 2.614 1.644 4.406 0 6.306-3.74 7.694-7.304 8.1.574.507 1.086 1.51 1.086 3.04l-.02 4.503c0 .44.288.95 1.1.788C27.42 29.817 32 23.667 32 16.417 32 7.35 24.836 0 15.998 0z"/></svg></a>
      <a class="block mx-3 w-6 h-6 text-raven-700 hover:text-raven-900" target="_blank" rel="noopener nofollow" title="LinkedIn" href="https://www.linkedin.com/in/riddhimanr"><svg class="fill-current" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M29.692 0H2.308A2.31 2.31 0 0 0 0 2.308v27.384A2.31 2.31 0 0 0 2.308 32h27.384A2.31 2.31 0 0 0 32 29.692V2.308A2.31 2.31 0 0 0 29.692 0zM11.35 24.188H7.454V12.464h3.897v11.723zM9.402 10.863h-.025c-1.308 0-2.153-.9-2.153-2.025 0-1.15.872-2.026 2.205-2.026s2.153.875 2.18 2.026c0 1.125-.846 2.025-2.205 2.025zm16 13.324h-3.896v-6.272c0-1.576-.564-2.65-1.974-2.65-1.076 0-1.717.725-2 1.425-.103.25-.128.6-.128.95v6.547h-3.896V12.464h3.896v1.66c.518-.8 1.444-1.935 3.512-1.935 2.564 0 4.486 1.676 4.486 5.276v6.722z"/></svg></a>
    </div>

    <div class="footer-copyright text-sm text-center text-gray-400 mt-4">
      &#169; 2023-2023 R&#39;tichoke
    </div>
    <div class="text-sm sm:text-xs text-center text-gray-400 mt-2">
      Powered by <a href="https://www.axiomtheme.com/?utm_source=theme-footer&utm_medium=website&utm_campaign=referral">Axiom</a>
    </div>
  </div>
</footer>

<script src="/bundle.js?v=1679930341"></script>


</body>
</html>
