<!doctype html>
<html lang="en-us">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name="hugo-theme" content="Axiom 0.8.0">



  <link rel="icon" type="image/png" sizes="32x32" href="/">
  <link rel="icon" type="image/x-icon" href="/">
  <link rel="apple-touch-icon" href="/">
  <link rel="canonical" href="https://rtichoke.netlify.app/post/assessing_score_reliability/">
<link rel="preload" as="style" href="/bundle.css?v=1679930341" media="all">
<link rel="stylesheet" href="/bundle.css?v=1679930341" media="all">
<style>.cdata pre{background-color:#1f2937;color:#e5e7eb}.cdata :not(pre)>code{background-color:#f3f4f6;color:#7c3aed}.chroma .err{background-color:#991b1b;color:#fecaca}.chroma .hl{background-color:#374151}.chroma .ln{color:#9ca3af}.chroma .k,.chroma .kc,.chroma .kd,.chroma .kn,.chroma .kp,.chroma .kr{color:#60a5fa}.chroma .kt{color:#a78bfa}.chroma .na,.chroma .nb{color:#fbbf24}.chroma .nc{color:#f87171}.chroma .no{color:#34d399}.chroma .nd{color:#f87171}.chroma .ne{color:#f87171}.chroma .nf{color:#fbbf24}.chroma .nt{color:#f87171}.chroma .l{color:#a78bfa}.chroma .dl,.chroma .ld,.chroma .s,.chroma .s2,.chroma .sa,.chroma .sb,.chroma .sc,.chroma .sd{color:#34d399}.chroma .se{color:#9ca3af}.chroma .s1,.chroma .sh,.chroma .si,.chroma .sr,.chroma .ss,.chroma .sx{color:#34d399}.chroma .il,.chroma .m,.chroma .mb,.chroma .mf,.chroma .mh,.chroma .mi,.chroma .mo{color:#a78bfa}.chroma .o,.chroma .ow{color:#93c5fd}.chroma .c,.chroma .c1,.chroma .ch,.chroma .cm,.chroma .cp,.chroma .cpf,.chroma .cs,.chroma .p{color:#9ca3af}.chroma .ge{font-style:italic}.chroma .gs{font-weight:700}
</style>



<title>Using bootstrapped sampling to assess variability in score predictions  : R&#39;tichoke</title>

<meta property="og:title" content="Using bootstrapped sampling to assess variability in score predictions ">
<meta property="og:site_name" content="R&#39;tichoke">
<meta property="og:url" content="https://rtichoke.netlify.app/post/assessing_score_reliability/">
<link rel="image_src" href="https://rtichoke.netlify.app/">
<meta property="og:image" content="https://rtichoke.netlify.app/">
<meta property="og:image:width" content="">
<meta property="og:image:height" content="">
<meta property="og:type" content="article">
<meta property="og:locale" content="en_us">
<meta property="og:description" content="How to assess reliability of a credit scoring model using bootstrapped sampling">
<meta name="description" content="How to assess reliability of a credit scoring model using bootstrapped sampling">
<meta property="og:updated_time" content="2021-09-30T00:00:00Z">
<meta property="fb:app_id" content="">
<meta name="author" content="Riddhiman">
<meta property="article:author" content="https://linkedin.com/in/riddhimanr">
<meta property="article:published_time" content="2021-09-30T00:00:00Z">
<meta property="article:modified_time" content="2021-09-30T00:00:00Z">
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "Using bootstrapped sampling to assess variability in score predictions ",
  "alternativeHeadline": "Credit risk series (Post #3)",
  "url": "https://rtichoke.netlify.app/post/assessing_score_reliability/",
  "image": "https://rtichoke.netlify.app/",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://rtichoke.netlify.app/post/assessing_score_reliability/"
  },
  "description": "How to assess reliability of a credit scoring model using bootstrapped sampling",
  "author": {
    "@type": "Person",
    "name": "Riddhiman"
  },
  "publisher": {
    "@type": "Organization",
    "name": "R'tichoke",
    "logo": {
      "@type": "ImageObject",
      "url": "https://rtichoke.netlify.app/"
    }
  },
  "datePublished": "2021-09-30T00:00:00Z",
  "dateModified": "2021-09-30T00:00:00Z",
  "articleBody": "\r\n\r\n\r\n\u003cp\u003eWhen building risk scorecards, apart from the variety of performance metrics, analysts also assess something known as \u003ccode\u003erisk-ranking\u003c/code\u003e i.e.Â whether or not the observed event rates increase (or decrease) monotonically with increasing (or decreasing) scores. Sometimes, models are not able to risk-rank borrowers in the tails (regions of very high or very low scores). While this is expected, it would be nice if we could quantify this effect. One way to do this would be to use bootstrapped samples to assess variability in model predictions.\u003c/p\u003e\r\n\u003cdiv id=\"basic-idea\" class=\"section level2\"\u003e\r\n\u003ch2\u003eBasic idea\u003c/h2\u003e\r\n\u003cp\u003eThe underlying idea is very simple - less available data for estimation equates to lower quality of estimation. As a simple example, we can observe this effect when trying to estimate quantiles of a probability distribution.\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Number of samples to be drawn from a probability distribution\r\nn_samples \u0026lt;- 1000\r\n\r\n# Number of times, sampling should be repeated\r\nrepeats \u0026lt;- 100\r\n\r\n# Mean and std-dev for a standard normal distribution\r\nmu \u0026lt;- 5\r\nstd_dev \u0026lt;- 2\r\n\r\n# Sample\r\nsamples \u0026lt;- rnorm(n_samples * repeats, mean = 10)\r\n\r\n# Fit into a matrix like object with `n_samples\u0026#39; number of rows \r\n# and `repeats` number of columns\r\nsamples \u0026lt;- matrix(samples, nrow = n_samples, ncol = repeats)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Compute mean across each column\r\nsample_means \u0026lt;- apply(samples, 1, mean)\r\n\r\n# Similarly, compute 75% and 95% quantile across each column\r\nsample_75_quantile \u0026lt;- apply(samples, 1, quantile, p = 0.75)\r\nsample_95_quantile \u0026lt;- apply(samples, 1, quantile, p = 0.95)\r\nsample_99_quantile \u0026lt;- apply(samples, 1, quantile, p = 0.99)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003esd(sample_means)/mean(sample_means)\r\n## [1] 0.009803362\r\nsd(sample_75_quantile)/mean(sample_75_quantile)\r\n## [1] 0.01259488\r\nsd(sample_95_quantile)/mean(sample_75_quantile)\r\n## [1] 0.01929882\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003ecombined_vec \u0026lt;- c(sample_means, sample_75_quantile, sample_95_quantile, sample_99_quantile)\r\n\r\nplot(density(sample_means), \r\n     col = \u0026quot;#6F69AC\u0026quot;, \r\n     lwd = 3, \r\n     main = \u0026quot;Estimating the mean vs tail quantiles\u0026quot;, \r\n     xlab = \u0026quot;\u0026quot;, \r\n     xlim = c(min(combined_vec), max(combined_vec)))\r\n\r\nlines(density(sample_75_quantile), col = \u0026quot;#95DAC1\u0026quot;, lwd = 3)\r\nlines(density(sample_95_quantile), col = \u0026quot;#FFEBA1\u0026quot;, lwd = 3)\r\nlines(density(sample_99_quantile), col = \u0026quot;#FD6F96\u0026quot;, lwd = 3)\r\ngrid()\r\n\r\nlegend(\u0026quot;topright\u0026quot;, \r\n       fill = c(\u0026quot;#6F69AC\u0026quot;, \u0026quot;#95DAC1\u0026quot;, \u0026quot;#FFEBA1\u0026quot;, \u0026quot;#FD6F96\u0026quot;), \r\n       legend = c(\u0026quot;Mean\u0026quot;, \u0026quot;75% Quantile\u0026quot;, \u0026quot;95% Quantile\u0026quot;, \u0026quot;99% Quantile\u0026quot;), \r\n       cex = 0.7)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003e\u003cimg src=\"chart1-1.png\" /\u003e\u003c/p\u003e\r\n\u003cp\u003eIt is easy to notice that the uncertainty in estimating the sample 99% quantile is much higher than the uncertainty in estimating the sample mean. We will now try to extend this idea to a scorecard model.\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"libraries\" class=\"section level2\"\u003e\r\n\u003ch2\u003eLibraries\u003c/h2\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e#install.packages(\u0026quot;pacman\u0026quot;)\r\npacman::p_load(dplyr, magrittr, rsample, ggplot2)\r\n## Installing package into \u0026#39;C:/Users/riddh/AppData/Local/R/win-library/4.2\u0026#39;\r\n## (as \u0026#39;lib\u0026#39; is unspecified)\r\n## also installing the dependencies \u0026#39;listenv\u0026#39;, \u0026#39;parallelly\u0026#39;, \u0026#39;future\u0026#39;, \u0026#39;globals\u0026#39;, \u0026#39;warp\u0026#39;, \u0026#39;cpp11\u0026#39;, \u0026#39;furrr\u0026#39;, \u0026#39;purrr\u0026#39;, \u0026#39;slider\u0026#39;, \u0026#39;tidyr\u0026#39;\r\n## Warning: unable to access index for repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2:\r\n##   cannot open URL \u0026#39;http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2/PACKAGES\u0026#39;\r\n## package \u0026#39;listenv\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;parallelly\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;future\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;globals\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;warp\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;cpp11\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;furrr\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;purrr\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;slider\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;tidyr\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;rsample\u0026#39; successfully unpacked and MD5 sums checked\r\n## \r\n## The downloaded binary packages are in\r\n## \tC:\\Users\\riddh\\AppData\\Local\\Temp\\Rtmp0qD5ZX\\downloaded_packages\r\n## \r\n## rsample installed\r\n## Installing package into \u0026#39;C:/Users/riddh/AppData/Local/R/win-library/4.2\u0026#39;\r\n## (as \u0026#39;lib\u0026#39; is unspecified)\r\n## also installing the dependencies \u0026#39;gtable\u0026#39;, \u0026#39;isoband\u0026#39;\r\n## Warning: unable to access index for repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2:\r\n##   cannot open URL \u0026#39;http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2/PACKAGES\u0026#39;\r\n## package \u0026#39;gtable\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;isoband\u0026#39; successfully unpacked and MD5 sums checked\r\n## package \u0026#39;ggplot2\u0026#39; successfully unpacked and MD5 sums checked\r\n## \r\n## The downloaded binary packages are in\r\n## \tC:\\Users\\riddh\\AppData\\Local\\Temp\\Rtmp0qD5ZX\\downloaded_packages\r\n## \r\n## ggplot2 installed\u003c/code\u003e\u003c/pre\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"sample-data\" class=\"section level2\"\u003e\r\n\u003ch2\u003eSample data\u003c/h2\u003e\r\n\u003cp\u003eAs in previous posts, weâ€™ll use a small sample \u003ca href=\"https://github.com/royr2/blog/blob/main/download/credit_sample.csv\"\u003e(download here)\u003c/a\u003e of the \u003cstrong\u003eLending Club\u003c/strong\u003e dataset available on \u003ca href=\"https://www.kaggle.com/wordsforthewise/lending-club\"\u003eKaggle\u003c/a\u003e.\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003esample \u0026lt;- read.csv(\u0026quot;credit_sample.csv\u0026quot;)\u003c/code\u003e\u003c/pre\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"creating-a-target\" class=\"section level2\"\u003e\r\n\u003ch2\u003eCreating a target\u003c/h2\u003e\r\n\u003cp\u003eThe next step is to create a target (dependent variable) to model for.\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Mark which loan status will be tagged as default\r\ncodes \u0026lt;- c(\u0026quot;Charged Off\u0026quot;, \u0026quot;Does not meet the credit policy. Status:Charged Off\u0026quot;)\r\n\r\n# Apply above codes and create target\r\nsample %\u0026lt;\u0026gt;% mutate(bad_flag = ifelse(loan_status %in% codes, 1, 0))\r\n\r\n# Replace missing values with a default value\r\nsample[is.na(sample)] \u0026lt;- -1\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Get summary tally\r\ntable(sample$bad_flag)\r\n## \r\n##    0    1 \r\n## 8838 1162\u003c/code\u003e\u003c/pre\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"sampling\" class=\"section level2\"\u003e\r\n\u003ch2\u003eSampling\u003c/h2\u003e\r\n\u003cp\u003eWeâ€™ll use \u003ccode\u003ebootstrapped sampling\u003c/code\u003e to create multiple training sets. We will then repeatedly train a model on each training set and assess the variability in volatile model predictions across score ranges. Weâ€™ll use the \u003ccode\u003ebootstraps()\u003c/code\u003e function in the \u003ccode\u003ersample\u003c/code\u003e package.\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Create 100 samples\r\nboot_sample \u0026lt;- bootstraps(data = sample, times = 100)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003ehead(boot_sample, 3)\r\n## # A tibble: 3 Ã— 2\r\n##   splits               id          \r\n##   \u0026lt;list\u0026gt;               \u0026lt;chr\u0026gt;       \r\n## 1 \u0026lt;split [10000/3685]\u0026gt; Bootstrap001\r\n## 2 \u0026lt;split [10000/3676]\u0026gt; Bootstrap002\r\n## 3 \u0026lt;split [10000/3682]\u0026gt; Bootstrap003\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003eboot_sample$splits[[1]]\r\n## \u0026lt;Analysis/Assess/Total\u0026gt;\r\n## \u0026lt;10000/3685/10000\u0026gt;\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003eEach row represents a separate bootstrapped sample whereas within each sample, there are two sub-samples namely an \u003ccode\u003eanalysis set\u003c/code\u003e and an \u003ccode\u003eassessment set\u003c/code\u003e. To retrieve a bootstrapped sample as a \u003ccode\u003edata.frame\u003c/code\u003e, the package provides two helper functions - \u003ccode\u003eanalysis()\u003c/code\u003e and \u003ccode\u003eassessment()\u003c/code\u003e\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Show the first 5 rows and 5 columns of the first sample\r\nanalysis(boot_sample$splits[[1]]) %\u0026gt;% .[1:5, 1:5]\r\n##         V1        id member_id loan_amnt funded_amnt\r\n## 7967 46001  63407248        -1     16000       16000\r\n## 5186 86313 125551278        -1     10000       10000\r\n## 9323 95781   1414849        -1     12000       12000\r\n## 4080 56615  44065228        -1      4000        4000\r\n## 2695 80522 125327254        -1     28000       28000\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003eThe \u003ca href=\"https://rsample.tidymodels.org/articles/rsample.html\"\u003egetting started\u003c/a\u003e page of the \u003ccode\u003ersample\u003c/code\u003e package has additional information.\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"creating-a-modeling-function\" class=\"section level2\"\u003e\r\n\u003ch2\u003eCreating a modeling function\u003c/h2\u003e\r\n\u003cp\u003eWeâ€™ll use a simple \u003ccode\u003eglm()\u003c/code\u003e model for illustrative purposes. First, weâ€™ll need to create a function that fits such a model to a given dataset\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003eglm_model \u0026lt;- function(df){\r\n  \r\n  # Fit a simple model with a set specification\r\n  mdl \u0026lt;- glm(bad_flag ~\r\n               loan_amnt + funded_amnt + annual_inc + delinq_2yrs +\r\n               inq_last_6mths + mths_since_last_delinq + fico_range_low +\r\n               mths_since_last_record + revol_util + total_pymnt,\r\n             family = \u0026quot;binomial\u0026quot;,\r\n             data = df)\r\n  \r\n  # Return fitted values\r\n  return(predict(mdl))\r\n}\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Test the function\r\n\r\n# Retrieve a data frame\r\ntrain \u0026lt;- analysis(boot_sample$splits[[1]])\r\n\r\n# Predict\r\npred \u0026lt;- glm_model(train)\r\n\r\n# Check output\r\nrange(pred)  # Output is on log odds scale\r\n## [1] -8.612908  1.279721\u003c/code\u003e\u003c/pre\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"fitting-the-model-repeatedly\" class=\"section level2\"\u003e\r\n\u003ch2\u003eFitting the model repeatedly\u003c/h2\u003e\r\n\u003cp\u003eNow we need to fit the model repeatedly on each of the bootstrapped samples and store the fitted values. And since we are using \u003ccode\u003eR\u003c/code\u003e, for-loops are not allowed ðŸ˜†\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# First apply the glm fitting function to each of the sample\r\n# Note the use of lapply\r\noutput \u0026lt;- lapply(boot_sample$splits, function(x){\r\n  train \u0026lt;- analysis(x)\r\n  pred \u0026lt;- glm_model(train)\r\n\r\n  return(pred)\r\n})\r\n\r\n# Collate all predictions into a vector \r\nboot_preds \u0026lt;- do.call(c, output)\r\nrange(boot_preds)\r\n## [1] -131.359579    5.070397\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Get outliers\r\nq_high \u0026lt;- quantile(boot_preds, 0.99)\r\nq_low \u0026lt;- quantile(boot_preds, 0.01)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Truncate the overall distribution to within the lower 1% and upper 1% quantiles\r\n# Doing this since it creates issues later on when scaling the output\r\nboot_preds[boot_preds \u0026gt; q_high] \u0026lt;- q_high\r\nboot_preds[boot_preds \u0026lt; q_low] \u0026lt;- q_low\r\n\r\nrange(boot_preds)\r\n## [1] -5.0187801 -0.2356427\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Convert to a data frame\r\nboot_preds \u0026lt;- data.frame(pred = boot_preds, \r\n                         id = rep(1:length(boot_sample$splits), each = nrow(sample)))\r\nhead(boot_preds)\r\n##        pred id\r\n## 1 -3.036820  1\r\n## 2 -1.564358  1\r\n## 3 -2.485234  1\r\n## 4 -2.195910  1\r\n## 5 -3.978170  1\r\n## 6 -2.070421  1\u003c/code\u003e\u003c/pre\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"scaling-model-predictions\" class=\"section level2\"\u003e\r\n\u003ch2\u003eScaling model predictions\u003c/h2\u003e\r\n\u003cp\u003eGiven \u003ccode\u003elog-odds\u003c/code\u003e, we can now scale the output and make it look like a credit score. Weâ€™ll use the industry standard \u003cstrong\u003epoints to double odds\u003c/strong\u003e methodology.\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003escaling_func \u0026lt;- function(vec, PDO = 30, OddsAtAnchor = 5, Anchor = 700){\r\n  beta \u0026lt;- PDO / log(2)\r\n  alpha \u0026lt;- Anchor - PDO * OddsAtAnchor\r\n  \r\n  # Simple linear scaling of the log odds\r\n  scr \u0026lt;- alpha - beta * vec  \r\n  \r\n  # Round off\r\n  return(round(scr, 0))\r\n}\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003eboot_preds$scores \u0026lt;- scaling_func(boot_preds$pred, 30, 2, 700)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Chart the distribution of predictions across all the samples\r\nggplot(boot_preds, aes(x = scores, color = factor(id))) + \r\n  geom_density() + \r\n  theme_minimal() + \r\n  theme(legend.position = \u0026quot;none\u0026quot;) + \r\n  scale_color_grey() + \r\n  labs(title = \u0026quot;Predictions from bootstrapped samples\u0026quot;, \r\n       subtitle = \u0026quot;Density function\u0026quot;, \r\n       x = \u0026quot;Predictions (Log odds)\u0026quot;, \r\n       y = \u0026quot;Density\u0026quot;)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003e\u003cimg src=\"chart2-1.png\" /\u003e\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"assessing-variability\" class=\"section level2\"\u003e\r\n\u003ch2\u003eAssessing variability\u003c/h2\u003e\r\n\u003cp\u003eNow that we have model predictions for each bootstrapped sample scaled in the form of a score, we can evaluate the variability in these predictions in a visual manner.\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Create bins using quantiles\r\nbreaks \u0026lt;- quantile(boot_preds$scores, probs = seq(0, 1, length.out = 20))\r\nboot_preds$bins \u0026lt;- cut(boot_preds$scores, breaks = unique(breaks), include.lowest = T, right = T)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e# Chart standard deviation of model predictions across each score bin\r\nboot_preds %\u0026gt;%\r\n  group_by(bins) %\u0026gt;%\r\n  summarise(std_dev = sd(scores)) %\u0026gt;%\r\n  ggplot(aes(x = bins, y = std_dev)) +\r\n  geom_col(color = \u0026quot;black\u0026quot;, fill = \u0026quot;#90AACB\u0026quot;) +\r\n  theme_minimal() + \r\n  theme(axis.text.x = element_text(angle = 90)) + \r\n  theme(legend.position = \u0026quot;none\u0026quot;) + \r\n  labs(title = \u0026quot;Variability in model predictions across samples\u0026quot;, \r\n       subtitle = \u0026quot;(measured using standard deviation)\u0026quot;, \r\n       x = \u0026quot;Score Range\u0026quot;, \r\n       y = \u0026quot;Standard Deviation\u0026quot;)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003e\u003cimg src=\"chart3-1.png\" /\u003e\u003c/p\u003e\r\n\u003cp\u003eAs expected, the modelâ€™s predictions are more reliable within a certain range of values (700-800) whereas there is significant variability in the modelâ€™s predictions in the lowest and highest score buckets.\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"parting-notes\" class=\"section level2\"\u003e\r\n\u003ch2\u003eParting notes\u003c/h2\u003e\r\n\u003cp\u003eWhile the outcome of this experiment is not unexpected, an interesting question could be - should analysts and model users define an \u003cstrong\u003eoperating range\u003c/strong\u003e for their models? In this example, we could set lower and upper limits at \u003ccode\u003e700\u003c/code\u003e and \u003ccode\u003e800\u003c/code\u003e respectively and any borrower receiving a score beyond these thresholds could be assigned a generic value of \u003ccode\u003e700-\u003c/code\u003e or \u003ccode\u003e800+\u003c/code\u003e.\u003c/p\u003e\r\n\u003cp\u003eThat said, binning features mitigates this to a certain extent since the model cannot generate predictions beyond a certain range of values.\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"an-useful-extension\" class=\"section level2\"\u003e\r\n\u003ch2\u003eAn useful extension\u003c/h2\u003e\r\n\u003cp\u003e\u003csmall\u003e\u003cem\u003eSpecial thanks to \u003ca href=\"https://disqus.com/by/richardwarnung/\"\u003eRichard Warnung\u003c/a\u003e for his comments\u003c/em\u003e\u003c/small\u003e\u003c/p\u003e\r\n\u003cp\u003eIn the above analysis, not only did we fit the model repeatedly on different datasets, but we made predictions on different datasets as well. We can remove the effects of the latter if we make predictions on the same test dataset. Hereâ€™s some code to do this.\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003e\r\n# Create overall training and testing datasets \r\nid \u0026lt;- sample(1:nrow(sample), size = nrow(sample)*0.8, replace = F)\r\n\r\ntrain_data \u0026lt;- sample[id,]\r\ntest_data \u0026lt;- sample[-id,]\r\n\r\n# Bootstrapped samples are now pulled only from the overall training dataset\r\nboot_sample \u0026lt;- bootstraps(data = train_data, times = 80)\r\n\r\n# Using the same function from before but predicting on the same test dataset\r\nglm_model \u0026lt;- function(train, test){\r\n  \r\n  mdl \u0026lt;- glm(bad_flag ~\r\n               loan_amnt + funded_amnt + annual_inc + delinq_2yrs +\r\n               inq_last_6mths + mths_since_last_delinq + fico_range_low +\r\n               mths_since_last_record + revol_util + total_pymnt,\r\n             family = \u0026quot;binomial\u0026quot;,\r\n             data = train)\r\n  \r\n  return(predict(mdl, newdata = test))\r\n}\r\n\r\n# Train and predict repeatedly\r\noutput \u0026lt;- lapply(boot_sample$splits, function(x){\r\n  train \u0026lt;- analysis(x)\r\n  pred \u0026lt;- glm_model(train, test_data)\r\n\r\n  return(pred)\r\n})\r\n\r\n# Collate data into a single data.frame\r\nboot_preds \u0026lt;- do.call(c, output)\r\nboot_preds \u0026lt;- data.frame(pred = boot_preds, \r\n                         id = rep(1:length(boot_sample$splits), each = nrow(sample)))\r\n\r\nboot_preds$scores \u0026lt;- scaling_func(boot_preds$pred, 30, 2, 700)\r\n\r\n# Forcing scores to a range\r\nboot_preds$scores \u0026lt;- sapply(boot_preds$scores, min, 900)\r\n\r\n# Bin the outputs for easier charting \r\nbreaks \u0026lt;- quantile(boot_preds$scores, probs = seq(0, 1, length.out = 20))\r\nboot_preds$bins \u0026lt;- cut(boot_preds$scores, breaks = unique(breaks), include.lowest = T, right = T)\r\n\r\n# Chart\r\nboot_preds %\u0026gt;%\r\n  group_by(bins) %\u0026gt;%\r\n  summarise(std_dev = sd(scores)) %\u0026gt;%\r\n  ggplot(aes(x = bins, y = std_dev)) +\r\n  geom_col(color = \u0026quot;black\u0026quot;, fill = \u0026quot;#90AACB\u0026quot;) +\r\n  theme_minimal() + \r\n  theme(axis.text.x = element_text(angle = 90)) + \r\n  theme(legend.position = \u0026quot;none\u0026quot;) + \r\n  labs(title = \u0026quot;Variability in model predictions across samples\u0026quot;, \r\n       subtitle = \u0026quot;Prediction set is fixed\u0026quot;, \r\n       x = \u0026quot;Score Range\u0026quot;, \r\n       y = \u0026quot;Standard Deviation\u0026quot;)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003e\u003cem\u003eThoughts? Comments? Helpful? Not helpful? Like to see anything else added in here? Let me know!\u003c/em\u003e\u003c/p\u003e\r\n\u003c/div\u003e"
}
</script>

<link rel="preload" as="script" href="/bundle.js?v=1679930341">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://stats.g.doubleclick.net">
<link rel="preconnect" href="https://www.googleadservices.com">
<link rel="preload" as="script" href="https://www.googletagmanager.com/gtag/js?id=UA-57026003-1">
<script src="https://www.googletagmanager.com/gtag/js?id=UA-57026003-1"></script>
<script>
  window.dataLayer=window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js',new Date());
  gtag('config','UA-57026003-1');
</script>

</head>
<body>

  <header id="nav" class="header">
  <div class="ax-l-i max-w-6xl">
    <div class="ax-logo">
      <a class="block" href="/" title="R&#39;tichoke"><span class="font-semibold text-raven-900">R'tichoke</span></a>
    </div>
    <div class="ax-user">
      <a class="p-2 w-8 h-8 block text-raven-500 hover:text-gray-800 focus:text-gray-800 focus:outline-none" target="_blank" rel="noopener nofollow" href="https://www.google.com/search?q=site:https://rtichoke.netlify.app/" title="Search">
        <svg class="fill-current" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M2.67 12.804c0-5.6 4.544-10.134 10.133-10.134s10.134 4.544 10.134 10.134-4.544 10.133-10.134 10.133S2.67 18.393 2.67 12.804zm28.943 16.923l-8.868-8.868c4.287-5.3 3.68-13.012-1.378-17.57S8.564-1.066 3.75 3.75s-5.017 12.558-.46 17.618 12.28 5.665 17.57 1.378l8.868 8.868a1.33 1.33 0 0 0 2.231-.597c.123-.46-.008-.952-.345-1.29h0z"/></svg>

      </a>
      <a class="p-2 block text-base leading-none text-raven-500 hover:text-gray-800 focus:text-gray-800 focus:outline-none" href="/post/">
        Posts
      </a>
      <a class="p-2 block text-base leading-none text-raven-500 hover:text-gray-800 focus:text-gray-800 focus:outline-none" href="/about/">
        About
      </a>
      <a class="p-2 block text-base leading-none text-raven-500 hover:text-gray-800 focus:text-gray-800 focus:outline-none" href="/rbloggers/">
        R-Bloggers
      </a>
    </div>
  </div>

  
  
</header>


  <main>
<div class="default-single">
  <div class="ax-title ax-l-o">
    <div class="ax-l-i max-w-4xl">
      <h1 class="post-title font-content-title font-normal leading-tight tracking-default text-40">Using bootstrapped sampling to assess variability in score predictions </h1>
      <p class="post-subtitle font-content-sans font-light text-xl text-raven-500 mt-3">Credit risk series (Post #3)</p>

      <div class="ax-meta flex items-center mt-5">
        <div class="flex-grow min-w-0">
          <div class="flex items-center">
  
  <div class="flex-shrink-0">
    <img
    class="w-12 h-12 sm:w-14 sm:h-14 object-cover p-3px rounded-full border border-blue-300"
    src="data:image/svg&#43;xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI&#43;PHBhdGggZmlsbD0iI2VjZWZmMSIgZD0iTTAgMGgzMnYzMkgweiIvPjxwYXRoIGZpbGw9IiM0NTVhNjQiIGQ9Ik0yOS4zMDUgMjQuNDA3Yy0uNzk1LS42My0xLjc2NS0xLjA4LTIuODE2LTEuM0wyMS40NzYgMjIuMWExLjEzIDEuMTMgMCAwIDEtLjkwNS0xLjEydi0xLjE1Yy4zMjItLjQ1My42MjYtMS4wNTQuOTQ0LTEuNjgyLjI0Ny0uNDg3LjYyLTEuMjIuODA1LTEuNCAxLjAxNS0xLjAyIDEuOTk1LTIuMTY1IDIuMy0zLjY0LjI4My0xLjM4NS4wMDUtMi4xMTItLjMyMi0yLjY5NyAwLTEuNDYtLjA0Ni0zLjI5LS4zOS00LjYyLS4wNC0xLjgtLjM2OC0yLjgxNC0xLjE5LTMuNy0uNTgtLjYzLTEuNDM1LS43NzUtMi4xMjMtLjg5LS4yNy0uMDQ2LS42NDItLjEtLjc4LS4xODNDMTguNTk0LjM0NyAxNy40LjAyNSAxNS45NTIgMGMtMyAuMTIzLTYuNzEgMi4wNC03Ljk1IDUuNDU0LS4zODQgMS4wNC0uMzQ1IDIuNzQ3LS4zMTMgNC4xMmwtLjAzLjgyNWMtLjI5NS41NzYtLjU4NSAxLjMwNy0uMyAyLjY5Ny4zMDIgMS40OCAxLjI4MiAyLjYyNiAyLjMxNSAzLjY2LjE3LjE3NC41NS45MTQuODAyIDEuNDAzbC45NSAxLjY3NXYxLjE1YzAgLjU0Ni0uMzgyIDEuMDE3LS45IDEuMTJMNS41IDIzLjExYy0xLjA0NS4yMjItMi4wMTQuNjctMi44MDcgMS4yOThhMS4xNSAxLjE1IDAgMCAwLS40MjcuODA1IDEuMTQgMS4xNCAwIDAgMCAuMjkzLjg1OUM1Ljk3NSAyOS44MzggMTAuODczIDMyIDE2IDMyczEwLjAyNy0yLjE2IDEzLjQ0LTUuOTNhMS4xNCAxLjE0IDAgMCAwLS4xMzUtMS42NjR6Ii8&#43;PC9zdmc&#43;DQo="
    alt="">
	</div>
    
  <div class="flex-shrink-0 ml-2 leading-tight font-content-sans">
    <a class="block text-sm text-raven-800 hover:text-raven-900 hover:underline focus:underline" target="_blank" rel="noopener nofollow" title="Riddhiman" href="https://linkedin.com/in/riddhimanr">Riddhiman</a>
    <time class="text-sm text-raven-500" datetime="2021-09-30T00:00:00Z">Sep 30, 2021</time>
  </div>
</div>

        </div>
        <div>
          

<div class="flex items-center">
    <a class="flex-shrink-0 block text-raven-800 hover:text-raven-900" target="_blank" rel="noopener nofollow" title="Share on Twitter" href="https://twitter.com/intent/tweet?text=Using%20bootstrapped%20sampling%20to%20assess%20variability%20in%20score%20predictions%20%20by%20%40%25%21s%28%3cnil%3e%29%20https%3a%2f%2frtichoke.netlify.app%2fpost%2fassessing_score_reliability%2f"><svg class="w-6 h-6 fill-current" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M32 6.078c-1.2.522-2.458.868-3.78 1.036 1.36-.812 2.398-2.088 2.886-3.626a13.11 13.11 0 0 1-4.16 1.588C25.742 3.794 24.026 3 22.154 3a6.56 6.56 0 0 0-6.556 6.562c0 .52.044 1.02.152 1.496-5.454-.266-10.28-2.88-13.522-6.862-.566.982-.898 2.106-.898 3.316a6.57 6.57 0 0 0 2.914 5.452 6.48 6.48 0 0 1-2.964-.808v.072c0 3.188 2.274 5.836 5.256 6.446-.534.146-1.116.216-1.72.216-.42 0-.844-.024-1.242-.112.85 2.598 3.262 4.508 6.13 4.57a13.18 13.18 0 0 1-8.134 2.798c-.538 0-1.054-.024-1.57-.1C2.906 27.93 6.35 29 10.064 29c12.072 0 18.672-10 18.672-18.668 0-.3-.01-.57-.024-.848C30.014 8.56 31.108 7.406 32 6.078z"/></svg>
</a>
    <a class="ml-3 flex-shrink-0 block text-raven-800 hover:text-raven-900" target="_blank" rel="noopener nofollow" title="Share on Facebook" href="https://www.facebook.com/dialog/share?app_id=&display=page&href=https%3a%2f%2frtichoke.netlify.app%2fpost%2fassessing_score_reliability%2f"><svg class="w-6 h-6 fill-current" viewBox="-7 -3.5 39 39" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M30.234 0H1.765C.8.001 0 .79 0 1.766v28.47C.001 31.2.79 32 1.766 32h15.328V19.625h-4.156V14.78h4.156v-3.564c0-4.134 2.523-6.384 6.21-6.384 1.766 0 3.284.13 3.726.2v4.32h-2.543c-2.006 0-2.394.953-2.394 2.352v3.085h4.797l-.625 4.844h-4.172V32h8.14C31.21 32 32 31.2 32 30.234V1.765C32 .8 31.21 0 30.234 0z"/></svg>
</a>
  
</div>
        </div>
      </div>
    </div>
  </div><div class="ax-content ax-l-o">
    <div class="ax-l-i max-w-4xl">
      <article class="cdata">



<p>When building risk scorecards, apart from the variety of performance metrics, analysts also assess something known as <code>risk-ranking</code> i.e.Â whether or not the observed event rates increase (or decrease) monotonically with increasing (or decreasing) scores. Sometimes, models are not able to risk-rank borrowers in the tails (regions of very high or very low scores). While this is expected, it would be nice if we could quantify this effect. One way to do this would be to use bootstrapped samples to assess variability in model predictions.</p>
<div id="basic-idea" class="section level2">
<h2>Basic idea</h2>
<p>The underlying idea is very simple - less available data for estimation equates to lower quality of estimation. As a simple example, we can observe this effect when trying to estimate quantiles of a probability distribution.</p>
<pre class="r"><code># Number of samples to be drawn from a probability distribution
n_samples &lt;- 1000

# Number of times, sampling should be repeated
repeats &lt;- 100

# Mean and std-dev for a standard normal distribution
mu &lt;- 5
std_dev &lt;- 2

# Sample
samples &lt;- rnorm(n_samples * repeats, mean = 10)

# Fit into a matrix like object with `n_samples&#39; number of rows 
# and `repeats` number of columns
samples &lt;- matrix(samples, nrow = n_samples, ncol = repeats)</code></pre>
<pre class="r"><code># Compute mean across each column
sample_means &lt;- apply(samples, 1, mean)

# Similarly, compute 75% and 95% quantile across each column
sample_75_quantile &lt;- apply(samples, 1, quantile, p = 0.75)
sample_95_quantile &lt;- apply(samples, 1, quantile, p = 0.95)
sample_99_quantile &lt;- apply(samples, 1, quantile, p = 0.99)</code></pre>
<pre class="r"><code>sd(sample_means)/mean(sample_means)
## [1] 0.009803362
sd(sample_75_quantile)/mean(sample_75_quantile)
## [1] 0.01259488
sd(sample_95_quantile)/mean(sample_75_quantile)
## [1] 0.01929882</code></pre>
<pre class="r"><code>combined_vec &lt;- c(sample_means, sample_75_quantile, sample_95_quantile, sample_99_quantile)

plot(density(sample_means), 
     col = &quot;#6F69AC&quot;, 
     lwd = 3, 
     main = &quot;Estimating the mean vs tail quantiles&quot;, 
     xlab = &quot;&quot;, 
     xlim = c(min(combined_vec), max(combined_vec)))

lines(density(sample_75_quantile), col = &quot;#95DAC1&quot;, lwd = 3)
lines(density(sample_95_quantile), col = &quot;#FFEBA1&quot;, lwd = 3)
lines(density(sample_99_quantile), col = &quot;#FD6F96&quot;, lwd = 3)
grid()

legend(&quot;topright&quot;, 
       fill = c(&quot;#6F69AC&quot;, &quot;#95DAC1&quot;, &quot;#FFEBA1&quot;, &quot;#FD6F96&quot;), 
       legend = c(&quot;Mean&quot;, &quot;75% Quantile&quot;, &quot;95% Quantile&quot;, &quot;99% Quantile&quot;), 
       cex = 0.7)</code></pre>
<p><img src="chart1-1.png" /></p>
<p>It is easy to notice that the uncertainty in estimating the sample 99% quantile is much higher than the uncertainty in estimating the sample mean. We will now try to extend this idea to a scorecard model.</p>
</div>
<div id="libraries" class="section level2">
<h2>Libraries</h2>
<pre class="r"><code>#install.packages(&quot;pacman&quot;)
pacman::p_load(dplyr, magrittr, rsample, ggplot2)
## Installing package into &#39;C:/Users/riddh/AppData/Local/R/win-library/4.2&#39;
## (as &#39;lib&#39; is unspecified)
## also installing the dependencies &#39;listenv&#39;, &#39;parallelly&#39;, &#39;future&#39;, &#39;globals&#39;, &#39;warp&#39;, &#39;cpp11&#39;, &#39;furrr&#39;, &#39;purrr&#39;, &#39;slider&#39;, &#39;tidyr&#39;
## Warning: unable to access index for repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2:
##   cannot open URL &#39;http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2/PACKAGES&#39;
## package &#39;listenv&#39; successfully unpacked and MD5 sums checked
## package &#39;parallelly&#39; successfully unpacked and MD5 sums checked
## package &#39;future&#39; successfully unpacked and MD5 sums checked
## package &#39;globals&#39; successfully unpacked and MD5 sums checked
## package &#39;warp&#39; successfully unpacked and MD5 sums checked
## package &#39;cpp11&#39; successfully unpacked and MD5 sums checked
## package &#39;furrr&#39; successfully unpacked and MD5 sums checked
## package &#39;purrr&#39; successfully unpacked and MD5 sums checked
## package &#39;slider&#39; successfully unpacked and MD5 sums checked
## package &#39;tidyr&#39; successfully unpacked and MD5 sums checked
## package &#39;rsample&#39; successfully unpacked and MD5 sums checked
## 
## The downloaded binary packages are in
## 	C:\Users\riddh\AppData\Local\Temp\Rtmp0qD5ZX\downloaded_packages
## 
## rsample installed
## Installing package into &#39;C:/Users/riddh/AppData/Local/R/win-library/4.2&#39;
## (as &#39;lib&#39; is unspecified)
## also installing the dependencies &#39;gtable&#39;, &#39;isoband&#39;
## Warning: unable to access index for repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2:
##   cannot open URL &#39;http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2/PACKAGES&#39;
## package &#39;gtable&#39; successfully unpacked and MD5 sums checked
## package &#39;isoband&#39; successfully unpacked and MD5 sums checked
## package &#39;ggplot2&#39; successfully unpacked and MD5 sums checked
## 
## The downloaded binary packages are in
## 	C:\Users\riddh\AppData\Local\Temp\Rtmp0qD5ZX\downloaded_packages
## 
## ggplot2 installed</code></pre>
</div>
<div id="sample-data" class="section level2">
<h2>Sample data</h2>
<p>As in previous posts, weâ€™ll use a small sample <a href="https://github.com/royr2/blog/blob/main/download/credit_sample.csv">(download here)</a> of the <strong>Lending Club</strong> dataset available on <a href="https://www.kaggle.com/wordsforthewise/lending-club">Kaggle</a>.</p>
<pre class="r"><code>sample &lt;- read.csv(&quot;credit_sample.csv&quot;)</code></pre>
</div>
<div id="creating-a-target" class="section level2">
<h2>Creating a target</h2>
<p>The next step is to create a target (dependent variable) to model for.</p>
<pre class="r"><code># Mark which loan status will be tagged as default
codes &lt;- c(&quot;Charged Off&quot;, &quot;Does not meet the credit policy. Status:Charged Off&quot;)

# Apply above codes and create target
sample %&lt;&gt;% mutate(bad_flag = ifelse(loan_status %in% codes, 1, 0))

# Replace missing values with a default value
sample[is.na(sample)] &lt;- -1</code></pre>
<pre class="r"><code># Get summary tally
table(sample$bad_flag)
## 
##    0    1 
## 8838 1162</code></pre>
</div>
<div id="sampling" class="section level2">
<h2>Sampling</h2>
<p>Weâ€™ll use <code>bootstrapped sampling</code> to create multiple training sets. We will then repeatedly train a model on each training set and assess the variability in volatile model predictions across score ranges. Weâ€™ll use the <code>bootstraps()</code> function in the <code>rsample</code> package.</p>
<pre class="r"><code># Create 100 samples
boot_sample &lt;- bootstraps(data = sample, times = 100)</code></pre>
<pre class="r"><code>head(boot_sample, 3)
## # A tibble: 3 Ã— 2
##   splits               id          
##   &lt;list&gt;               &lt;chr&gt;       
## 1 &lt;split [10000/3685]&gt; Bootstrap001
## 2 &lt;split [10000/3676]&gt; Bootstrap002
## 3 &lt;split [10000/3682]&gt; Bootstrap003</code></pre>
<pre class="r"><code>boot_sample$splits[[1]]
## &lt;Analysis/Assess/Total&gt;
## &lt;10000/3685/10000&gt;</code></pre>
<p>Each row represents a separate bootstrapped sample whereas within each sample, there are two sub-samples namely an <code>analysis set</code> and an <code>assessment set</code>. To retrieve a bootstrapped sample as a <code>data.frame</code>, the package provides two helper functions - <code>analysis()</code> and <code>assessment()</code></p>
<pre class="r"><code># Show the first 5 rows and 5 columns of the first sample
analysis(boot_sample$splits[[1]]) %&gt;% .[1:5, 1:5]
##         V1        id member_id loan_amnt funded_amnt
## 7967 46001  63407248        -1     16000       16000
## 5186 86313 125551278        -1     10000       10000
## 9323 95781   1414849        -1     12000       12000
## 4080 56615  44065228        -1      4000        4000
## 2695 80522 125327254        -1     28000       28000</code></pre>
<p>The <a href="https://rsample.tidymodels.org/articles/rsample.html">getting started</a> page of the <code>rsample</code> package has additional information.</p>
</div>
<div id="creating-a-modeling-function" class="section level2">
<h2>Creating a modeling function</h2>
<p>Weâ€™ll use a simple <code>glm()</code> model for illustrative purposes. First, weâ€™ll need to create a function that fits such a model to a given dataset</p>
<pre class="r"><code>glm_model &lt;- function(df){
  
  # Fit a simple model with a set specification
  mdl &lt;- glm(bad_flag ~
               loan_amnt + funded_amnt + annual_inc + delinq_2yrs +
               inq_last_6mths + mths_since_last_delinq + fico_range_low +
               mths_since_last_record + revol_util + total_pymnt,
             family = &quot;binomial&quot;,
             data = df)
  
  # Return fitted values
  return(predict(mdl))
}</code></pre>
<pre class="r"><code># Test the function

# Retrieve a data frame
train &lt;- analysis(boot_sample$splits[[1]])

# Predict
pred &lt;- glm_model(train)

# Check output
range(pred)  # Output is on log odds scale
## [1] -8.612908  1.279721</code></pre>
</div>
<div id="fitting-the-model-repeatedly" class="section level2">
<h2>Fitting the model repeatedly</h2>
<p>Now we need to fit the model repeatedly on each of the bootstrapped samples and store the fitted values. And since we are using <code>R</code>, for-loops are not allowed ðŸ˜†</p>
<pre class="r"><code># First apply the glm fitting function to each of the sample
# Note the use of lapply
output &lt;- lapply(boot_sample$splits, function(x){
  train &lt;- analysis(x)
  pred &lt;- glm_model(train)

  return(pred)
})

# Collate all predictions into a vector 
boot_preds &lt;- do.call(c, output)
range(boot_preds)
## [1] -131.359579    5.070397</code></pre>
<pre class="r"><code># Get outliers
q_high &lt;- quantile(boot_preds, 0.99)
q_low &lt;- quantile(boot_preds, 0.01)</code></pre>
<pre class="r"><code># Truncate the overall distribution to within the lower 1% and upper 1% quantiles
# Doing this since it creates issues later on when scaling the output
boot_preds[boot_preds &gt; q_high] &lt;- q_high
boot_preds[boot_preds &lt; q_low] &lt;- q_low

range(boot_preds)
## [1] -5.0187801 -0.2356427</code></pre>
<pre class="r"><code># Convert to a data frame
boot_preds &lt;- data.frame(pred = boot_preds, 
                         id = rep(1:length(boot_sample$splits), each = nrow(sample)))
head(boot_preds)
##        pred id
## 1 -3.036820  1
## 2 -1.564358  1
## 3 -2.485234  1
## 4 -2.195910  1
## 5 -3.978170  1
## 6 -2.070421  1</code></pre>
</div>
<div id="scaling-model-predictions" class="section level2">
<h2>Scaling model predictions</h2>
<p>Given <code>log-odds</code>, we can now scale the output and make it look like a credit score. Weâ€™ll use the industry standard <strong>points to double odds</strong> methodology.</p>
<pre class="r"><code>scaling_func &lt;- function(vec, PDO = 30, OddsAtAnchor = 5, Anchor = 700){
  beta &lt;- PDO / log(2)
  alpha &lt;- Anchor - PDO * OddsAtAnchor
  
  # Simple linear scaling of the log odds
  scr &lt;- alpha - beta * vec  
  
  # Round off
  return(round(scr, 0))
}</code></pre>
<pre class="r"><code>boot_preds$scores &lt;- scaling_func(boot_preds$pred, 30, 2, 700)</code></pre>
<pre class="r"><code># Chart the distribution of predictions across all the samples
ggplot(boot_preds, aes(x = scores, color = factor(id))) + 
  geom_density() + 
  theme_minimal() + 
  theme(legend.position = &quot;none&quot;) + 
  scale_color_grey() + 
  labs(title = &quot;Predictions from bootstrapped samples&quot;, 
       subtitle = &quot;Density function&quot;, 
       x = &quot;Predictions (Log odds)&quot;, 
       y = &quot;Density&quot;)</code></pre>
<p><img src="chart2-1.png" /></p>
</div>
<div id="assessing-variability" class="section level2">
<h2>Assessing variability</h2>
<p>Now that we have model predictions for each bootstrapped sample scaled in the form of a score, we can evaluate the variability in these predictions in a visual manner.</p>
<pre class="r"><code># Create bins using quantiles
breaks &lt;- quantile(boot_preds$scores, probs = seq(0, 1, length.out = 20))
boot_preds$bins &lt;- cut(boot_preds$scores, breaks = unique(breaks), include.lowest = T, right = T)</code></pre>
<pre class="r"><code># Chart standard deviation of model predictions across each score bin
boot_preds %&gt;%
  group_by(bins) %&gt;%
  summarise(std_dev = sd(scores)) %&gt;%
  ggplot(aes(x = bins, y = std_dev)) +
  geom_col(color = &quot;black&quot;, fill = &quot;#90AACB&quot;) +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 90)) + 
  theme(legend.position = &quot;none&quot;) + 
  labs(title = &quot;Variability in model predictions across samples&quot;, 
       subtitle = &quot;(measured using standard deviation)&quot;, 
       x = &quot;Score Range&quot;, 
       y = &quot;Standard Deviation&quot;)</code></pre>
<p><img src="chart3-1.png" /></p>
<p>As expected, the modelâ€™s predictions are more reliable within a certain range of values (700-800) whereas there is significant variability in the modelâ€™s predictions in the lowest and highest score buckets.</p>
</div>
<div id="parting-notes" class="section level2">
<h2>Parting notes</h2>
<p>While the outcome of this experiment is not unexpected, an interesting question could be - should analysts and model users define an <strong>operating range</strong> for their models? In this example, we could set lower and upper limits at <code>700</code> and <code>800</code> respectively and any borrower receiving a score beyond these thresholds could be assigned a generic value of <code>700-</code> or <code>800+</code>.</p>
<p>That said, binning features mitigates this to a certain extent since the model cannot generate predictions beyond a certain range of values.</p>
</div>
<div id="an-useful-extension" class="section level2">
<h2>An useful extension</h2>
<p><small><em>Special thanks to <a href="https://disqus.com/by/richardwarnung/">Richard Warnung</a> for his comments</em></small></p>
<p>In the above analysis, not only did we fit the model repeatedly on different datasets, but we made predictions on different datasets as well. We can remove the effects of the latter if we make predictions on the same test dataset. Hereâ€™s some code to do this.</p>
<pre class="r"><code>
# Create overall training and testing datasets 
id &lt;- sample(1:nrow(sample), size = nrow(sample)*0.8, replace = F)

train_data &lt;- sample[id,]
test_data &lt;- sample[-id,]

# Bootstrapped samples are now pulled only from the overall training dataset
boot_sample &lt;- bootstraps(data = train_data, times = 80)

# Using the same function from before but predicting on the same test dataset
glm_model &lt;- function(train, test){
  
  mdl &lt;- glm(bad_flag ~
               loan_amnt + funded_amnt + annual_inc + delinq_2yrs +
               inq_last_6mths + mths_since_last_delinq + fico_range_low +
               mths_since_last_record + revol_util + total_pymnt,
             family = &quot;binomial&quot;,
             data = train)
  
  return(predict(mdl, newdata = test))
}

# Train and predict repeatedly
output &lt;- lapply(boot_sample$splits, function(x){
  train &lt;- analysis(x)
  pred &lt;- glm_model(train, test_data)

  return(pred)
})

# Collate data into a single data.frame
boot_preds &lt;- do.call(c, output)
boot_preds &lt;- data.frame(pred = boot_preds, 
                         id = rep(1:length(boot_sample$splits), each = nrow(sample)))

boot_preds$scores &lt;- scaling_func(boot_preds$pred, 30, 2, 700)

# Forcing scores to a range
boot_preds$scores &lt;- sapply(boot_preds$scores, min, 900)

# Bin the outputs for easier charting 
breaks &lt;- quantile(boot_preds$scores, probs = seq(0, 1, length.out = 20))
boot_preds$bins &lt;- cut(boot_preds$scores, breaks = unique(breaks), include.lowest = T, right = T)

# Chart
boot_preds %&gt;%
  group_by(bins) %&gt;%
  summarise(std_dev = sd(scores)) %&gt;%
  ggplot(aes(x = bins, y = std_dev)) +
  geom_col(color = &quot;black&quot;, fill = &quot;#90AACB&quot;) +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 90)) + 
  theme(legend.position = &quot;none&quot;) + 
  labs(title = &quot;Variability in model predictions across samples&quot;, 
       subtitle = &quot;Prediction set is fixed&quot;, 
       x = &quot;Score Range&quot;, 
       y = &quot;Standard Deviation&quot;)</code></pre>
<p><em>Thoughts? Comments? Helpful? Not helpful? Like to see anything else added in here? Let me know!</em></p>
</div>

      </article>
      

      
<div id="disqus_thread"></div>
<script>
var disqus_config = function () {
  this.page.url = "https://rtichoke.netlify.app/post/assessing_score_reliability/";
  this.page.identifier = "02a6ef999d7e5c3d6a253a112e108440";
  this.page.title = "Using bootstrapped sampling to assess variability in score predictions ";
};

(function() {
  window.setTimeout(function() {
    var d = document;
    var s = d.createElement('script');
    s.src = 'https://rtichoke.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    s.setAttribute("defer", "");
    d.body.appendChild(s);
  }, 2500);
})();
</script>

    </div>
  </div>
</div>

  </main>
  <footer class="footer">
  <div class="ax-l-i max-w-6xl">
    <nav class="flex items-center justify-center">
    </nav>
    <div class="footer-social flex items-center justify-center mt-4">
      <a class="block mx-3 w-6 h-6 text-raven-700 hover:text-raven-900" target="_blank" rel="noopener nofollow" title="Twitter" href="https://twitter.com/r0yr2"><svg class="fill-current" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M32 6.078c-1.2.522-2.458.868-3.78 1.036 1.36-.812 2.398-2.088 2.886-3.626a13.11 13.11 0 0 1-4.16 1.588C25.742 3.794 24.026 3 22.154 3a6.56 6.56 0 0 0-6.556 6.562c0 .52.044 1.02.152 1.496-5.454-.266-10.28-2.88-13.522-6.862-.566.982-.898 2.106-.898 3.316a6.57 6.57 0 0 0 2.914 5.452 6.48 6.48 0 0 1-2.964-.808v.072c0 3.188 2.274 5.836 5.256 6.446-.534.146-1.116.216-1.72.216-.42 0-.844-.024-1.242-.112.85 2.598 3.262 4.508 6.13 4.57a13.18 13.18 0 0 1-8.134 2.798c-.538 0-1.054-.024-1.57-.1C2.906 27.93 6.35 29 10.064 29c12.072 0 18.672-10 18.672-18.668 0-.3-.01-.57-.024-.848C30.014 8.56 31.108 7.406 32 6.078z"/></svg></a>
      <a class="block mx-3 w-6 h-6 text-raven-700 hover:text-raven-900" target="_blank" rel="noopener nofollow" title="GitHub" href="https://github.com/royr2"><svg class="fill-current" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M15.998 0C7.164 0 0 7.35 0 16.417 0 23.67 4.584 29.82 10.944 31.994c.8.15 1.092-.356 1.092-.79l-.022-2.792c-4.45.99-5.4-2.202-5.4-2.202-.726-1.896-1.776-2.4-1.776-2.4-1.454-1.018.108-.998.108-.998 1.606.117 2.45 1.693 2.45 1.693 1.428 2.507 3.746 1.784 4.658 1.363.144-1.06.558-1.784 1.016-2.195-3.552-.415-7.288-1.823-7.288-8.113 0-1.792.624-3.258 1.648-4.406-.166-.415-.714-2.085.156-4.344 0 0 1.344-.44 4.4 1.683 1.276-.364 2.644-.546 4.006-.552a14.98 14.98 0 0 1 4.006.554C23.062 6.37 24.404 6.8 24.404 6.8c.872 2.26.324 3.93.16 4.344 1.026 1.148 1.644 2.614 1.644 4.406 0 6.306-3.74 7.694-7.304 8.1.574.507 1.086 1.51 1.086 3.04l-.02 4.503c0 .44.288.95 1.1.788C27.42 29.817 32 23.667 32 16.417 32 7.35 24.836 0 15.998 0z"/></svg></a>
      <a class="block mx-3 w-6 h-6 text-raven-700 hover:text-raven-900" target="_blank" rel="noopener nofollow" title="LinkedIn" href="https://www.linkedin.com/in/riddhimanr"><svg class="fill-current" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M29.692 0H2.308A2.31 2.31 0 0 0 0 2.308v27.384A2.31 2.31 0 0 0 2.308 32h27.384A2.31 2.31 0 0 0 32 29.692V2.308A2.31 2.31 0 0 0 29.692 0zM11.35 24.188H7.454V12.464h3.897v11.723zM9.402 10.863h-.025c-1.308 0-2.153-.9-2.153-2.025 0-1.15.872-2.026 2.205-2.026s2.153.875 2.18 2.026c0 1.125-.846 2.025-2.205 2.025zm16 13.324h-3.896v-6.272c0-1.576-.564-2.65-1.974-2.65-1.076 0-1.717.725-2 1.425-.103.25-.128.6-.128.95v6.547h-3.896V12.464h3.896v1.66c.518-.8 1.444-1.935 3.512-1.935 2.564 0 4.486 1.676 4.486 5.276v6.722z"/></svg></a>
    </div>

    <div class="footer-copyright text-sm text-center text-gray-400 mt-4">
      &#169; 2023-2023 R&#39;tichoke
    </div>
    <div class="text-sm sm:text-xs text-center text-gray-400 mt-2">
      Powered by <a href="https://www.axiomtheme.com/?utm_source=theme-footer&utm_medium=website&utm_campaign=referral">Axiom</a>
    </div>
  </div>
</footer>

<script src="/bundle.js?v=1679930341"></script>


</body>
</html>
