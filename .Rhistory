xgb_cal_fixed <- xgb_val_results_calibrated %>%
cal_plot_breaks(truth = default, estimate = .pred_yes, event_level = "second") +
labs(title = "XGBoost Probability Calibration (After Platt Scaling)")
summary(xgb_val_results_calibrated$.pred_yes_calibrated)
summary(xgb_val_results_calibrated$.pred_yes)
# Create calibration plot for the calibrated model
xgb_cal_fixed <- xgb_val_results_calibrated %>%
cal_plot_breaks(truth = default, estimate = .pred_yes, event_level = "second", num_breaks = 8) +
labs(title = "XGBoost Probability Calibration (After Platt Scaling)")
# Create calibration plot for the calibrated model
xgb_cal_fixed <- xgb_val_results_calibrated %>%
cal_plot_breaks(truth = default, estimate = .pred_yes, event_level = "second", num_breaks = 9) +
labs(title = "XGBoost Probability Calibration (After Platt Scaling)")
# Create calibration plot for the calibrated model
xgb_cal_fixed <- xgb_val_results_calibrated %>%
cal_plot_breaks(truth = default, estimate = .pred_yes_calibrated, event_level = "second", num_breaks = 9) +
labs(title = "XGBoost Probability Calibration (After Platt Scaling)")
# Create calibration plot for the calibrated model
xgb_cal_fixed <- xgb_val_results_calibrated %>%
cal_plot_breaks(truth = default, estimate = .pred_yes_calibrated, event_level = "second", num_breaks = 5) +
labs(title = "XGBoost Probability Calibration (After Platt Scaling)")
# Create calibration plot for the calibrated model
xgb_cal_fixed <- xgb_val_results_calibrated %>%
cal_plot_breaks(truth = default, estimate = .pred_yes_calibrated, event_level = "second", num_breaks = 2) +
labs(title = "XGBoost Probability Calibration (After Platt Scaling)")
#| label: calibration-fix
# Create a calibration model using logistic regression (Platt scaling)
# First, prepare the validation predictions for calibration training
cal_data <- xgb_val_results %>%
select(.pred_yes, default) %>%
mutate(default_numeric = as.numeric(default == "yes"))
cal_data
cal_data %>% head()
# Method 1: Manual Platt scaling (more transparent)
# Fit a logistic regression model to calibrate probabilities
calibration_model <- glm(default_numeric ~ .pred_yes,
data = cal_data,
family = binomial())
# Function to apply calibration
apply_calibration <- function(predictions, cal_model) {
calibrated_probs <- predict(cal_model,
newdata = data.frame(.pred_yes = predictions),
type = "response")
return(calibrated_probs)
}
# Apply calibration to validation predictions
xgb_val_results_calibrated <- xgb_val_results %>%
mutate(.pred_yes_calibrated = apply_calibration(.pred_yes, calibration_model),
.pred_no_calibrated = 1 - .pred_yes_calibrated)
# Check for any issues with the calibrated data
cat("Checking calibrated data quality:\n")
cat("Number of rows:", nrow(xgb_val_results_calibrated), "\n")
cat("Any NA values in calibrated predictions:", any(is.na(xgb_val_results_calibrated$.pred_yes_calibrated)), "\n")
cat("Range of calibrated predictions:", range(xgb_val_results_calibrated$.pred_yes_calibrated, na.rm = TRUE), "\n")
cat("Structure of default variable:", class(xgb_val_results_calibrated$default), "\n")
cat("Levels of default variable:", levels(xgb_val_results_calibrated$default), "\n")
# Method 2: Using probably package (more robust)
# Create calibration using the probably package
cal_object <- cal_estimate_logistic(xgb_val_results,
truth = default,
estimate = .pred_yes,
event_level = "second")
xgb_val_results_calibrated$.pred_yes_calibrated %>% hist()
xgb_val_results_calibrated$.pred_yes %>% hist()
xgb_val_results_calibrated$.pred_yes_calibrated %>% hist()
#| label: calibration-fix
# Create a calibration model using logistic regression (Platt scaling)
# First, prepare the validation predictions for calibration training
cal_data <- xgb_val_results %>%
select(.pred_yes, default) %>%
mutate(default_numeric = as.numeric(default == "yes"))
# Method 1: Manual Platt scaling (more transparent)
# Fit a logistic regression model to calibrate probabilities
calibration_model <- glm(default_numeric ~ .pred_yes,
data = cal_data,
family = binomial())
# Function to apply calibration
apply_calibration <- function(predictions, cal_model) {
calibrated_probs <- predict(cal_model,
newdata = data.frame(.pred_yes = predictions),
type = "response")
return(calibrated_probs)
}
# Apply calibration to validation predictions
xgb_val_results_calibrated <- xgb_val_results %>%
mutate(.pred_yes_calibrated = apply_calibration(.pred_yes, calibration_model),
.pred_no_calibrated = 1 - .pred_yes_calibrated)
# Method 2: Using probably package (more robust)
# Create calibration using the probably package
cal_object <- cal_estimate_logistic(xgb_val_results,
truth = default,
estimate = .pred_yes,
event_level = "second")
xgb_val_results
# Method 2: Using probably package (more robust)
# Create calibration using the probably package
cal_object <- cal_estimate_logistic(xgb_val_results,
truth = default,
estimate = .pred_yes)
?cal_estimate_logistic
# Method 2: Using probably package (more robust)
# Create calibration using the probably package
cal_object <- cal_estimate_logistic(xgb_val_results,
truth = default,
estimate = starts_with(".pred_"))
# Apply calibration using probably
xgb_val_results_prob_cal <- xgb_val_results %>%
cal_apply(cal_object, .pred_yes)
# Apply calibration using probably
xgb_val_results_prob_cal <- xgb_val_results %>%
cal_apply(cal_object)
# Method 2: Using probably package (more robust)
# Create calibration using the probably package
cal_object <- cal_estimate_logistic(xgb_val_results,
truth = default,
estimate = starts_with(".pred_"))
# Apply calibration using probably
xgb_val_results_prob_cal <- xgb_val_results %>%
cal_apply(cal_object)
# Create calibration plots comparing both methods
cat("Creating calibration plots...\n")
# Original calibration plot (should work)
tryCatch({
original_cal <- xgb_val_results %>%
cal_plot_breaks(truth = default, estimate = .pred_yes, event_level = "second") +
labs(title = "Original XGBoost Calibration")
print(original_cal)
}, error = function(e) {
cat("Error in original calibration plot:", e$message, "\n")
})
# Manual calibration plot
tryCatch({
manual_cal <- xgb_val_results_calibrated %>%
cal_plot_breaks(truth = default, estimate = .pred_yes_calibrated, event_level = "second") +
labs(title = "Manual Platt Scaling Calibration")
print(manual_cal)
}, error = function(e) {
cat("Error in manual calibration plot:", e$message, "\n")
# Fallback to manual plotting
cal_data_plot <- xgb_val_results_calibrated %>%
mutate(
default_numeric = as.numeric(default == "yes"),
prob_bin = cut(.pred_yes_calibrated,
breaks = 10,
include.lowest = TRUE,
labels = FALSE)
) %>%
group_by(prob_bin) %>%
summarise(
mean_predicted = mean(.pred_yes_calibrated),
mean_observed = mean(default_numeric),
count = n(),
.groups = "drop"
) %>%
filter(count > 0)
manual_plot <- ggplot(cal_data_plot, aes(x = mean_predicted, y = mean_observed)) +
geom_point(aes(size = count), alpha = 0.7) +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(
title = "Manual Platt Scaling Calibration",
x = "Mean Predicted Probability",
y = "Mean Observed Rate",
size = "Count"
) +
theme_minimal() +
coord_equal(xlim = c(0, 1), ylim = c(0, 1))
print(manual_plot)
})
# probably package calibration plot
tryCatch({
prob_cal <- xgb_val_results_prob_cal %>%
cal_plot_breaks(truth = default, estimate = .pred_yes, event_level = "second") +
labs(title = "Probably Package Calibration")
print(prob_cal)
}, error = function(e) {
cat("Error in probably calibration plot:", e$message, "\n")
})
# Compare calibration metrics before and after
cat("Calibration comparison:\n")
# Calculate Brier score (lower is better) for original predictions
brier_original <- xgb_val_results %>%
mutate(default_numeric = as.numeric(default == "yes")) %>%
summarise(brier_score = mean((default_numeric - .pred_yes)^2)) %>%
pull(brier_score)
# Calculate Brier score for manual calibration
brier_manual <- xgb_val_results_calibrated %>%
mutate(default_numeric = as.numeric(default == "yes")) %>%
summarise(brier_score = mean((default_numeric - .pred_yes_calibrated)^2)) %>%
pull(brier_score)
# Calculate Brier score for probably package calibration
brier_probably <- xgb_val_results_prob_cal %>%
mutate(default_numeric = as.numeric(default == "yes")) %>%
summarise(brier_score = mean((default_numeric - .pred_yes)^2)) %>%
pull(brier_score)
cat("Original Brier Score:", round(brier_original, 4), "\n")
cat("Manual Calibration Brier Score:", round(brier_manual, 4), "\n")
cat("Probably Package Brier Score:", round(brier_probably, 4), "\n")
# ROC AUC should remain similar (calibration preserves ranking)
roc_original <- xgb_val_results %>%
roc_auc(truth = default, .pred_yes) %>%
pull(.estimate)
roc_manual <- xgb_val_results_calibrated %>%
roc_auc(truth = default, .pred_yes_calibrated) %>%
pull(.estimate)
roc_probably <- xgb_val_results_prob_cal %>%
roc_auc(truth = default, .pred_yes) %>%
pull(.estimate)
cat("Original ROC AUC:", round(roc_original, 4), "\n")
cat("Manual Calibration ROC AUC:", round(roc_manual, 4), "\n")
cat("Probably Package ROC AUC:", round(roc_probably, 4), "\n")
library(tidymodels)
library(xgboost)
library(vip)        # For variable importance
library(stringr)    # For string manipulation functions
library(probably)   # For calibration plots
library(ROSE)       # For imbalanced data visualization
library(corrplot)   # For correlation visualization
set.seed(123)
n <- 10000  # Larger sample size for more realistic modeling
# Create base features with realistic distributions
data <- tibble(
customer_id = paste0("CUS", formatC(1:n, width = 6, format = "d", flag = "0")),
# Demographics - with realistic age distribution for credit applicants
age = pmax(18, pmin(80, round(rnorm(n, 38, 13)))),
income = pmax(12000, round(rlnorm(n, log(52000), 0.8))),
employment_length = pmax(0, round(rexp(n, 1/6))),  # Exponential distribution for job tenure
home_ownership = sample(c("RENT", "MORTGAGE", "OWN"), n, replace = TRUE, prob = c(0.45, 0.40, 0.15)),
# Loan characteristics - with more realistic correlations
loan_amount = round(rlnorm(n, log(15000), 0.7) / 100) * 100,  # Log-normal for loan amounts
loan_term = sample(c(36, 60, 120), n, replace = TRUE, prob = c(0.6, 0.3, 0.1)),
# Credit history - with more realistic distributions
credit_score = round(pmin(850, pmax(300, rnorm(n, 700, 90)))),
dti_ratio = pmax(0, pmin(65, rlnorm(n, log(20), 0.4))),  # Debt-to-income ratio
delinq_2yrs = rpois(n, 0.4),  # Number of delinquencies in past 2 years
inq_last_6mths = rpois(n, 0.7),  # Number of inquiries in last 6 months
open_acc = pmax(1, round(rnorm(n, 10, 4))),  # Number of open accounts
pub_rec = rbinom(n, 2, 0.06),  # Number of public records
revol_util = pmin(100, pmax(0, rnorm(n, 40, 20))),  # Revolving utilization
total_acc = pmax(open_acc, open_acc + round(rnorm(n, 8, 6)))  # Total accounts
)
# Add realistic correlations between variables
data <- data %>%
mutate(
# Interest rate depends on credit score and loan term
interest_rate = 25 - (credit_score - 300) * (15/550) +
ifelse(loan_term == 36, -1, ifelse(loan_term == 60, 0, 1.5)) +
rnorm(n, 0, 1.5),
# Loan purpose with realistic probabilities
loan_purpose = sample(
c("debt_consolidation", "credit_card", "home_improvement", "major_purchase", "medical", "other"),
n, replace = TRUE,
prob = c(0.45, 0.25, 0.10, 0.08, 0.07, 0.05)
),
# Add some derived features that have predictive power
payment_amount = (loan_amount * (interest_rate/100/12) * (1 + interest_rate/100/12)^loan_term) /
((1 + interest_rate/100/12)^loan_term - 1),
payment_to_income_ratio = (payment_amount * 12) / income
)
# Create a more realistic default probability model with non-linear effects
logit_default <- with(data, {
-4.5 +  # Base intercept for ~10% default rate
-0.03 * (age - 18) +  # Age effect (stronger for younger borrowers)
-0.2 * log(income/10000) +  # Log-transformed income effect
-0.08 * employment_length +  # Employment length effect
ifelse(home_ownership == "OWN", -0.7, ifelse(home_ownership == "MORTGAGE", -0.3, 0)) +  # Home ownership
0.3 * log(loan_amount/1000) +  # Log-transformed loan amount
ifelse(loan_term == 36, 0, ifelse(loan_term == 60, 0.4, 0.8)) +  # Loan term
0.15 * interest_rate +  # Interest rate effect
ifelse(loan_purpose == "debt_consolidation", 0.5,
ifelse(loan_purpose == "credit_card", 0.4,
ifelse(loan_purpose == "medical", 0.6, 0))) +  # Loan purpose
-0.01 * (credit_score - 300) +  # Credit score (stronger effect at lower scores)
0.06 * dti_ratio +  # DTI ratio effect
0.4 * delinq_2yrs +  # Delinquencies effect (stronger effect for first delinquency)
0.3 * inq_last_6mths +  # Inquiries effect
-0.1 * log(open_acc + 1) +  # Open accounts (log-transformed)
0.8 * pub_rec +  # Public records (strong effect)
0.02 * revol_util +  # Revolving utilization
1.2 * payment_to_income_ratio +  # Payment to income ratio (strong effect)
rnorm(n, 0, 0.8)  # Add some noise for realistic variation
})
# Generate default flag with realistic default rate
prob_default <- plogis(logit_default)
data$default <- factor(rbinom(n, 1, prob_default), levels = c(0, 1), labels = c("no", "yes"))
# Check class distribution
table(data$default)
prop.table(table(data$default))
# Visualize the default rate
ggplot(data, aes(x = default, fill = default)) +
geom_bar(aes(y = ..prop.., group = 1)) +
scale_y_continuous(labels = scales::percent) +
labs(title = "Class Distribution in Credit Dataset",
y = "Percentage") +
theme_minimal()
# Visualize the relationship between key variables and default rate
ggplot(data, aes(x = credit_score, y = as.numeric(default) - 1)) +
geom_smooth(method = "loess") +
labs(title = "Default Rate by Credit Score",
x = "Credit Score", y = "Default Probability") +
theme_minimal()
# Examine correlation between numeric predictors
credit_cors <- data %>%
select(age, income, employment_length, loan_amount, interest_rate,
credit_score, dti_ratio, delinq_2yrs, revol_util, payment_to_income_ratio) %>%
cor()
corrplot(credit_cors, method = "circle", type = "upper",
tl.col = "black", tl.srt = 45, tl.cex = 0.7)
# Create initial train/test split (80/20)
set.seed(456)
initial_split <- initial_split(data, prop = 0.8, strata = default)
train_data <- training(initial_split)
test_data <- testing(initial_split)
# Create validation set from training data (75% train, 25% validation)
set.seed(789)
validation_split <- initial_split(train_data, prop = 0.75, strata = default)
# Check class imbalance in training data
train_class_counts <- table(training(validation_split)$default)
train_class_props <- prop.table(train_class_counts)
cat("Training data class distribution:\n")
print(train_class_counts)
cat("\nPercentage:\n")
print(train_class_props * 100)
# Visualize class imbalance
ROSE::roc.curve(training(validation_split)$default == "yes",
training(validation_split)$credit_score,
plotit = TRUE,                main = "ROC Curve for Credit Score Alone")
# Examine the distributions of key variables
par(mfrow = c(2, 2))
hist(training(validation_split)$credit_score,
main = "Credit Score Distribution", xlab = "Credit Score")
hist(training(validation_split)$dti_ratio,
main = "DTI Ratio Distribution", xlab = "DTI Ratio")
hist(training(validation_split)$payment_to_income_ratio,
main = "Payment to Income Ratio",
xlab = "Payment to Income Ratio")
hist(log(training(validation_split)$income),
main = "Log Income Distribution", xlab = "Log Income")
par(mfrow = c(1, 1))
# Create a recipe
credit_recipe <- recipe(default ~ ., data = training(validation_split)) %>%
# Remove ID column
step_rm(customer_id) %>%
# Convert categorical variables to factors
step_string2factor(home_ownership, loan_purpose) %>%
# Create additional domain-specific features
step_mutate(
# We already have payment_to_income_ratio from data generation
# Add more credit risk indicators
credit_utilization = revol_util / 100,
acc_to_age_ratio = total_acc / age,
delinq_per_acc = ifelse(total_acc > 0, delinq_2yrs / total_acc, 0),
inq_rate = inq_last_6mths / (open_acc + 0.1),  # Inquiry rate relative to open accounts
term_factor = loan_term / 12,  # Term in years
log_income = log(income),  # Log transform income
log_loan = log(loan_amount),  # Log transform loan amount
payment_ratio = payment_amount / (income / 12),  # Monthly payment to monthly income
util_to_income = (revol_util / 100) * (dti_ratio / 100)  # Interaction term
) %>%
# Handle categorical variables
step_dummy(all_nominal_predictors()) %>%
# Impute missing values (if any)
step_impute_median(all_numeric_predictors()) %>%
# Transform highly skewed variables
step_YeoJohnson(income, loan_amount, payment_amount) %>%
# Remove highly correlated predictors
step_corr(all_numeric_predictors(), threshold = 0.85) %>%
# Normalize numeric predictors
step_normalize(all_numeric_predictors()) %>%
# Remove zero-variance predictors
step_zv(all_predictors())
# Prep the recipe to examine the steps
prepped_recipe <- prep(credit_recipe)
prepped_recipe
# Check the transformed data
recipe_data <- bake(prepped_recipe, new_data = NULL)
glimpse(recipe_data)
# Verify
table(recipe_data$default)
# Define the logistic regression model
log_reg_spec <- logistic_reg(penalty = tune(), mixture = tune()) %>%
set_engine("glmnet") %>%
set_mode("classification")
# Create a logistic regression workflow
log_reg_workflow <- workflow() %>%
add_recipe(credit_recipe) %>%
add_model(log_reg_spec)
# Define the tuning grid for logistic regression
log_reg_grid <- grid_regular(
penalty(range = c(-5, 0), trans = log10_trans()),
mixture(range = c(0, 1)),
levels = c(10, 5)
)
# Define the XGBoost model with tunable parameters
xgb_spec <- boost_tree(
trees = tune(),
tree_depth = tune(),
min_n = tune(),
loss_reduction = tune(),
mtry = tune(),
learn_rate = tune()
) %>%
set_engine("xgboost", objective = "binary:logistic") %>%
set_mode("classification")
# Create an XGBoost workflow
xgb_workflow <- workflow() %>%
add_recipe(credit_recipe) %>%
add_model(xgb_spec)
# Define the tuning grid for XGBoost
xgb_grid <- grid_latin_hypercube(
trees(range = c(50, 100)),
tree_depth(range = c(3, 6)),
min_n(range = c(2, 8)),
loss_reduction(range = c(0.001, 1.0)),
mtry(range = c(5, 20)),
learn_rate(range = c(-4, -1), trans = log10_trans()),
size = 10
)
# Create cross-validation folds with stratification
set.seed(234)
cv_folds <- vfold_cv(training(validation_split), v = 3, strata = default)
# Define the metrics to evaluate
classification_metrics <- metric_set(
roc_auc,  # Area under the ROC curve
pr_auc,    # Area under the precision-recall curve
)
# Tune the logistic regression model
set.seed(345)
log_reg_tuned <- tune_grid(
log_reg_workflow,
resamples = cv_folds,
grid = log_reg_grid,
metrics = classification_metrics,
control = control_grid(save_pred = TRUE, verbose = TRUE)
)
# Tune the XGBoost model
set.seed(456)
xgb_tuned <- tune_grid(
xgb_workflow,
resamples = cv_folds,
grid = xgb_grid,
metrics = classification_metrics,
control = control_grid(save_pred = TRUE, verbose = TRUE)
)
# Collect and visualize logistic regression tuning results
log_reg_results <- log_reg_tuned %>% collect_metrics()
log_reg_results %>% filter(.metric == "roc_auc") %>% arrange(desc(mean)) %>% head()
#| label: model-evaluation
# Select best hyperparameters based on ROC AUC
best_log_reg_params <- select_best(log_reg_tuned, metric = "roc_auc")
best_xgb_params <- select_best(xgb_tuned, metric = "roc_auc")
# Finalize workflows with best parameters
final_log_reg_workflow <- log_reg_workflow %>%
finalize_workflow(best_log_reg_params)
final_xgb_workflow <- xgb_workflow %>%
finalize_workflow(best_xgb_params)
# Fit the final models on the full training data
final_log_reg_model <- final_log_reg_workflow %>%
fit(data = training(validation_split))
final_xgb_model <- final_xgb_workflow %>%
fit(data = training(validation_split))
# Make predictions on the validation set with both models
log_reg_val_results <- final_log_reg_model %>%
predict(testing(validation_split)) %>%
bind_cols(predict(final_log_reg_model, testing(validation_split), type = "prob")) %>%
bind_cols(testing(validation_split) %>% select(default, customer_id))
xgb_val_results <- final_xgb_model %>%
predict(testing(validation_split)) %>%
bind_cols(predict(final_xgb_model, testing(validation_split), type = "prob")) %>%
bind_cols(testing(validation_split) %>% select(default, customer_id))
# Evaluate model performance on validation set
log_reg_val_metrics <- log_reg_val_results %>%
metrics(truth = default, estimate = .pred_class, .pred_yes)
xgb_val_metrics <- xgb_val_results %>%
metrics(truth = default, estimate = .pred_class, .pred_yes)
cat("Logistic Regression Validation Metrics:\n")
print(log_reg_val_metrics)
cat("\nXGBoost Validation Metrics:\n")
print(xgb_val_metrics)
#| label: feature-importance
#| fig-width: 10
#| fig-height: 12
# Extract feature importance from XGBoost model
xgb_importance <- final_xgb_model %>%
extract_fit_parsnip() %>%
vip(num_features = 15) +
labs(title = "XGBoost Feature Importance")
xgb_importance
# Extract coefficients from logistic regression model
log_reg_importance <- final_log_reg_model %>%
extract_fit_parsnip() %>%
vip(num_features = 15) +
labs(title = "Logistic Regression Coefficient Importance")
log_reg_importance
# Create calibration plots for both models
log_reg_cal <- log_reg_val_results %>%
cal_plot_breaks(truth = default, estimate = .pred_yes, event_level = "second") +
labs(title = "Logistic Regression Probability Calibration")
xgb_cal <- xgb_val_results %>%
cal_plot_breaks(truth = default, estimate = .pred_yes, event_level = "second") +
labs(title = "XGBoost Probability Calibration")
log_reg_cal
xgb_cal
xgb_cal <- xgb_val_results %>%
cal_plot_breaks(truth = default, estimate = .pred_yes, event_level = "second", num_breaks = 7) +
labs(title = "XGBoost Probability Calibration")
log_reg_cal
log_reg_cal
xgb_cal
xgb_cal <- xgb_val_results %>%
cal_plot_breaks(truth = default, estimate = .pred_yes, event_level = "second", num_breaks = 5) +
labs(title = "XGBoost Probability Calibration")
xgb_cal
