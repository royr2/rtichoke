---
title: "Optimizing XGBoost Hyperparameters Using Bayesian Optimization in R"
date: "2024-09-18"
categories: [R, Analytics, Machine Learning]
image: "../images/bayesian_opt.png"
execute:
  echo: true
  warning: false
  message: false
  eval: true
  cache: true
---

## Introduction

Hyperparameter tuning for machine learning models represents a computationally intensive and time-consuming process. This tutorial demonstrates the implementation of Bayesian optimization techniques to efficiently identify optimal XGBoost hyperparameters, thereby reducing computational overhead while enhancing model performance. The methodology presented provides a systematic approach to hyperparameter optimization that significantly outperforms traditional grid search methods.

## Package Dependencies

This tutorial requires several specialized R packages for machine learning, optimization, and data preprocessing. The following packages must be installed and loaded before proceeding with the implementation.

```{r}
#| label: setup
#| message: false
#| warning: false

# Load required packages
library(xgboost)
library(ParBayesianOptimization)
library(mlbench)
library(dplyr)
library(recipes)
library(rsample)
```

## Data Acquisition and Initial Processing

This tutorial utilizes the Boston Housing dataset, a canonical regression problem that incorporates both numerical and categorical variables. This dataset provides an appropriate foundation for demonstrating Bayesian optimization techniques in a supervised learning context.

```{r}
#| label: data-load

# Load the Boston Housing dataset
data("BostonHousing2")

# Quick look at the data structure
str(BostonHousing2)
```

XGBoost algorithms require numerical input features exclusively. Therefore, we employ the `recipes` package to systematically transform categorical variables through appropriate preprocessing techniques:

```{r}
#| label: data-prep

# Create a recipe for preprocessing
rec <- recipe(cmedv ~ ., data = BostonHousing2) %>%
  # Collapse categories where population is < 3%
  step_other(town, chas, threshold = .03, other = "Other") %>% 
  # Create dummy variables for all factor variables 
  step_dummy(all_nominal_predictors())

# Train the recipe on the dataset
prep <- prep(rec, training = BostonHousing2)

# Create the final model matrix
model_df <- bake(prep, new_data = BostonHousing2)

# Check the column names after one-hot encoding
colnames(model_df)
```

Subsequently, we partition the dataset into training and testing subsets to enable proper model evaluation and prevent data leakage:

```{r}
#| label: train-test-split

# Create a 70/30 train-test split
splits <- rsample::initial_split(model_df, prop = 0.7)
train_df <- rsample::training(splits)
test_df <- rsample::testing(splits)

# Prepare the training data for XGBoost
X <- train_df %>%
  select(!medv, !cmedv) %>%
  as.matrix()

# Get the target variable
y <- train_df %>% pull(cmedv)

# Create cross-validation folds
folds <- list(
  fold1 = as.integer(seq(1, nrow(X), by = 5)),
  fold2 = as.integer(seq(2, nrow(X), by = 5))
)
```

## Bayesian Optimization Framework Implementation

The implementation of Bayesian optimization requires the development of two fundamental components:

1. **Objective Function**: A function that evaluates model performance for given hyperparameter configurations
2. **Parameter Space Definition**: The bounds and constraints that define the search space for optimization

```{r}
#| label: objective-function

# Our objective function takes hyperparameters as inputs
obj_func <- function(eta, max_depth, min_child_weight, subsample, lambda, alpha) {
  
  param <- list(
    # Learning parameters
    eta = eta,                       # Learning rate
    max_depth = max_depth,           # Tree depth
    min_child_weight = min_child_weight, # Min observations per node
    subsample = subsample,           # Data subsampling
    lambda = lambda,                 # L2 regularization
    alpha = alpha,                   # L1 regularization
    
    booster = "gbtree",             # Use tree model
    objective = "reg:squarederror",  # Regression task
    eval_metric = "mape"            # Mean Absolute Percentage Error
  )
  
  xgbcv <- xgb.cv(params = param,
                  data = X,
                  label = y,
                  nround = 50,
                  folds = folds,
                  prediction = TRUE,
                  early_stopping_rounds = 5,
                  verbose = 0,
                  maximize = FALSE)
  
  lst <- list(
    # First argument must be named as "Score"
    # Function finds maxima so inverting the output
    Score = -min(xgbcv$evaluation_log$test_mape_mean),
    
    # Get number of trees for the best performing model
    nrounds = xgbcv$best_iteration
  )
  
  return(lst)
}

# Define the search space for each parameter
bounds <- list(
  eta = c(0.001, 0.2),             # Learning rate range
  max_depth = c(1L, 10L),           # Tree depth range
  min_child_weight = c(1, 50),      # Min observations range
  subsample = c(0.1, 1),            # Subsampling range
  lambda = c(1, 10),                # L2 regularization range
  alpha = c(1, 10)                  # L1 regularization range
)
```

## Executing the Optimization Process

The optimization procedure employs intelligent search strategies to systematically explore the hyperparameter space, utilizing Gaussian process modeling to predict promising parameter combinations:

```{r}
#| label: bayesian-opt

set.seed(1234)
bayes_out <- bayesOpt(
  FUN = obj_func,                    # Our objective function
  bounds = bounds,                   # Parameter bounds
  initPoints = length(bounds) + 2,   # Initial random points
  iters.n = 10,                      # Number of iterations
  verbose = 0                        # Suppress output
)

# View top results
bayes_out$scoreSummary[1:5, c(3:8, 13)]

# Get the best parameters
best_params <- getBestPars(bayes_out)
data.frame(best_params)
```

## Final Model Development and Evaluation

Upon identification of optimal hyperparameter configurations, we proceed to train the final XGBoost model and evaluate its performance on the held-out test dataset.

```{r}
#| label: final-model

# Combine best params with base params
opt_params <- append(
  list(booster = "gbtree", 
       objective = "reg:squarederror", 
       eval_metric = "mae"), 
  best_params
)

# Run cross-validation to determine optimal number of rounds
xgbcv <- xgb.cv(
  params = opt_params,
  data = X,
  label = y,
  nround = 100,
  folds = folds,
  prediction = TRUE,
  early_stopping_rounds = 5,
  verbose = 0,
  maximize = FALSE
)

# Get optimal number of rounds
nrounds = xgbcv$best_iteration

# Fit the final XGBoost model
mdl <- xgboost(
  data = X, 
  label = y, 
  params = opt_params, 
  maximize = FALSE, 
  early_stopping_rounds = 5, 
  nrounds = nrounds, 
  verbose = 0
)

# Make predictions on the test set
actuals <- test_df$cmedv
predicted <- test_df %>%
  select_at(mdl$feature_names) %>%
  as.matrix() %>%
  predict(mdl, newdata = .)

# Evaluate performance using Mean Absolute Percentage Error (MAPE)
mape <- mean(abs(actuals - predicted)/actuals)
cat("MAPE on test set:", mape)
```
## Key Advantages of Bayesian Optimization

Bayesian optimization demonstrates significant superiority over conventional hyperparameter tuning methodologies, particularly grid search approaches:

1. **Computational Efficiency**: Achieves optimal parameter identification through substantially fewer iterations
2. **Adaptive Learning**: Incorporates knowledge from previous evaluations to concentrate search efforts on promising parameter regions
3. **Scalability**: Maintains computational efficiency regardless of hyperparameter dimensionality
4. **Time Optimization**: Completes optimization procedures in significantly reduced timeframes while achieving equivalent or superior performance outcomes

### Practical Considerations

This methodology becomes increasingly critical as model complexity and hyperparameter spaces expand. For production environments, increasing the iteration count (`iters.n`) could help ensure comprehensive exploration of the parameter landscape.

## Key Takeaways

- **Gaussian process modeling enables intelligent parameter space exploration** by learning from previous evaluations
- **Proper data preprocessing is critical** for XGBoost implementation, particularly categorical variable encoding
- **Cross-validation integration ensures robust hyperparameter evaluation** and prevents overfitting to specific data partitions
- **The methodology scales effectively** to high-dimensional hyperparameter spaces without proportional computational increases
- **Production implementations benefit from increased iteration counts** to ensure thorough parameter space exploration

This technique can extend beyond XGBoost to any machine learning algorithm requiring hyperparameter optimization, providing a foundation for efficient model development across diverse analytical contexts.
