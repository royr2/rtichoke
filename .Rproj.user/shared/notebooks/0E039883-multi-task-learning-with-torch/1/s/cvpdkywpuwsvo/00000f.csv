"0","#| label: enhanced-training-loop"
"0",""
"0","# Hyperparameters"
"0","epochs <- 200  # Increased epochs since we have early stopping"
"0",""
"0","# Enhanced training history tracking"
"0","training_history <- data.frame("
"0","  epoch = integer(),"
"0","  train_reg_loss = numeric(),"
"0","  train_cls_loss = numeric(),"
"0","  train_total_loss = numeric(),"
"0","  val_reg_loss = numeric(),"
"0","  val_cls_loss = numeric(),"
"0","  val_total_loss = numeric(),"
"0","  val_accuracy = numeric()"
"0",")"
"0",""
"0","cat(""Starting enhanced training with overfitting prevention...\n"")"
"1","Starting enhanced training with overfitting prevention...
"
"0","for (epoch in 1:epochs) {"
"0","  # Training phase"
"0","  model$train()"
"0","  optimizer$zero_grad()"
"0","  "
"0","  # Forward pass on training data"
"0","  outputs <- model(x_train)"
"0","  "
"0","  # Calculate training loss for each task"
"0","  train_reg_loss <- regression_loss_fn("
"0","    outputs$regression$squeeze(), "
"0","    y_reg_train"
"0","  )"
"0","  "
"0","  train_cls_loss <- classification_loss_fn("
"0","    outputs$classification$squeeze(), "
"0","    y_cls_train"
"0","  )"
"0","  "
"0","  # Weighted combined training loss"
"0","  train_total_loss <- task_weights[""regression""] * train_reg_loss + "
"0","                     task_weights[""classification""] * train_cls_loss"
"0","  "
"0","  # Backward pass and optimize"
"0","  train_total_loss$backward()"
"0","  "
"0","  # Gradient clipping to prevent exploding gradients"
"0","  torch_nn_utils_clip_grad_norm_(model$parameters, max_norm = 1.0)"
"0","  "
"0","  optimizer$step()"
"0","  "
"0","  # Validation phase"
"0","  model$eval()"
"0","  with_no_grad({"
"0","    val_outputs <- model(x_val)"
"0","    "
"0","    # Calculate validation losses"
"0","    val_reg_loss <- regression_loss_fn("
"0","      val_outputs$regression$squeeze(), "
"0","      y_reg_val"
"0","    )"
"0","    "
"0","    val_cls_loss <- classification_loss_fn("
"0","      val_outputs$classification$squeeze(), "
"0","      y_cls_val"
"0","    )"
"0","    "
"0","    val_total_loss <- task_weights[""regression""] * val_reg_loss + "
"0","                     task_weights[""classification""] * val_cls_loss"
"0","    "
"0","    # Calculate validation accuracy"
"0","    val_cls_probs <- nnf_sigmoid(val_outputs$classification$squeeze())"
"0","    val_cls_preds <- (val_cls_probs > 0.5)$to(torch_int())"
"0","    val_accuracy <- (val_cls_preds == y_cls_val$to(torch_int()))$sum()$item() / length(val_indices)"
"0","  })"
"0","  "
"0","  # Record history"
"0","  training_history <- rbind("
"0","    training_history,"
"0","    data.frame("
"0","      epoch = epoch,"
"0","      train_reg_loss = as.numeric(train_reg_loss$item()),"
"0","      train_cls_loss = as.numeric(train_cls_loss$item()),"
"0","      train_total_loss = as.numeric(train_total_loss$item()),"
"0","      val_reg_loss = as.numeric(val_reg_loss$item()),"
"0","      val_cls_loss = as.numeric(val_cls_loss$item()),"
"0","      val_total_loss = as.numeric(val_total_loss$item()),"
"0","      val_accuracy = val_accuracy"
"0","    )"
"0","  )"
"0","  "
"0","  # Early stopping logic"
"0","  current_val_loss <- as.numeric(val_total_loss$item())"
"0","  if (current_val_loss < best_val_loss) {"
"0","    best_val_loss <- current_val_loss"
"0","    patience_counter <- 0"
"0","    # Save best model state"
"0","    best_model_state <- model$state_dict()"
"0","  } else {"
"0","    patience_counter <- patience_counter + 1"
"0","  }"
"0","  "
"0","  # Print progress every 25 epochs"
"0","  if (epoch %% 25 == 0 || epoch == 1) {"
"0","    cat(sprintf(""Epoch %d - Train Loss: %.4f, Val Loss: %.4f, Val Acc: %.3f, Patience: %d/%d\n"", "
"0","                epoch, train_total_loss$item(), val_total_loss$item(), "
"0","                val_accuracy, patience_counter, patience))"
"0","  }"
"0","  "
"0","  # Early stopping"
"0","  if (patience_counter >= patience) {"
"0","    cat(sprintf(""\nEarly stopping at epoch %d. Best validation loss: %.4f\n"", epoch, best_val_loss))"
"0","    break"
"0","  }"
"0","}"
"2","Error in torch_nn_utils_clip_grad_norm_(model$parameters, max_norm = 1) : 
  could not find function ""torch_nn_utils_clip_grad_norm_""
"
